{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Forecasting Service Metrics\n",
    "\n",
    "Authors: Tatiana Barrios, Anisha Anandkrishnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from pandas import concat\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Data preparation: Use one of the methods described in Project 1 (Advanced), Task 1 to pre-process the trace. Remove possible outliers. Reduce the dimensionality of the feature space to k = 16 using tree-based feature selection. Then, split the processed trace into training and test samples (x(t),y(t)) by assigning the samples with t < T to the training set and t ≥T to the test set. T is chosen so that the training set contains 70% of the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task II - Using Recurrent Neural Networks (RNNs) for forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anisha/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/anisha/anaconda3/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_all_..usr</th>\n",
       "      <th>0_all_..sys</th>\n",
       "      <th>0_all_..iowait</th>\n",
       "      <th>0_all_..soft</th>\n",
       "      <th>0_all_..idle</th>\n",
       "      <th>0_cpu0_.usr</th>\n",
       "      <th>0_cpu0_.sys</th>\n",
       "      <th>0_cpu0_.iowait</th>\n",
       "      <th>0_cpu0_.soft</th>\n",
       "      <th>0_cpu0_.idle</th>\n",
       "      <th>...</th>\n",
       "      <th>36_RxBytes.1</th>\n",
       "      <th>36_TxBytes.1</th>\n",
       "      <th>40_RxPacktes.1</th>\n",
       "      <th>40_TxPacktes.1</th>\n",
       "      <th>40_RxBytes.1</th>\n",
       "      <th>40_TxBytes.1</th>\n",
       "      <th>41_RxPacktes.1</th>\n",
       "      <th>41_TxPacktes.1</th>\n",
       "      <th>41_RxBytes.1</th>\n",
       "      <th>41_TxBytes.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.069116</td>\n",
       "      <td>1.088334</td>\n",
       "      <td>0.766004</td>\n",
       "      <td>0.573967</td>\n",
       "      <td>-0.579449</td>\n",
       "      <td>1.364018</td>\n",
       "      <td>0.977205</td>\n",
       "      <td>-0.00831</td>\n",
       "      <td>-0.355042</td>\n",
       "      <td>-2.078731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.583733</td>\n",
       "      <td>1.335871</td>\n",
       "      <td>1.090350</td>\n",
       "      <td>1.069987</td>\n",
       "      <td>1.211176</td>\n",
       "      <td>0.975349</td>\n",
       "      <td>1.053685</td>\n",
       "      <td>1.064968</td>\n",
       "      <td>0.971475</td>\n",
       "      <td>1.180097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247507</td>\n",
       "      <td>2.503497</td>\n",
       "      <td>0.377145</td>\n",
       "      <td>0.573967</td>\n",
       "      <td>-0.865816</td>\n",
       "      <td>1.364018</td>\n",
       "      <td>4.193380</td>\n",
       "      <td>-0.00831</td>\n",
       "      <td>-0.355042</td>\n",
       "      <td>-2.078731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.551250</td>\n",
       "      <td>1.485179</td>\n",
       "      <td>1.190029</td>\n",
       "      <td>1.263907</td>\n",
       "      <td>1.110672</td>\n",
       "      <td>1.287438</td>\n",
       "      <td>1.269613</td>\n",
       "      <td>1.224345</td>\n",
       "      <td>1.294599</td>\n",
       "      <td>1.157325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.158311</td>\n",
       "      <td>1.421314</td>\n",
       "      <td>0.571574</td>\n",
       "      <td>-0.290273</td>\n",
       "      <td>-0.579449</td>\n",
       "      <td>1.384053</td>\n",
       "      <td>0.993448</td>\n",
       "      <td>-0.00831</td>\n",
       "      <td>-0.355042</td>\n",
       "      <td>-1.518028</td>\n",
       "      <td>...</td>\n",
       "      <td>1.636638</td>\n",
       "      <td>1.320954</td>\n",
       "      <td>1.119925</td>\n",
       "      <td>1.012137</td>\n",
       "      <td>1.234329</td>\n",
       "      <td>0.859493</td>\n",
       "      <td>1.037388</td>\n",
       "      <td>1.111521</td>\n",
       "      <td>0.901761</td>\n",
       "      <td>1.236388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.425898</td>\n",
       "      <td>1.421314</td>\n",
       "      <td>0.766004</td>\n",
       "      <td>3.382748</td>\n",
       "      <td>-1.135338</td>\n",
       "      <td>1.404089</td>\n",
       "      <td>-0.630882</td>\n",
       "      <td>-0.00831</td>\n",
       "      <td>2.626215</td>\n",
       "      <td>-0.945519</td>\n",
       "      <td>...</td>\n",
       "      <td>1.503362</td>\n",
       "      <td>1.510045</td>\n",
       "      <td>1.144023</td>\n",
       "      <td>1.126207</td>\n",
       "      <td>1.215636</td>\n",
       "      <td>1.070883</td>\n",
       "      <td>1.113167</td>\n",
       "      <td>1.141096</td>\n",
       "      <td>1.054886</td>\n",
       "      <td>1.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.158311</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>5.432315</td>\n",
       "      <td>1.654267</td>\n",
       "      <td>-2.129200</td>\n",
       "      <td>2.355777</td>\n",
       "      <td>2.585292</td>\n",
       "      <td>-0.00831</td>\n",
       "      <td>-0.355042</td>\n",
       "      <td>-2.078731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.556992</td>\n",
       "      <td>1.341298</td>\n",
       "      <td>1.134713</td>\n",
       "      <td>1.095245</td>\n",
       "      <td>1.194114</td>\n",
       "      <td>1.050206</td>\n",
       "      <td>1.113574</td>\n",
       "      <td>1.146025</td>\n",
       "      <td>1.094834</td>\n",
       "      <td>1.186764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1751 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0_all_..usr  0_all_..sys  0_all_..iowait  0_all_..soft  0_all_..idle  \\\n",
       "0     0.069116     1.088334        0.766004      0.573967     -0.579449   \n",
       "1     0.247507     2.503497        0.377145      0.573967     -0.865816   \n",
       "2     0.158311     1.421314        0.571574     -0.290273     -0.579449   \n",
       "3     0.425898     1.421314        0.766004      3.382748     -1.135338   \n",
       "4     0.158311     0.006151        5.432315      1.654267     -2.129200   \n",
       "\n",
       "   0_cpu0_.usr  0_cpu0_.sys  0_cpu0_.iowait  0_cpu0_.soft  0_cpu0_.idle  ...  \\\n",
       "0     1.364018     0.977205        -0.00831     -0.355042     -2.078731  ...   \n",
       "1     1.364018     4.193380        -0.00831     -0.355042     -2.078731  ...   \n",
       "2     1.384053     0.993448        -0.00831     -0.355042     -1.518028  ...   \n",
       "3     1.404089    -0.630882        -0.00831      2.626215     -0.945519  ...   \n",
       "4     2.355777     2.585292        -0.00831     -0.355042     -2.078731  ...   \n",
       "\n",
       "   36_RxBytes.1  36_TxBytes.1  40_RxPacktes.1  40_TxPacktes.1  40_RxBytes.1  \\\n",
       "0      1.583733      1.335871        1.090350        1.069987      1.211176   \n",
       "1      1.551250      1.485179        1.190029        1.263907      1.110672   \n",
       "2      1.636638      1.320954        1.119925        1.012137      1.234329   \n",
       "3      1.503362      1.510045        1.144023        1.126207      1.215636   \n",
       "4      1.556992      1.341298        1.134713        1.095245      1.194114   \n",
       "\n",
       "   40_TxBytes.1  41_RxPacktes.1  41_TxPacktes.1  41_RxBytes.1  41_TxBytes.1  \n",
       "0      0.975349        1.053685        1.064968      0.971475      1.180097  \n",
       "1      1.287438        1.269613        1.224345      1.294599      1.157325  \n",
       "2      0.859493        1.037388        1.111521      0.901761      1.236388  \n",
       "3      1.070883        1.113167        1.141096      1.054886      1.212100  \n",
       "4      1.050206        1.113574        1.146025      1.094834      1.186764  \n",
       "\n",
       "[5 rows x 1751 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('X.csv')\n",
    "Y = pd.read_csv('Y.csv')\n",
    "X.index = pd.to_datetime(X['TimeStamp'])\n",
    "Y.index = pd.to_datetime(Y['TimeStamp'])\n",
    "X_dropped = X.drop(labels=[\"Unnamed: 0\", \"TimeStamp\"], axis=1, inplace=False)\n",
    "Y_dropped = Y.drop(labels=[\"Unnamed: 0\", \"TimeStamp\"], axis=1, inplace=False)\n",
    "X_preprocessed = pd.DataFrame()\n",
    "X_tmp = preprocessing.StandardScaler().fit_transform(X_dropped)\n",
    "for i, n in enumerate(X_dropped):\n",
    "        X_preprocessed[n] = X_tmp[:, i]\n",
    "        \n",
    "X_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14481, 1751)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dropped samples:  136\n",
      "Stored 'X_clean' (DataFrame)\n",
      "Stored 'Y_clean' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "#converting Y_dropped to make it of the same form as X_preprocessed\n",
    "Y_new = pd.DataFrame()\n",
    "Y_tmp=Y_dropped.to_numpy()\n",
    "\n",
    "for i, n in enumerate(Y_dropped):\n",
    "        Y_new[n] = Y_tmp[:, i]  \n",
    "# outlier rejection   \n",
    "remove = []\n",
    "for i in X_preprocessed:\n",
    "    for j in range(len(X_preprocessed[i])):\n",
    "        if j not in remove and abs(X_preprocessed[i][j]) > 80:\n",
    "            remove.append(j)\n",
    "X_clean = X_preprocessed.drop(labels=remove, axis=0, inplace=False)\n",
    "Y_clean = Y_new.drop(labels=remove, axis=0, inplace=False)\n",
    "\n",
    "print(\"Number of dropped samples: \", (len(remove)))\n",
    "%store X_clean\n",
    "%store Y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14345, 1751)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = X_clean.reset_index()\n",
    "Y_clean = Y_clean.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = X_clean.drop(X_clean.columns[0], axis=1)\n",
    "Y_clean = Y_clean.drop(Y_clean.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_all_..usr</th>\n",
       "      <th>0_all_..sys</th>\n",
       "      <th>0_all_..iowait</th>\n",
       "      <th>0_all_..soft</th>\n",
       "      <th>0_all_..idle</th>\n",
       "      <th>0_cpu0_.usr</th>\n",
       "      <th>0_cpu0_.sys</th>\n",
       "      <th>0_cpu0_.iowait</th>\n",
       "      <th>0_cpu0_.soft</th>\n",
       "      <th>0_cpu0_.idle</th>\n",
       "      <th>...</th>\n",
       "      <th>36_RxBytes.1</th>\n",
       "      <th>36_TxBytes.1</th>\n",
       "      <th>40_RxPacktes.1</th>\n",
       "      <th>40_TxPacktes.1</th>\n",
       "      <th>40_RxBytes.1</th>\n",
       "      <th>40_TxBytes.1</th>\n",
       "      <th>41_RxPacktes.1</th>\n",
       "      <th>41_TxPacktes.1</th>\n",
       "      <th>41_RxBytes.1</th>\n",
       "      <th>41_TxBytes.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.069116</td>\n",
       "      <td>1.088334</td>\n",
       "      <td>0.766004</td>\n",
       "      <td>0.573967</td>\n",
       "      <td>-0.579449</td>\n",
       "      <td>1.364018</td>\n",
       "      <td>0.977205</td>\n",
       "      <td>-0.00831</td>\n",
       "      <td>-0.355042</td>\n",
       "      <td>-2.078731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.583733</td>\n",
       "      <td>1.335871</td>\n",
       "      <td>1.090350</td>\n",
       "      <td>1.069987</td>\n",
       "      <td>1.211176</td>\n",
       "      <td>0.975349</td>\n",
       "      <td>1.053685</td>\n",
       "      <td>1.064968</td>\n",
       "      <td>0.971475</td>\n",
       "      <td>1.180097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247507</td>\n",
       "      <td>2.503497</td>\n",
       "      <td>0.377145</td>\n",
       "      <td>0.573967</td>\n",
       "      <td>-0.865816</td>\n",
       "      <td>1.364018</td>\n",
       "      <td>4.193380</td>\n",
       "      <td>-0.00831</td>\n",
       "      <td>-0.355042</td>\n",
       "      <td>-2.078731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.551250</td>\n",
       "      <td>1.485179</td>\n",
       "      <td>1.190029</td>\n",
       "      <td>1.263907</td>\n",
       "      <td>1.110672</td>\n",
       "      <td>1.287438</td>\n",
       "      <td>1.269613</td>\n",
       "      <td>1.224345</td>\n",
       "      <td>1.294599</td>\n",
       "      <td>1.157325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.158311</td>\n",
       "      <td>1.421314</td>\n",
       "      <td>0.571574</td>\n",
       "      <td>-0.290273</td>\n",
       "      <td>-0.579449</td>\n",
       "      <td>1.384053</td>\n",
       "      <td>0.993448</td>\n",
       "      <td>-0.00831</td>\n",
       "      <td>-0.355042</td>\n",
       "      <td>-1.518028</td>\n",
       "      <td>...</td>\n",
       "      <td>1.636638</td>\n",
       "      <td>1.320954</td>\n",
       "      <td>1.119925</td>\n",
       "      <td>1.012137</td>\n",
       "      <td>1.234329</td>\n",
       "      <td>0.859493</td>\n",
       "      <td>1.037388</td>\n",
       "      <td>1.111521</td>\n",
       "      <td>0.901761</td>\n",
       "      <td>1.236388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.425898</td>\n",
       "      <td>1.421314</td>\n",
       "      <td>0.766004</td>\n",
       "      <td>3.382748</td>\n",
       "      <td>-1.135338</td>\n",
       "      <td>1.404089</td>\n",
       "      <td>-0.630882</td>\n",
       "      <td>-0.00831</td>\n",
       "      <td>2.626215</td>\n",
       "      <td>-0.945519</td>\n",
       "      <td>...</td>\n",
       "      <td>1.503362</td>\n",
       "      <td>1.510045</td>\n",
       "      <td>1.144023</td>\n",
       "      <td>1.126207</td>\n",
       "      <td>1.215636</td>\n",
       "      <td>1.070883</td>\n",
       "      <td>1.113167</td>\n",
       "      <td>1.141096</td>\n",
       "      <td>1.054886</td>\n",
       "      <td>1.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.158311</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>5.432315</td>\n",
       "      <td>1.654267</td>\n",
       "      <td>-2.129200</td>\n",
       "      <td>2.355777</td>\n",
       "      <td>2.585292</td>\n",
       "      <td>-0.00831</td>\n",
       "      <td>-0.355042</td>\n",
       "      <td>-2.078731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.556992</td>\n",
       "      <td>1.341298</td>\n",
       "      <td>1.134713</td>\n",
       "      <td>1.095245</td>\n",
       "      <td>1.194114</td>\n",
       "      <td>1.050206</td>\n",
       "      <td>1.113574</td>\n",
       "      <td>1.146025</td>\n",
       "      <td>1.094834</td>\n",
       "      <td>1.186764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1751 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0_all_..usr  0_all_..sys  0_all_..iowait  0_all_..soft  0_all_..idle  \\\n",
       "0     0.069116     1.088334        0.766004      0.573967     -0.579449   \n",
       "1     0.247507     2.503497        0.377145      0.573967     -0.865816   \n",
       "2     0.158311     1.421314        0.571574     -0.290273     -0.579449   \n",
       "3     0.425898     1.421314        0.766004      3.382748     -1.135338   \n",
       "4     0.158311     0.006151        5.432315      1.654267     -2.129200   \n",
       "\n",
       "   0_cpu0_.usr  0_cpu0_.sys  0_cpu0_.iowait  0_cpu0_.soft  0_cpu0_.idle  ...  \\\n",
       "0     1.364018     0.977205        -0.00831     -0.355042     -2.078731  ...   \n",
       "1     1.364018     4.193380        -0.00831     -0.355042     -2.078731  ...   \n",
       "2     1.384053     0.993448        -0.00831     -0.355042     -1.518028  ...   \n",
       "3     1.404089    -0.630882        -0.00831      2.626215     -0.945519  ...   \n",
       "4     2.355777     2.585292        -0.00831     -0.355042     -2.078731  ...   \n",
       "\n",
       "   36_RxBytes.1  36_TxBytes.1  40_RxPacktes.1  40_TxPacktes.1  40_RxBytes.1  \\\n",
       "0      1.583733      1.335871        1.090350        1.069987      1.211176   \n",
       "1      1.551250      1.485179        1.190029        1.263907      1.110672   \n",
       "2      1.636638      1.320954        1.119925        1.012137      1.234329   \n",
       "3      1.503362      1.510045        1.144023        1.126207      1.215636   \n",
       "4      1.556992      1.341298        1.134713        1.095245      1.194114   \n",
       "\n",
       "   40_TxBytes.1  41_RxPacktes.1  41_TxPacktes.1  41_RxBytes.1  41_TxBytes.1  \n",
       "0      0.975349        1.053685        1.064968      0.971475      1.180097  \n",
       "1      1.287438        1.269613        1.224345      1.294599      1.157325  \n",
       "2      0.859493        1.037388        1.111521      0.901761      1.236388  \n",
       "3      1.070883        1.113167        1.141096      1.054886      1.212100  \n",
       "4      1.050206        1.113574        1.146025      1.094834      1.186764  \n",
       "\n",
       "[5 rows x 1751 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReadsAvg</th>\n",
       "      <th>WritesAvg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59.1331</td>\n",
       "      <td>118.7723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.8588</td>\n",
       "      <td>118.4950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.8251</td>\n",
       "      <td>116.8042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63.8154</td>\n",
       "      <td>128.3462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.4993</td>\n",
       "      <td>118.2260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReadsAvg  WritesAvg\n",
       "0   59.1331   118.7723\n",
       "1   59.8588   118.4950\n",
       "2   57.8251   116.8042\n",
       "3   63.8154   128.3462\n",
       "4   57.4993   118.2260"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReadsAvg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59.1331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.8588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.8251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.4993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReadsAvg\n",
       "0   59.1331\n",
       "1   59.8588\n",
       "2   57.8251\n",
       "3   63.8154\n",
       "4   57.4993"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r X_clean\n",
    "%store -r Y_clean\n",
    "\n",
    "#Reduce the dimensionality of the feature space \n",
    "tree = ExtraTreesRegressor(n_estimators=100, n_jobs=20)\n",
    "tree = tree.fit(X_clean, Y_clean)\n",
    "model = SelectFromModel(tree,prefit=True,max_features=16,threshold=-np.inf)\n",
    "X_fs = model.transform(X_clean)\n",
    "top_features = []\n",
    "get_feat = model.get_support()\n",
    "for i, n in enumerate(X_clean):\n",
    "    if get_feat[i]:\n",
    "        top_features.append(n)\n",
    "feature_name = []\n",
    "for i in range(16):\n",
    "    feature_name.append('feature'+str(i+1)+\" : \"+top_features[i])\n",
    "X_latest = pd.DataFrame(data = X_fs,columns= feature_name)\n",
    "Y_latest = pd.DataFrame(data = Y_clean,columns= Y_dropped.columns)\n",
    "Y_latest = Y_latest.drop(labels=[\"WritesAvg\"], axis=1, inplace=False)\n",
    "X_latest.head()\n",
    "Y_latest.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1 : 1_i127_intr.s</th>\n",
       "      <th>feature2 : 5_campg.s</th>\n",
       "      <th>feature3 : 4_RxPacktes</th>\n",
       "      <th>feature4 : 4_TxPacktes</th>\n",
       "      <th>feature5 : 15_RxPacktes</th>\n",
       "      <th>feature6 : 15_TxBytes</th>\n",
       "      <th>feature7 : 17_TxPacktes</th>\n",
       "      <th>feature8 : 29_TxBytes</th>\n",
       "      <th>feature9 : 41_TxBytes</th>\n",
       "      <th>feature10 : 4_RxPacktes.1</th>\n",
       "      <th>feature11 : 4_TxPacktes.1</th>\n",
       "      <th>feature12 : 4_TxBytes.1</th>\n",
       "      <th>feature13 : 15_RxPacktes.1</th>\n",
       "      <th>feature14 : 17_TxPacktes.1</th>\n",
       "      <th>feature15 : 40_RxPacktes.1</th>\n",
       "      <th>feature16 : 41_TxBytes.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.119895</td>\n",
       "      <td>0.019228</td>\n",
       "      <td>1.039869</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>1.028539</td>\n",
       "      <td>1.184277</td>\n",
       "      <td>1.034312</td>\n",
       "      <td>1.165585</td>\n",
       "      <td>1.180097</td>\n",
       "      <td>1.039869</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>1.214220</td>\n",
       "      <td>1.028539</td>\n",
       "      <td>1.034312</td>\n",
       "      <td>1.090350</td>\n",
       "      <td>1.180097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.855128</td>\n",
       "      <td>0.051963</td>\n",
       "      <td>1.299455</td>\n",
       "      <td>1.238908</td>\n",
       "      <td>1.247687</td>\n",
       "      <td>1.219757</td>\n",
       "      <td>1.238812</td>\n",
       "      <td>1.201460</td>\n",
       "      <td>1.157325</td>\n",
       "      <td>1.299455</td>\n",
       "      <td>1.238908</td>\n",
       "      <td>1.194544</td>\n",
       "      <td>1.247687</td>\n",
       "      <td>1.238812</td>\n",
       "      <td>1.190029</td>\n",
       "      <td>1.157325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.879198</td>\n",
       "      <td>0.043744</td>\n",
       "      <td>1.025629</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>1.079864</td>\n",
       "      <td>1.105230</td>\n",
       "      <td>1.077493</td>\n",
       "      <td>1.237204</td>\n",
       "      <td>1.236388</td>\n",
       "      <td>1.025629</td>\n",
       "      <td>1.115817</td>\n",
       "      <td>1.240098</td>\n",
       "      <td>1.079864</td>\n",
       "      <td>1.077493</td>\n",
       "      <td>1.119925</td>\n",
       "      <td>1.236388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.566292</td>\n",
       "      <td>0.022571</td>\n",
       "      <td>1.138740</td>\n",
       "      <td>1.127852</td>\n",
       "      <td>1.132818</td>\n",
       "      <td>1.172840</td>\n",
       "      <td>1.135340</td>\n",
       "      <td>1.189385</td>\n",
       "      <td>1.212100</td>\n",
       "      <td>1.138740</td>\n",
       "      <td>1.127852</td>\n",
       "      <td>1.187528</td>\n",
       "      <td>1.132818</td>\n",
       "      <td>1.135340</td>\n",
       "      <td>1.144023</td>\n",
       "      <td>1.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.939372</td>\n",
       "      <td>0.048202</td>\n",
       "      <td>1.112293</td>\n",
       "      <td>0.980143</td>\n",
       "      <td>1.148704</td>\n",
       "      <td>1.167679</td>\n",
       "      <td>1.125563</td>\n",
       "      <td>1.219838</td>\n",
       "      <td>1.186764</td>\n",
       "      <td>1.112293</td>\n",
       "      <td>0.980143</td>\n",
       "      <td>1.005623</td>\n",
       "      <td>1.148704</td>\n",
       "      <td>1.125563</td>\n",
       "      <td>1.134713</td>\n",
       "      <td>1.186764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1 : 1_i127_intr.s  feature2 : 5_campg.s  feature3 : 4_RxPacktes  \\\n",
       "0                  1.119895              0.019228                1.039869   \n",
       "1                  0.855128              0.051963                1.299455   \n",
       "2                  0.879198              0.043744                1.025629   \n",
       "3                  0.566292              0.022571                1.138740   \n",
       "4                  0.939372              0.048202                1.112293   \n",
       "\n",
       "   feature4 : 4_TxPacktes  feature5 : 15_RxPacktes  feature6 : 15_TxBytes  \\\n",
       "0                1.088463                 1.028539               1.184277   \n",
       "1                1.238908                 1.247687               1.219757   \n",
       "2                1.115817                 1.079864               1.105230   \n",
       "3                1.127852                 1.132818               1.172840   \n",
       "4                0.980143                 1.148704               1.167679   \n",
       "\n",
       "   feature7 : 17_TxPacktes  feature8 : 29_TxBytes  feature9 : 41_TxBytes  \\\n",
       "0                 1.034312               1.165585               1.180097   \n",
       "1                 1.238812               1.201460               1.157325   \n",
       "2                 1.077493               1.237204               1.236388   \n",
       "3                 1.135340               1.189385               1.212100   \n",
       "4                 1.125563               1.219838               1.186764   \n",
       "\n",
       "   feature10 : 4_RxPacktes.1  feature11 : 4_TxPacktes.1  \\\n",
       "0                   1.039869                   1.088463   \n",
       "1                   1.299455                   1.238908   \n",
       "2                   1.025629                   1.115817   \n",
       "3                   1.138740                   1.127852   \n",
       "4                   1.112293                   0.980143   \n",
       "\n",
       "   feature12 : 4_TxBytes.1  feature13 : 15_RxPacktes.1  \\\n",
       "0                 1.214220                    1.028539   \n",
       "1                 1.194544                    1.247687   \n",
       "2                 1.240098                    1.079864   \n",
       "3                 1.187528                    1.132818   \n",
       "4                 1.005623                    1.148704   \n",
       "\n",
       "   feature14 : 17_TxPacktes.1  feature15 : 40_RxPacktes.1  \\\n",
       "0                    1.034312                    1.090350   \n",
       "1                    1.238812                    1.190029   \n",
       "2                    1.077493                    1.119925   \n",
       "3                    1.135340                    1.144023   \n",
       "4                    1.125563                    1.134713   \n",
       "\n",
       "   feature16 : 41_TxBytes.1  \n",
       "0                  1.180097  \n",
       "1                  1.157325  \n",
       "2                  1.236388  \n",
       "3                  1.212100  \n",
       "4                  1.186764  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_latest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14345, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_latest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReadsAvg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14476</th>\n",
       "      <td>55.6183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14477</th>\n",
       "      <td>52.6730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>52.1335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>52.1671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>52.6722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ReadsAvg\n",
       "14476   55.6183\n",
       "14477   52.6730\n",
       "14478   52.1335\n",
       "14479   52.1671\n",
       "14480   52.6722"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_latest.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_latest = Y_latest.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_latest = Y_latest.drop(Y_latest.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReadsAvg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14340</th>\n",
       "      <td>55.6183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14341</th>\n",
       "      <td>52.6730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14342</th>\n",
       "      <td>52.1335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14343</th>\n",
       "      <td>52.1671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14344</th>\n",
       "      <td>52.6722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ReadsAvg\n",
       "14340   55.6183\n",
       "14341   52.6730\n",
       "14342   52.1335\n",
       "14343   52.1671\n",
       "14344   52.6722"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_latest.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10041, 16) (70% of the samples in training set and 16 features)\n"
     ]
    }
   ],
   "source": [
    "#splitting the processed trace into training and test samples and time index sorting\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_latest, Y_latest, test_size=0.3, shuffle = False)\n",
    "print(X_train.shape,\"(70% of the samples in training set and 16 features)\")\n",
    "X_train = X_train.sort_index(axis = 0)\n",
    "X_test = X_test.sort_index(axis = 0)\n",
    "Y_train = Y_train.sort_index(axis = 0)\n",
    "Y_test = Y_test.sort_index(axis = 0)\n",
    "\n",
    "\n",
    "## T is the index 9285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1 : 1_i127_intr.s</th>\n",
       "      <th>feature2 : 5_campg.s</th>\n",
       "      <th>feature3 : 4_RxPacktes</th>\n",
       "      <th>feature4 : 4_TxPacktes</th>\n",
       "      <th>feature5 : 15_RxPacktes</th>\n",
       "      <th>feature6 : 15_TxBytes</th>\n",
       "      <th>feature7 : 17_TxPacktes</th>\n",
       "      <th>feature8 : 29_TxBytes</th>\n",
       "      <th>feature9 : 41_TxBytes</th>\n",
       "      <th>feature10 : 4_RxPacktes.1</th>\n",
       "      <th>feature11 : 4_TxPacktes.1</th>\n",
       "      <th>feature12 : 4_TxBytes.1</th>\n",
       "      <th>feature13 : 15_RxPacktes.1</th>\n",
       "      <th>feature14 : 17_TxPacktes.1</th>\n",
       "      <th>feature15 : 40_RxPacktes.1</th>\n",
       "      <th>feature16 : 41_TxBytes.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10036</th>\n",
       "      <td>1.083791</td>\n",
       "      <td>0.006552</td>\n",
       "      <td>0.890140</td>\n",
       "      <td>0.862522</td>\n",
       "      <td>0.860309</td>\n",
       "      <td>0.847371</td>\n",
       "      <td>0.848144</td>\n",
       "      <td>0.930262</td>\n",
       "      <td>0.941195</td>\n",
       "      <td>0.890140</td>\n",
       "      <td>0.862522</td>\n",
       "      <td>0.921023</td>\n",
       "      <td>0.860309</td>\n",
       "      <td>0.848144</td>\n",
       "      <td>0.876752</td>\n",
       "      <td>0.941195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10037</th>\n",
       "      <td>0.999547</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.870610</td>\n",
       "      <td>0.846657</td>\n",
       "      <td>0.887601</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.888474</td>\n",
       "      <td>0.898441</td>\n",
       "      <td>0.907049</td>\n",
       "      <td>0.870610</td>\n",
       "      <td>0.846657</td>\n",
       "      <td>0.905672</td>\n",
       "      <td>0.887601</td>\n",
       "      <td>0.888474</td>\n",
       "      <td>0.851011</td>\n",
       "      <td>0.907049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10038</th>\n",
       "      <td>0.975477</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.833584</td>\n",
       "      <td>0.861975</td>\n",
       "      <td>0.874158</td>\n",
       "      <td>0.873696</td>\n",
       "      <td>0.874623</td>\n",
       "      <td>0.900591</td>\n",
       "      <td>0.922680</td>\n",
       "      <td>0.833584</td>\n",
       "      <td>0.861975</td>\n",
       "      <td>0.919101</td>\n",
       "      <td>0.874158</td>\n",
       "      <td>0.874623</td>\n",
       "      <td>0.871276</td>\n",
       "      <td>0.922680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10039</th>\n",
       "      <td>1.107860</td>\n",
       "      <td>0.042908</td>\n",
       "      <td>0.879968</td>\n",
       "      <td>0.880576</td>\n",
       "      <td>0.786581</td>\n",
       "      <td>0.913781</td>\n",
       "      <td>0.781335</td>\n",
       "      <td>0.919159</td>\n",
       "      <td>0.935334</td>\n",
       "      <td>0.879968</td>\n",
       "      <td>0.880576</td>\n",
       "      <td>0.925998</td>\n",
       "      <td>0.786581</td>\n",
       "      <td>0.781335</td>\n",
       "      <td>0.867989</td>\n",
       "      <td>0.935334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10040</th>\n",
       "      <td>1.023616</td>\n",
       "      <td>0.006970</td>\n",
       "      <td>0.860438</td>\n",
       "      <td>0.870728</td>\n",
       "      <td>0.971512</td>\n",
       "      <td>0.911640</td>\n",
       "      <td>0.955690</td>\n",
       "      <td>0.914619</td>\n",
       "      <td>0.892267</td>\n",
       "      <td>0.860438</td>\n",
       "      <td>0.870728</td>\n",
       "      <td>0.884023</td>\n",
       "      <td>0.971512</td>\n",
       "      <td>0.955690</td>\n",
       "      <td>0.883325</td>\n",
       "      <td>0.892267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1 : 1_i127_intr.s  feature2 : 5_campg.s  feature3 : 4_RxPacktes  \\\n",
       "10036                  1.083791              0.006552                0.890140   \n",
       "10037                  0.999547              0.024800                0.870610   \n",
       "10038                  0.975477              0.008223                0.833584   \n",
       "10039                  1.107860              0.042908                0.879968   \n",
       "10040                  1.023616              0.006970                0.860438   \n",
       "\n",
       "       feature4 : 4_TxPacktes  feature5 : 15_RxPacktes  feature6 : 15_TxBytes  \\\n",
       "10036                0.862522                 0.860309               0.847371   \n",
       "10037                0.846657                 0.887601               0.845140   \n",
       "10038                0.861975                 0.874158               0.873696   \n",
       "10039                0.880576                 0.786581               0.913781   \n",
       "10040                0.870728                 0.971512               0.911640   \n",
       "\n",
       "       feature7 : 17_TxPacktes  feature8 : 29_TxBytes  feature9 : 41_TxBytes  \\\n",
       "10036                 0.848144               0.930262               0.941195   \n",
       "10037                 0.888474               0.898441               0.907049   \n",
       "10038                 0.874623               0.900591               0.922680   \n",
       "10039                 0.781335               0.919159               0.935334   \n",
       "10040                 0.955690               0.914619               0.892267   \n",
       "\n",
       "       feature10 : 4_RxPacktes.1  feature11 : 4_TxPacktes.1  \\\n",
       "10036                   0.890140                   0.862522   \n",
       "10037                   0.870610                   0.846657   \n",
       "10038                   0.833584                   0.861975   \n",
       "10039                   0.879968                   0.880576   \n",
       "10040                   0.860438                   0.870728   \n",
       "\n",
       "       feature12 : 4_TxBytes.1  feature13 : 15_RxPacktes.1  \\\n",
       "10036                 0.921023                    0.860309   \n",
       "10037                 0.905672                    0.887601   \n",
       "10038                 0.919101                    0.874158   \n",
       "10039                 0.925998                    0.786581   \n",
       "10040                 0.884023                    0.971512   \n",
       "\n",
       "       feature14 : 17_TxPacktes.1  feature15 : 40_RxPacktes.1  \\\n",
       "10036                    0.848144                    0.876752   \n",
       "10037                    0.888474                    0.851011   \n",
       "10038                    0.874623                    0.871276   \n",
       "10039                    0.781335                    0.867989   \n",
       "10040                    0.955690                    0.883325   \n",
       "\n",
       "       feature16 : 41_TxBytes.1  \n",
       "10036                  0.941195  \n",
       "10037                  0.907049  \n",
       "10038                  0.922680  \n",
       "10039                  0.935334  \n",
       "10040                  0.892267  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1 : 1_i127_intr.s</th>\n",
       "      <th>feature2 : 5_campg.s</th>\n",
       "      <th>feature3 : 4_RxPacktes</th>\n",
       "      <th>feature4 : 4_TxPacktes</th>\n",
       "      <th>feature5 : 15_RxPacktes</th>\n",
       "      <th>feature6 : 15_TxBytes</th>\n",
       "      <th>feature7 : 17_TxPacktes</th>\n",
       "      <th>feature8 : 29_TxBytes</th>\n",
       "      <th>feature9 : 41_TxBytes</th>\n",
       "      <th>feature10 : 4_RxPacktes.1</th>\n",
       "      <th>feature11 : 4_TxPacktes.1</th>\n",
       "      <th>feature12 : 4_TxBytes.1</th>\n",
       "      <th>feature13 : 15_RxPacktes.1</th>\n",
       "      <th>feature14 : 17_TxPacktes.1</th>\n",
       "      <th>feature15 : 40_RxPacktes.1</th>\n",
       "      <th>feature16 : 41_TxBytes.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10041</th>\n",
       "      <td>1.119895</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>0.860438</td>\n",
       "      <td>0.841733</td>\n",
       "      <td>0.818353</td>\n",
       "      <td>0.933630</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.932650</td>\n",
       "      <td>0.933608</td>\n",
       "      <td>0.860438</td>\n",
       "      <td>0.841733</td>\n",
       "      <td>0.926773</td>\n",
       "      <td>0.818353</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.844439</td>\n",
       "      <td>0.933608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10042</th>\n",
       "      <td>1.180069</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.862879</td>\n",
       "      <td>0.872369</td>\n",
       "      <td>0.868456</td>\n",
       "      <td>0.920929</td>\n",
       "      <td>0.854662</td>\n",
       "      <td>0.912441</td>\n",
       "      <td>0.910287</td>\n",
       "      <td>0.862879</td>\n",
       "      <td>0.872369</td>\n",
       "      <td>0.907818</td>\n",
       "      <td>0.868456</td>\n",
       "      <td>0.854662</td>\n",
       "      <td>0.863060</td>\n",
       "      <td>0.910287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10043</th>\n",
       "      <td>1.119895</td>\n",
       "      <td>0.008502</td>\n",
       "      <td>0.856776</td>\n",
       "      <td>0.919418</td>\n",
       "      <td>0.912855</td>\n",
       "      <td>0.883211</td>\n",
       "      <td>0.912101</td>\n",
       "      <td>0.868774</td>\n",
       "      <td>0.888634</td>\n",
       "      <td>0.856776</td>\n",
       "      <td>0.919418</td>\n",
       "      <td>0.882061</td>\n",
       "      <td>0.912855</td>\n",
       "      <td>0.912101</td>\n",
       "      <td>0.914543</td>\n",
       "      <td>0.888634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10044</th>\n",
       "      <td>0.975477</td>\n",
       "      <td>0.007248</td>\n",
       "      <td>0.833177</td>\n",
       "      <td>0.894252</td>\n",
       "      <td>0.811021</td>\n",
       "      <td>0.950526</td>\n",
       "      <td>0.819628</td>\n",
       "      <td>0.957691</td>\n",
       "      <td>0.949711</td>\n",
       "      <td>0.833177</td>\n",
       "      <td>0.894252</td>\n",
       "      <td>0.947562</td>\n",
       "      <td>0.811021</td>\n",
       "      <td>0.819628</td>\n",
       "      <td>0.895921</td>\n",
       "      <td>0.949711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10045</th>\n",
       "      <td>0.951407</td>\n",
       "      <td>0.043326</td>\n",
       "      <td>0.877120</td>\n",
       "      <td>0.851581</td>\n",
       "      <td>0.870492</td>\n",
       "      <td>0.898870</td>\n",
       "      <td>0.868105</td>\n",
       "      <td>0.891108</td>\n",
       "      <td>0.922233</td>\n",
       "      <td>0.877120</td>\n",
       "      <td>0.851581</td>\n",
       "      <td>0.919461</td>\n",
       "      <td>0.870492</td>\n",
       "      <td>0.868105</td>\n",
       "      <td>0.853202</td>\n",
       "      <td>0.922233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1 : 1_i127_intr.s  feature2 : 5_campg.s  feature3 : 4_RxPacktes  \\\n",
       "10041                  1.119895              0.007527                0.860438   \n",
       "10042                  1.180069              0.006830                0.862879   \n",
       "10043                  1.119895              0.008502                0.856776   \n",
       "10044                  0.975477              0.007248                0.833177   \n",
       "10045                  0.951407              0.043326                0.877120   \n",
       "\n",
       "       feature4 : 4_TxPacktes  feature5 : 15_RxPacktes  feature6 : 15_TxBytes  \\\n",
       "10041                0.841733                 0.818353               0.933630   \n",
       "10042                0.872369                 0.868456               0.920929   \n",
       "10043                0.919418                 0.912855               0.883211   \n",
       "10044                0.894252                 0.811021               0.950526   \n",
       "10045                0.851581                 0.870492               0.898870   \n",
       "\n",
       "       feature7 : 17_TxPacktes  feature8 : 29_TxBytes  feature9 : 41_TxBytes  \\\n",
       "10041                 0.803333               0.932650               0.933608   \n",
       "10042                 0.854662               0.912441               0.910287   \n",
       "10043                 0.912101               0.868774               0.888634   \n",
       "10044                 0.819628               0.957691               0.949711   \n",
       "10045                 0.868105               0.891108               0.922233   \n",
       "\n",
       "       feature10 : 4_RxPacktes.1  feature11 : 4_TxPacktes.1  \\\n",
       "10041                   0.860438                   0.841733   \n",
       "10042                   0.862879                   0.872369   \n",
       "10043                   0.856776                   0.919418   \n",
       "10044                   0.833177                   0.894252   \n",
       "10045                   0.877120                   0.851581   \n",
       "\n",
       "       feature12 : 4_TxBytes.1  feature13 : 15_RxPacktes.1  \\\n",
       "10041                 0.926773                    0.818353   \n",
       "10042                 0.907818                    0.868456   \n",
       "10043                 0.882061                    0.912855   \n",
       "10044                 0.947562                    0.811021   \n",
       "10045                 0.919461                    0.870492   \n",
       "\n",
       "       feature14 : 17_TxPacktes.1  feature15 : 40_RxPacktes.1  \\\n",
       "10041                    0.803333                    0.844439   \n",
       "10042                    0.854662                    0.863060   \n",
       "10043                    0.912101                    0.914543   \n",
       "10044                    0.819628                    0.895921   \n",
       "10045                    0.868105                    0.853202   \n",
       "\n",
       "       feature16 : 41_TxBytes.1  \n",
       "10041                  0.933608  \n",
       "10042                  0.910287  \n",
       "10043                  0.888634  \n",
       "10044                  0.949711  \n",
       "10045                  0.922233  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReadsAvg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10041</th>\n",
       "      <td>59.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10042</th>\n",
       "      <td>59.7941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10043</th>\n",
       "      <td>59.6689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10044</th>\n",
       "      <td>58.4393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10045</th>\n",
       "      <td>57.2402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ReadsAvg\n",
       "10041   59.4228\n",
       "10042   59.7941\n",
       "10043   59.6689\n",
       "10044   58.4393\n",
       "10045   57.2402"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The goal of this task is to find a non-linear, LSTM model to forecast the target variable at specific\n",
    "   points in time. Similar to step 3 of Task I, we define a model on the sequence of inputs x with lag size\n",
    "   l which predicts the sequence of outputs with time horizon h. Similar to step 4, 5 and 6 of Task I we\n",
    "   create the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Similar to step 7 of Task I, for values of l = 0,...,10 and h = 0,...,10 train LSTM models. Evaluate\n",
    "   the models by computing the error (NMAE) on the test set. Display the results in a table with rows\n",
    "   representing the time horizon h = 0,...,10 and columns representing the lag l = 0,...,10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The size of the output for your neural network will be the same as the time steps in your LSTM which is equal to lag but the output should be the same as the horizon (and lag and horizon are independent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMAE function\n",
    "def nmae_get(y, y_hat):\n",
    "    y_av = np.mean(y)\n",
    "    y_sum = np.sum(np.abs(y - y_hat))\n",
    "    return y_sum/(len(y)*y_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMAE for h=0:::10\n",
    "def nmaes_array(df_test, df_pre, h):\n",
    "    nmaes = []\n",
    "    for i in range(0, h+1):\n",
    "        y_predict_i = df_pre[:, i]\n",
    "        y_test_o = df_test.iloc[:, i].to_numpy()\n",
    "        nmaes.append(nmae_get(y_test_o, y_predict_i))\n",
    "        \n",
    "    return nmaes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rnn_reshape(samples, time_step, features):\n",
    "    X = samples.to_numpy()\n",
    "    X = X.reshape(X.shape[0], time_step, features)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input,LSTM\n",
    "from keras.optimizers import Adam\n",
    "def LSTM_model(input_X, input_Y, l, h, n_features):\n",
    "    X = Rnn_reshape(input_X, l, n_features)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu',dropout=0.3,return_sequences=True, input_shape=(l, n_features)))\n",
    "    model.add(LSTM(50, activation='relu',dropout=0.2))\n",
    "    model.add(Dense(h))\n",
    "    opt = Adam(lr=0.01)\n",
    "    model.compile(optimizer=opt, loss='mae',metrics=['accuracy'])\n",
    "    model.fit(X, input_Y, epochs=50, verbose=1, batch_size=32,validation_split=0.2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning batch size l=1\n",
    "b_s = [32, 64, 128]\n",
    "def LSTM_tune_bs(input_X, input_Y,test_X,test_Y, l, h, n_features):\n",
    "    for i in range(len(b_s)):\n",
    "        X = Rnn_reshape(input_X, 1, n_features)\n",
    "    \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(80, activation='relu',dropout=0.3,return_sequences=True, input_shape=(l, n_features)))\n",
    "        model.add(LSTM(80, activation='relu',dropout=0.2))\n",
    "        model.add(Dense(h))\n",
    "        opt = Adam(lr=0.01)\n",
    "        model.compile(optimizer=opt, loss='mae',metrics=['accuracy'])\n",
    "        model.fit(X, input_Y, epochs=10, verbose=1, batch_size=b_s[i],validation_split=0.2)\n",
    "        if b_s[i]==32:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for batch size=\",b_s[i],\"is:\",nmaes_t)\n",
    "        if b_s[i]==64:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for batch size=\",b_s[i],\"is:\",nmaes_t)\n",
    "        if b_s[i]==128:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for batch size=\",b_s[i],\"is:\",nmaes_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning epochs l=1\n",
    "epochs = [10, 50, 100, 200, 500]\n",
    "def LSTM_tune_epochs(input_X, input_Y,test_X,test_Y, l, h, n_features):\n",
    "    for i in range(len(epochs)):\n",
    "        X = Rnn_reshape(input_X, 1, n_features)\n",
    "    \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(80, activation='relu',dropout=0.3,return_sequences=True, input_shape=(l, n_features)))\n",
    "        model.add(LSTM(80, activation='relu',dropout=0.2))\n",
    "        model.add(Dense(h))\n",
    "        opt = Adam(lr=0.01)\n",
    "        model.compile(optimizer=opt, loss='mae',metrics=['accuracy'])\n",
    "        model.fit(X, input_Y, epochs=epochs[i], verbose=1, batch_size=32,validation_split=0.2)\n",
    "        if epochs[i]==10:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for epochs=\",epochs[i],\"is:\",nmaes_t)\n",
    "        if epochs[i]==50:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for epochs=\",epochs[i],\"is:\",nmaes_t)\n",
    "        if epochs[i]==100:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for epochs=\",epochs[i],\"is:\",nmaes_t)\n",
    "        if epochs[i]==200:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for epochs=\",epochs[i],\"is:\",nmaes_t)\n",
    "        if epochs[i]==500:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for epochs=\",epochs[i],\"is:\",nmaes_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "#tuning nodes l=1\n",
    "units = [10, 50, 80, 128]\n",
    "def LSTM_tune_nodes(input_X, input_Y,test_X,test_Y, l, h, n_features):\n",
    "    for i in range(len(units)):\n",
    "        X = Rnn_reshape(input_X, 1, n_features)\n",
    "    \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units[i], activation='relu',dropout=0.3,return_sequences=True, input_shape=(l, n_features)))\n",
    "        model.add(LSTM(units[i], activation='relu',dropout=0.2))\n",
    "        model.add(Dense(h))\n",
    "        opt = Adam(lr=0.01)\n",
    "        model.compile(optimizer=opt, loss='mae',metrics=['accuracy'])\n",
    "        model.fit(X, input_Y, epochs= 50, verbose=1, batch_size=32,validation_split=0.2)\n",
    "        if units[i]==10:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for units=\",units[i],\"is:\",nmaes_t)\n",
    "        if units[i]==50:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for units=\",units[i],\"is:\",nmaes_t)\n",
    "        if units[i]==80:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for units=\",units[i],\"is:\",nmaes_t)\n",
    "        if units[i]==128:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for units=\",units[i],\"is:\",nmaes_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning learningrate l=1\n",
    "l_r = [1e-2, 1e-3, 1e-4]\n",
    "def LSTM_tune_learningrate(input_X, input_Y,test_X,test_Y, l, h, n_features):\n",
    "    for i in range(len(l_r[i])):\n",
    "        X = Rnn_reshape(input_X, 1, n_features)\n",
    "    \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, activation='relu',dropout=0.3,return_sequences=True, input_shape=(l, n_features)))\n",
    "        model.add(LSTM(50, activation='relu',dropout=0.2))\n",
    "        model.add(Dense(h))\n",
    "        opt = Adam(lr=l_r[i])\n",
    "        model.compile(optimizer=opt, loss='mae',metrics=['accuracy'])\n",
    "        model.fit(X, input_Y, epochs=50, verbose=1, batch_size=32,validation_split=0.2)\n",
    "        if l_r[i]==1e-2:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for learning rate=\",l_r[i],\"is:\",nmaes_t)\n",
    "        if l_r[i]==1e-3:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for learning rate=\",l_r[i],\"is:\",nmaes_t)\n",
    "        if l_r[i]==1e-4:\n",
    "            Rnn_test = Rnn_reshape(test_X, 1, 16)\n",
    "            Rnn_pred = model.predict(Rnn_test,verbose=0)\n",
    "            nmaes_t =  np.round(metrics.mean_absolute_error(Rnn_pred,test_Y)/np.mean(test_Y), decimals=6)\n",
    "            print(\"the NMAE for learning rate=\",l_r[i],\"is:\",nmaes_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(data_X,data_Y,h,l):\n",
    "    num_range = len(data_X)-1*l-1*h\n",
    "    X_output = np.zeros((num_range,l+1,16))\n",
    "    Y_output = np.zeros((num_range,h+1,2))\n",
    "    X = data_X[0:num_range]\n",
    "    Y = data_Y[l*1:l*1+num_range]['ReadsAvg']\n",
    "    target = ['ReadsAvg']\n",
    "    index = X.index\n",
    "    Y.index = index\n",
    "    feature = [col for col in data_X] \n",
    "    \n",
    "    for i in range(l):\n",
    "        if(i==0):\n",
    "            X = X\n",
    "        if(i>0):\n",
    "            X_add = data_X[(i+1)*1:(i+1)*1+num_range]\n",
    "            X_add.columns = [j+str(i+2) for j in feature] \n",
    "            X_add.index = index\n",
    "            X = pd.concat([X,X_add],axis=1)\n",
    "    \n",
    "    for i in range(h):\n",
    "        if(i==0):\n",
    "            Y = Y\n",
    "        if(i>0):\n",
    "            Y_add = data_Y[(l+i+1)*1:(l+i+1)*1+num_range]['ReadsAvg']\n",
    "            Y_add.columns = [j+str(i+2) for j in target] \n",
    "            Y_add.index = index\n",
    "            Y = pd.concat([Y,Y_add],axis=1)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8023 samples, validate on 2006 samples\n",
      "Epoch 1/10\n",
      "8023/8023 [==============================] - 31s 4ms/step - loss: 11.4896 - accuracy: 0.0877 - val_loss: 2.3635 - val_accuracy: 0.0892\n",
      "Epoch 2/10\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 3.0212 - accuracy: 0.0921 - val_loss: 1.6489 - val_accuracy: 0.0862\n",
      "Epoch 3/10\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.9695 - accuracy: 0.0939 - val_loss: 1.3915 - val_accuracy: 0.0912\n",
      "Epoch 4/10\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.7003 - accuracy: 0.0911 - val_loss: 1.2342 - val_accuracy: 0.0927\n",
      "Epoch 5/10\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.6155 - accuracy: 0.0905 - val_loss: 1.2238 - val_accuracy: 0.0977\n",
      "Epoch 6/10\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.6187 - accuracy: 0.0901 - val_loss: 1.3183 - val_accuracy: 0.0997\n",
      "Epoch 7/10\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.6112 - accuracy: 0.0876 - val_loss: 1.2053 - val_accuracy: 0.0982\n",
      "Epoch 8/10\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5844 - accuracy: 0.0950 - val_loss: 1.2618 - val_accuracy: 0.1017\n",
      "Epoch 9/10\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5983 - accuracy: 0.0927 - val_loss: 1.2498 - val_accuracy: 0.1062\n",
      "Epoch 10/10\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5836 - accuracy: 0.0901 - val_loss: 1.1994 - val_accuracy: 0.0857\n",
      "the NMAE for epochs= 10 is: ReadsAvg    0.028751\n",
      "ReadsAvg    0.028753\n",
      "ReadsAvg    0.028753\n",
      "ReadsAvg    0.028753\n",
      "ReadsAvg    0.028754\n",
      "ReadsAvg    0.028754\n",
      "ReadsAvg    0.028755\n",
      "ReadsAvg    0.028756\n",
      "ReadsAvg    0.028756\n",
      "ReadsAvg    0.028757\n",
      "ReadsAvg    0.028758\n",
      "dtype: float64\n",
      "Train on 8023 samples, validate on 2006 samples\n",
      "Epoch 1/50\n",
      "8023/8023 [==============================] - 31s 4ms/step - loss: 11.4807 - accuracy: 0.0864 - val_loss: 1.2830 - val_accuracy: 0.0927\n",
      "Epoch 2/50\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 3.1824 - accuracy: 0.0930 - val_loss: 2.3777 - val_accuracy: 0.0907\n",
      "Epoch 3/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 2.2410 - accuracy: 0.0921 - val_loss: 1.6250 - val_accuracy: 0.0897\n",
      "Epoch 4/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.7261 - accuracy: 0.0961 - val_loss: 1.2730 - val_accuracy: 0.0867\n",
      "Epoch 5/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.6357 - accuracy: 0.0975 - val_loss: 1.2318 - val_accuracy: 0.0867\n",
      "Epoch 6/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.6165 - accuracy: 0.0947 - val_loss: 1.2831 - val_accuracy: 0.0872\n",
      "Epoch 7/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5987 - accuracy: 0.0988 - val_loss: 1.2034 - val_accuracy: 0.1022\n",
      "Epoch 8/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5784 - accuracy: 0.0985 - val_loss: 1.2049 - val_accuracy: 0.0892\n",
      "Epoch 9/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5760 - accuracy: 0.0947 - val_loss: 1.2031 - val_accuracy: 0.1007\n",
      "Epoch 10/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5649 - accuracy: 0.0980 - val_loss: 1.2120 - val_accuracy: 0.0957\n",
      "Epoch 11/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5624 - accuracy: 0.0931 - val_loss: 1.2253 - val_accuracy: 0.1067\n",
      "Epoch 12/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5731 - accuracy: 0.0982 - val_loss: 1.4032 - val_accuracy: 0.0932\n",
      "Epoch 13/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5613 - accuracy: 0.0935 - val_loss: 1.2166 - val_accuracy: 0.0922\n",
      "Epoch 14/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5653 - accuracy: 0.1007 - val_loss: 1.2362 - val_accuracy: 0.0877\n",
      "Epoch 15/50\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5411 - accuracy: 0.0955 - val_loss: 1.1886 - val_accuracy: 0.0922\n",
      "Epoch 16/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5468 - accuracy: 0.0941 - val_loss: 1.1776 - val_accuracy: 0.0947\n",
      "Epoch 17/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5463 - accuracy: 0.0951 - val_loss: 1.2074 - val_accuracy: 0.0937\n",
      "Epoch 18/50\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5414 - accuracy: 0.0927 - val_loss: 1.2433 - val_accuracy: 0.0997\n",
      "Epoch 19/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5296 - accuracy: 0.0919 - val_loss: 1.1886 - val_accuracy: 0.0877\n",
      "Epoch 20/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5377 - accuracy: 0.0942 - val_loss: 1.2023 - val_accuracy: 0.0882\n",
      "Epoch 21/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5385 - accuracy: 0.0924 - val_loss: 1.2114 - val_accuracy: 0.0877\n",
      "Epoch 22/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5383 - accuracy: 0.0985 - val_loss: 1.2073 - val_accuracy: 0.0887\n",
      "Epoch 23/50\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 1.5258 - accuracy: 0.0957 - val_loss: 1.1776 - val_accuracy: 0.1097\n",
      "Epoch 24/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5290 - accuracy: 0.1006 - val_loss: 1.1966 - val_accuracy: 0.0852\n",
      "Epoch 25/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5331 - accuracy: 0.0940 - val_loss: 1.2427 - val_accuracy: 0.0872\n",
      "Epoch 26/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5356 - accuracy: 0.0909 - val_loss: 1.2421 - val_accuracy: 0.0862\n",
      "Epoch 27/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5304 - accuracy: 0.0946 - val_loss: 1.2301 - val_accuracy: 0.0972\n",
      "Epoch 28/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5273 - accuracy: 0.0920 - val_loss: 1.2003 - val_accuracy: 0.0897\n",
      "Epoch 29/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5282 - accuracy: 0.0977 - val_loss: 1.2231 - val_accuracy: 0.0902\n",
      "Epoch 30/50\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5329 - accuracy: 0.0960 - val_loss: 1.2670 - val_accuracy: 0.0862\n",
      "Epoch 31/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5287 - accuracy: 0.0950 - val_loss: 1.1932 - val_accuracy: 0.0927\n",
      "Epoch 32/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5303 - accuracy: 0.0911 - val_loss: 1.1907 - val_accuracy: 0.1012\n",
      "Epoch 33/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5406 - accuracy: 0.0950 - val_loss: 1.3152 - val_accuracy: 0.0952\n",
      "Epoch 34/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5293 - accuracy: 0.0967 - val_loss: 1.1839 - val_accuracy: 0.0932\n",
      "Epoch 35/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5414 - accuracy: 0.0952 - val_loss: 1.2433 - val_accuracy: 0.0932\n",
      "Epoch 36/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5218 - accuracy: 0.0977 - val_loss: 1.2077 - val_accuracy: 0.1077\n",
      "Epoch 37/50\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.5393 - accuracy: 0.0911 - val_loss: 1.2768 - val_accuracy: 0.1022\n",
      "Epoch 38/50\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5261 - accuracy: 0.0926 - val_loss: 1.1963 - val_accuracy: 0.0997\n",
      "Epoch 39/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5317 - accuracy: 0.0907 - val_loss: 1.2590 - val_accuracy: 0.0902\n",
      "Epoch 40/50\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5272 - accuracy: 0.0890 - val_loss: 1.1776 - val_accuracy: 0.0952\n",
      "Epoch 41/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5290 - accuracy: 0.0995 - val_loss: 1.1994 - val_accuracy: 0.0947\n",
      "Epoch 42/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5324 - accuracy: 0.0932 - val_loss: 1.1911 - val_accuracy: 0.0867\n",
      "Epoch 43/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5339 - accuracy: 0.0901 - val_loss: 1.2729 - val_accuracy: 0.0922\n",
      "Epoch 44/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5295 - accuracy: 0.0967 - val_loss: 1.2335 - val_accuracy: 0.0922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5247 - accuracy: 0.0961 - val_loss: 1.2031 - val_accuracy: 0.1037\n",
      "Epoch 46/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5262 - accuracy: 0.0991 - val_loss: 1.1919 - val_accuracy: 0.1037\n",
      "Epoch 47/50\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5334 - accuracy: 0.0937 - val_loss: 1.1701 - val_accuracy: 0.1052\n",
      "Epoch 48/50\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5326 - accuracy: 0.0861 - val_loss: 1.1819 - val_accuracy: 0.1007\n",
      "Epoch 49/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5243 - accuracy: 0.0899 - val_loss: 1.1976 - val_accuracy: 0.0932\n",
      "Epoch 50/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5328 - accuracy: 0.0894 - val_loss: 1.1932 - val_accuracy: 0.0932\n",
      "the NMAE for epochs= 50 is: ReadsAvg    0.028488\n",
      "ReadsAvg    0.028489\n",
      "ReadsAvg    0.028490\n",
      "ReadsAvg    0.028490\n",
      "ReadsAvg    0.028491\n",
      "ReadsAvg    0.028491\n",
      "ReadsAvg    0.028491\n",
      "ReadsAvg    0.028492\n",
      "ReadsAvg    0.028493\n",
      "ReadsAvg    0.028493\n",
      "ReadsAvg    0.028495\n",
      "dtype: float64\n",
      "Train on 8023 samples, validate on 2006 samples\n",
      "Epoch 1/100\n",
      "8023/8023 [==============================] - 32s 4ms/step - loss: 11.4530 - accuracy: 0.0976 - val_loss: 2.4202 - val_accuracy: 0.0902\n",
      "Epoch 2/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 3.0205 - accuracy: 0.0925 - val_loss: 1.3815 - val_accuracy: 0.0927\n",
      "Epoch 3/100\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 2.0794 - accuracy: 0.0966 - val_loss: 1.3953 - val_accuracy: 0.0877\n",
      "Epoch 4/100\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.7020 - accuracy: 0.0909 - val_loss: 1.2770 - val_accuracy: 0.0917\n",
      "Epoch 5/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.6227 - accuracy: 0.0935 - val_loss: 1.3174 - val_accuracy: 0.0937\n",
      "Epoch 6/100\n",
      "8023/8023 [==============================] - 13s 2ms/step - loss: 1.6448 - accuracy: 0.0881 - val_loss: 1.2297 - val_accuracy: 0.0937\n",
      "Epoch 7/100\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.6065 - accuracy: 0.0955 - val_loss: 1.2426 - val_accuracy: 0.1002\n",
      "Epoch 8/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5770 - accuracy: 0.0942 - val_loss: 1.1709 - val_accuracy: 0.0927\n",
      "Epoch 9/100\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.6191 - accuracy: 0.0927 - val_loss: 1.2523 - val_accuracy: 0.0907\n",
      "Epoch 10/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5970 - accuracy: 0.0925 - val_loss: 1.2019 - val_accuracy: 0.0967\n",
      "Epoch 11/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5864 - accuracy: 0.0899 - val_loss: 1.3025 - val_accuracy: 0.0922\n",
      "Epoch 12/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5753 - accuracy: 0.1018 - val_loss: 1.2291 - val_accuracy: 0.1132\n",
      "Epoch 13/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5785 - accuracy: 0.0932 - val_loss: 1.2098 - val_accuracy: 0.1002\n",
      "Epoch 14/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5705 - accuracy: 0.0997 - val_loss: 1.2383 - val_accuracy: 0.0887\n",
      "Epoch 15/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5644 - accuracy: 0.0952 - val_loss: 1.2949 - val_accuracy: 0.1032\n",
      "Epoch 16/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5928 - accuracy: 0.0949 - val_loss: 1.2043 - val_accuracy: 0.0927\n",
      "Epoch 17/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5645 - accuracy: 0.0925 - val_loss: 1.2065 - val_accuracy: 0.0867\n",
      "Epoch 18/100\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5509 - accuracy: 0.1011 - val_loss: 1.2443 - val_accuracy: 0.1037\n",
      "Epoch 19/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5499 - accuracy: 0.0874 - val_loss: 1.2292 - val_accuracy: 0.0972\n",
      "Epoch 20/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5433 - accuracy: 0.0936 - val_loss: 1.2062 - val_accuracy: 0.0952\n",
      "Epoch 21/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5444 - accuracy: 0.0936 - val_loss: 1.2197 - val_accuracy: 0.0862\n",
      "Epoch 22/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5393 - accuracy: 0.0850 - val_loss: 1.1859 - val_accuracy: 0.0962\n",
      "Epoch 23/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5426 - accuracy: 0.0891 - val_loss: 1.1896 - val_accuracy: 0.1097\n",
      "Epoch 24/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5297 - accuracy: 0.0971 - val_loss: 1.1832 - val_accuracy: 0.0932\n",
      "Epoch 25/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5456 - accuracy: 0.0939 - val_loss: 1.2180 - val_accuracy: 0.1042\n",
      "Epoch 26/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5281 - accuracy: 0.0987 - val_loss: 1.1822 - val_accuracy: 0.0847\n",
      "Epoch 27/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5480 - accuracy: 0.0902 - val_loss: 1.1958 - val_accuracy: 0.0897\n",
      "Epoch 28/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5287 - accuracy: 0.0951 - val_loss: 1.2267 - val_accuracy: 0.0877\n",
      "Epoch 29/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5235 - accuracy: 0.0892 - val_loss: 1.1841 - val_accuracy: 0.1007\n",
      "Epoch 30/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5354 - accuracy: 0.0900 - val_loss: 1.1838 - val_accuracy: 0.0897\n",
      "Epoch 31/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5339 - accuracy: 0.0945 - val_loss: 1.2078 - val_accuracy: 0.1047\n",
      "Epoch 32/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5248 - accuracy: 0.0944 - val_loss: 1.1873 - val_accuracy: 0.0877\n",
      "Epoch 33/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5340 - accuracy: 0.0957 - val_loss: 1.2391 - val_accuracy: 0.0922\n",
      "Epoch 34/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5377 - accuracy: 0.1003 - val_loss: 1.2196 - val_accuracy: 0.1002\n",
      "Epoch 35/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5333 - accuracy: 0.0986 - val_loss: 1.1913 - val_accuracy: 0.0912\n",
      "Epoch 36/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5348 - accuracy: 0.0890 - val_loss: 1.1828 - val_accuracy: 0.0947\n",
      "Epoch 37/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5321 - accuracy: 0.0939 - val_loss: 1.1933 - val_accuracy: 0.1092\n",
      "Epoch 38/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5276 - accuracy: 0.0940 - val_loss: 1.2679 - val_accuracy: 0.0982\n",
      "Epoch 39/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5280 - accuracy: 0.0915 - val_loss: 1.2088 - val_accuracy: 0.1052\n",
      "Epoch 40/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5285 - accuracy: 0.0982 - val_loss: 1.1930 - val_accuracy: 0.0937\n",
      "Epoch 41/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5319 - accuracy: 0.0950 - val_loss: 1.2183 - val_accuracy: 0.0937\n",
      "Epoch 42/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5317 - accuracy: 0.0912 - val_loss: 1.3049 - val_accuracy: 0.0942\n",
      "Epoch 43/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5265 - accuracy: 0.0935 - val_loss: 1.2074 - val_accuracy: 0.0887\n",
      "Epoch 44/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5242 - accuracy: 0.0947 - val_loss: 1.1949 - val_accuracy: 0.1042\n",
      "Epoch 45/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5176 - accuracy: 0.0992 - val_loss: 1.2143 - val_accuracy: 0.0852\n",
      "Epoch 46/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5339 - accuracy: 0.0921 - val_loss: 1.2374 - val_accuracy: 0.0892\n",
      "Epoch 47/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5332 - accuracy: 0.0919 - val_loss: 1.1987 - val_accuracy: 0.0862\n",
      "Epoch 48/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5218 - accuracy: 0.0985 - val_loss: 1.1871 - val_accuracy: 0.0882\n",
      "Epoch 49/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5209 - accuracy: 0.0970 - val_loss: 1.1874 - val_accuracy: 0.0882\n",
      "Epoch 50/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5234 - accuracy: 0.0944 - val_loss: 1.2059 - val_accuracy: 0.0867\n",
      "Epoch 51/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5256 - accuracy: 0.1052 - val_loss: 1.1753 - val_accuracy: 0.0932\n",
      "Epoch 52/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5267 - accuracy: 0.0875 - val_loss: 1.2158 - val_accuracy: 0.0922\n",
      "Epoch 53/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5327 - accuracy: 0.0927 - val_loss: 1.2536 - val_accuracy: 0.0907\n",
      "Epoch 54/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5271 - accuracy: 0.0941 - val_loss: 1.2956 - val_accuracy: 0.0892\n",
      "Epoch 55/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5229 - accuracy: 0.0914 - val_loss: 1.1894 - val_accuracy: 0.0997\n",
      "Epoch 56/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5264 - accuracy: 0.0955 - val_loss: 1.1884 - val_accuracy: 0.0902\n",
      "Epoch 57/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5267 - accuracy: 0.0892 - val_loss: 1.2117 - val_accuracy: 0.0927\n",
      "Epoch 58/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5257 - accuracy: 0.0958 - val_loss: 1.1768 - val_accuracy: 0.0992\n",
      "Epoch 59/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5160 - accuracy: 0.0926 - val_loss: 1.1820 - val_accuracy: 0.0872\n",
      "Epoch 60/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5247 - accuracy: 0.0904 - val_loss: 1.2005 - val_accuracy: 0.0882\n",
      "Epoch 61/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5146 - accuracy: 0.0990 - val_loss: 1.1988 - val_accuracy: 0.0922\n",
      "Epoch 62/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5204 - accuracy: 0.0911 - val_loss: 1.2404 - val_accuracy: 0.0917\n",
      "Epoch 63/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5265 - accuracy: 0.0983 - val_loss: 1.2111 - val_accuracy: 0.0897\n",
      "Epoch 64/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5408 - accuracy: 0.0887 - val_loss: 1.2280 - val_accuracy: 0.0942\n",
      "Epoch 65/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5213 - accuracy: 0.0925 - val_loss: 1.2273 - val_accuracy: 0.0862\n",
      "Epoch 66/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5165 - accuracy: 0.0920 - val_loss: 1.1855 - val_accuracy: 0.0857\n",
      "Epoch 67/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5246 - accuracy: 0.0941 - val_loss: 1.2485 - val_accuracy: 0.0877\n",
      "Epoch 68/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5181 - accuracy: 0.0942 - val_loss: 1.2070 - val_accuracy: 0.0852\n",
      "Epoch 69/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5232 - accuracy: 0.0976 - val_loss: 1.2035 - val_accuracy: 0.0857\n",
      "Epoch 70/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5157 - accuracy: 0.0927 - val_loss: 1.1870 - val_accuracy: 0.0907\n",
      "Epoch 71/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5159 - accuracy: 0.0901 - val_loss: 1.2295 - val_accuracy: 0.0852\n",
      "Epoch 72/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5189 - accuracy: 0.0877 - val_loss: 1.2026 - val_accuracy: 0.1037\n",
      "Epoch 73/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5178 - accuracy: 0.0891 - val_loss: 1.2352 - val_accuracy: 0.0852\n",
      "Epoch 74/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5223 - accuracy: 0.0922 - val_loss: 1.2093 - val_accuracy: 0.0997\n",
      "Epoch 75/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5154 - accuracy: 0.0910 - val_loss: 1.1917 - val_accuracy: 0.0917\n",
      "Epoch 76/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5176 - accuracy: 0.0946 - val_loss: 1.1790 - val_accuracy: 0.0897\n",
      "Epoch 77/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5186 - accuracy: 0.0937 - val_loss: 1.1992 - val_accuracy: 0.0847\n",
      "Epoch 78/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5094 - accuracy: 0.0988 - val_loss: 1.2020 - val_accuracy: 0.0902\n",
      "Epoch 79/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5204 - accuracy: 0.0886 - val_loss: 1.2097 - val_accuracy: 0.0997\n",
      "Epoch 80/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5180 - accuracy: 0.0882 - val_loss: 1.2902 - val_accuracy: 0.0932\n",
      "Epoch 81/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5189 - accuracy: 0.0914 - val_loss: 1.1978 - val_accuracy: 0.0897\n",
      "Epoch 82/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5128 - accuracy: 0.0906 - val_loss: 1.1860 - val_accuracy: 0.0842\n",
      "Epoch 83/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5144 - accuracy: 0.0956 - val_loss: 1.2794 - val_accuracy: 0.0957\n",
      "Epoch 84/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5255 - accuracy: 0.1020 - val_loss: 1.3413 - val_accuracy: 0.0837\n",
      "Epoch 85/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5270 - accuracy: 0.0992 - val_loss: 1.2641 - val_accuracy: 0.1087\n",
      "Epoch 86/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5257 - accuracy: 0.0934 - val_loss: 1.2448 - val_accuracy: 0.0852\n",
      "Epoch 87/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5128 - accuracy: 0.0954 - val_loss: 1.1991 - val_accuracy: 0.0867\n",
      "Epoch 88/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5115 - accuracy: 0.0887 - val_loss: 1.1899 - val_accuracy: 0.0897\n",
      "Epoch 89/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5178 - accuracy: 0.0925 - val_loss: 1.2462 - val_accuracy: 0.0877\n",
      "Epoch 90/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5166 - accuracy: 0.0931 - val_loss: 1.2316 - val_accuracy: 0.1007\n",
      "Epoch 91/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5129 - accuracy: 0.0910 - val_loss: 1.2041 - val_accuracy: 0.0907\n",
      "Epoch 92/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5094 - accuracy: 0.0971 - val_loss: 1.2512 - val_accuracy: 0.0917\n",
      "Epoch 93/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5060 - accuracy: 0.0968 - val_loss: 1.2320 - val_accuracy: 0.0922\n",
      "Epoch 94/100\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5103 - accuracy: 0.0987 - val_loss: 1.1784 - val_accuracy: 0.0902\n",
      "Epoch 95/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5080 - accuracy: 0.0970 - val_loss: 1.1981 - val_accuracy: 0.1062\n",
      "Epoch 96/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5179 - accuracy: 0.0955 - val_loss: 1.2209 - val_accuracy: 0.1057\n",
      "Epoch 97/100\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5214 - accuracy: 0.0976 - val_loss: 1.2157 - val_accuracy: 0.0857\n",
      "Epoch 98/100\n",
      "8023/8023 [==============================] - 8s 992us/step - loss: 1.5049 - accuracy: 0.0971 - val_loss: 1.2539 - val_accuracy: 0.0833\n",
      "Epoch 99/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5040 - accuracy: 0.0899 - val_loss: 1.2899 - val_accuracy: 0.0877\n",
      "Epoch 100/100\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5194 - accuracy: 0.0902 - val_loss: 1.2108 - val_accuracy: 0.0907\n",
      "the NMAE for epochs= 100 is: ReadsAvg    0.028755\n",
      "ReadsAvg    0.028757\n",
      "ReadsAvg    0.028757\n",
      "ReadsAvg    0.028757\n",
      "ReadsAvg    0.028758\n",
      "ReadsAvg    0.028758\n",
      "ReadsAvg    0.028759\n",
      "ReadsAvg    0.028760\n",
      "ReadsAvg    0.028760\n",
      "ReadsAvg    0.028761\n",
      "ReadsAvg    0.028762\n",
      "dtype: float64\n",
      "Train on 8023 samples, validate on 2006 samples\n",
      "Epoch 1/200\n",
      "8023/8023 [==============================] - 32s 4ms/step - loss: 11.3731 - accuracy: 0.0962 - val_loss: 2.3843 - val_accuracy: 0.0877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 3.0637 - accuracy: 0.0951 - val_loss: 1.5814 - val_accuracy: 0.0927\n",
      "Epoch 3/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 2.0406 - accuracy: 0.0946 - val_loss: 1.3055 - val_accuracy: 0.1052\n",
      "Epoch 4/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.6920 - accuracy: 0.0921 - val_loss: 1.2205 - val_accuracy: 0.0907\n",
      "Epoch 5/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.6138 - accuracy: 0.0936 - val_loss: 1.3283 - val_accuracy: 0.0887\n",
      "Epoch 6/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.6093 - accuracy: 0.0992 - val_loss: 1.2274 - val_accuracy: 0.1052\n",
      "Epoch 7/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5902 - accuracy: 0.0917 - val_loss: 1.3390 - val_accuracy: 0.0902\n",
      "Epoch 8/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.6014 - accuracy: 0.0935 - val_loss: 1.1966 - val_accuracy: 0.0947\n",
      "Epoch 9/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5734 - accuracy: 0.0955 - val_loss: 1.2613 - val_accuracy: 0.0852\n",
      "Epoch 10/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5912 - accuracy: 0.0939 - val_loss: 1.3464 - val_accuracy: 0.0927\n",
      "Epoch 11/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5642 - accuracy: 0.0972 - val_loss: 1.2217 - val_accuracy: 0.0902\n",
      "Epoch 12/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5785 - accuracy: 0.0920 - val_loss: 1.2760 - val_accuracy: 0.0897\n",
      "Epoch 13/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5702 - accuracy: 0.0983 - val_loss: 1.2323 - val_accuracy: 0.1042\n",
      "Epoch 14/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5713 - accuracy: 0.0971 - val_loss: 1.2003 - val_accuracy: 0.0892\n",
      "Epoch 15/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5717 - accuracy: 0.0922 - val_loss: 1.1767 - val_accuracy: 0.1017\n",
      "Epoch 16/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5640 - accuracy: 0.0919 - val_loss: 1.2390 - val_accuracy: 0.0837\n",
      "Epoch 17/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5600 - accuracy: 0.0936 - val_loss: 1.3255 - val_accuracy: 0.0897\n",
      "Epoch 18/200\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 1.5690 - accuracy: 0.0952 - val_loss: 1.2300 - val_accuracy: 0.0987\n",
      "Epoch 19/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5557 - accuracy: 0.0965 - val_loss: 1.2365 - val_accuracy: 0.0937\n",
      "Epoch 20/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5453 - accuracy: 0.0945 - val_loss: 1.1971 - val_accuracy: 0.0872\n",
      "Epoch 21/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5510 - accuracy: 0.0921 - val_loss: 1.2088 - val_accuracy: 0.0997\n",
      "Epoch 22/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5437 - accuracy: 0.0920 - val_loss: 1.2172 - val_accuracy: 0.0997\n",
      "Epoch 23/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5472 - accuracy: 0.0956 - val_loss: 1.2383 - val_accuracy: 0.0912\n",
      "Epoch 24/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5498 - accuracy: 0.0851 - val_loss: 1.1973 - val_accuracy: 0.0982\n",
      "Epoch 25/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5460 - accuracy: 0.0912 - val_loss: 1.2211 - val_accuracy: 0.0852\n",
      "Epoch 26/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5431 - accuracy: 0.0940 - val_loss: 1.2116 - val_accuracy: 0.0937\n",
      "Epoch 27/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5547 - accuracy: 0.0967 - val_loss: 1.2213 - val_accuracy: 0.1007\n",
      "Epoch 28/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5608 - accuracy: 0.0921 - val_loss: 1.1783 - val_accuracy: 0.0877\n",
      "Epoch 29/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5445 - accuracy: 0.0961 - val_loss: 1.2050 - val_accuracy: 0.1077\n",
      "Epoch 30/200\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.5463 - accuracy: 0.0905 - val_loss: 1.2045 - val_accuracy: 0.0857\n",
      "Epoch 31/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5454 - accuracy: 0.0996 - val_loss: 1.2674 - val_accuracy: 0.0932\n",
      "Epoch 32/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5476 - accuracy: 0.0916 - val_loss: 1.1989 - val_accuracy: 0.0842\n",
      "Epoch 33/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5434 - accuracy: 0.0945 - val_loss: 1.2236 - val_accuracy: 0.0872\n",
      "Epoch 34/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5395 - accuracy: 0.0951 - val_loss: 1.2044 - val_accuracy: 0.0857\n",
      "Epoch 35/200\n",
      "8023/8023 [==============================] - 8s 997us/step - loss: 1.5452 - accuracy: 0.0967 - val_loss: 1.1849 - val_accuracy: 0.0972\n",
      "Epoch 36/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5384 - accuracy: 0.0983 - val_loss: 1.2008 - val_accuracy: 0.0892\n",
      "Epoch 37/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5455 - accuracy: 0.0924 - val_loss: 1.2099 - val_accuracy: 0.0927\n",
      "Epoch 38/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5460 - accuracy: 0.0956 - val_loss: 1.1822 - val_accuracy: 0.0852\n",
      "Epoch 39/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5431 - accuracy: 0.0968 - val_loss: 1.2131 - val_accuracy: 0.0897\n",
      "Epoch 40/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5406 - accuracy: 0.0885 - val_loss: 1.1773 - val_accuracy: 0.0997\n",
      "Epoch 41/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5388 - accuracy: 0.0990 - val_loss: 1.1969 - val_accuracy: 0.0997\n",
      "Epoch 42/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5347 - accuracy: 0.0952 - val_loss: 1.2418 - val_accuracy: 0.0892\n",
      "Epoch 43/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5428 - accuracy: 0.0982 - val_loss: 1.1783 - val_accuracy: 0.1007\n",
      "Epoch 44/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5338 - accuracy: 0.0937 - val_loss: 1.2051 - val_accuracy: 0.0907\n",
      "Epoch 45/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5352 - accuracy: 0.0966 - val_loss: 1.2179 - val_accuracy: 0.1037\n",
      "Epoch 46/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5350 - accuracy: 0.0935 - val_loss: 1.1976 - val_accuracy: 0.0887\n",
      "Epoch 47/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5350 - accuracy: 0.0946 - val_loss: 1.1791 - val_accuracy: 0.1052\n",
      "Epoch 48/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5398 - accuracy: 0.0924 - val_loss: 1.1809 - val_accuracy: 0.0932\n",
      "Epoch 49/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5277 - accuracy: 0.0963 - val_loss: 1.1673 - val_accuracy: 0.0977\n",
      "Epoch 50/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5268 - accuracy: 0.0926 - val_loss: 1.1794 - val_accuracy: 0.0992\n",
      "Epoch 51/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5214 - accuracy: 0.0975 - val_loss: 1.2069 - val_accuracy: 0.0902\n",
      "Epoch 52/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5331 - accuracy: 0.0957 - val_loss: 1.2037 - val_accuracy: 0.0837\n",
      "Epoch 53/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5350 - accuracy: 0.0940 - val_loss: 1.1882 - val_accuracy: 0.0897\n",
      "Epoch 54/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5344 - accuracy: 0.0949 - val_loss: 1.2605 - val_accuracy: 0.0922\n",
      "Epoch 55/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5264 - accuracy: 0.0983 - val_loss: 1.1881 - val_accuracy: 0.0962\n",
      "Epoch 56/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5246 - accuracy: 0.0936 - val_loss: 1.1933 - val_accuracy: 0.0872\n",
      "Epoch 57/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5414 - accuracy: 0.0871 - val_loss: 1.1896 - val_accuracy: 0.0972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5386 - accuracy: 0.0992 - val_loss: 1.2023 - val_accuracy: 0.1012\n",
      "Epoch 59/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5298 - accuracy: 0.0949 - val_loss: 1.2445 - val_accuracy: 0.0887\n",
      "Epoch 60/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5311 - accuracy: 0.0945 - val_loss: 1.2360 - val_accuracy: 0.0942\n",
      "Epoch 61/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5282 - accuracy: 0.1018 - val_loss: 1.2365 - val_accuracy: 0.0852\n",
      "Epoch 62/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5313 - accuracy: 0.0958 - val_loss: 1.2015 - val_accuracy: 0.0897\n",
      "Epoch 63/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5347 - accuracy: 0.0925 - val_loss: 1.2182 - val_accuracy: 0.0902\n",
      "Epoch 64/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5317 - accuracy: 0.0929 - val_loss: 1.1834 - val_accuracy: 0.0937\n",
      "Epoch 65/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5285 - accuracy: 0.0972 - val_loss: 1.2112 - val_accuracy: 0.0997\n",
      "Epoch 66/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5299 - accuracy: 0.0924 - val_loss: 1.2157 - val_accuracy: 0.0932\n",
      "Epoch 67/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5182 - accuracy: 0.0973 - val_loss: 1.2183 - val_accuracy: 0.0852\n",
      "Epoch 68/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5289 - accuracy: 0.0992 - val_loss: 1.1684 - val_accuracy: 0.0862\n",
      "Epoch 69/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5222 - accuracy: 0.0958 - val_loss: 1.1808 - val_accuracy: 0.0932\n",
      "Epoch 70/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5238 - accuracy: 0.1005 - val_loss: 1.2287 - val_accuracy: 0.0932\n",
      "Epoch 71/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5289 - accuracy: 0.0956 - val_loss: 1.2236 - val_accuracy: 0.0922\n",
      "Epoch 72/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5228 - accuracy: 0.0906 - val_loss: 1.1804 - val_accuracy: 0.0967\n",
      "Epoch 73/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5220 - accuracy: 0.0957 - val_loss: 1.2522 - val_accuracy: 0.0927\n",
      "Epoch 74/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5333 - accuracy: 0.0921 - val_loss: 1.2141 - val_accuracy: 0.0932\n",
      "Epoch 75/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5234 - accuracy: 0.0957 - val_loss: 1.2025 - val_accuracy: 0.1097\n",
      "Epoch 76/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5326 - accuracy: 0.1010 - val_loss: 1.1991 - val_accuracy: 0.1052\n",
      "Epoch 77/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5193 - accuracy: 0.0947 - val_loss: 1.2400 - val_accuracy: 0.0927\n",
      "Epoch 78/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5274 - accuracy: 0.0975 - val_loss: 1.2019 - val_accuracy: 0.0962\n",
      "Epoch 79/200\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.5148 - accuracy: 0.0955 - val_loss: 1.1781 - val_accuracy: 0.0942\n",
      "Epoch 80/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5140 - accuracy: 0.0998 - val_loss: 1.1784 - val_accuracy: 0.1102\n",
      "Epoch 81/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5191 - accuracy: 0.0982 - val_loss: 1.2005 - val_accuracy: 0.0922\n",
      "Epoch 82/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5234 - accuracy: 0.0915 - val_loss: 1.2057 - val_accuracy: 0.0962\n",
      "Epoch 83/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5256 - accuracy: 0.0982 - val_loss: 1.1780 - val_accuracy: 0.0972\n",
      "Epoch 84/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5255 - accuracy: 0.0975 - val_loss: 1.2331 - val_accuracy: 0.1052\n",
      "Epoch 85/200\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 1.5244 - accuracy: 0.0977 - val_loss: 1.2487 - val_accuracy: 0.0857\n",
      "Epoch 86/200\n",
      "8023/8023 [==============================] - 8s 982us/step - loss: 1.5149 - accuracy: 0.0978 - val_loss: 1.1976 - val_accuracy: 0.0887\n",
      "Epoch 87/200\n",
      "8023/8023 [==============================] - 8s 979us/step - loss: 1.5158 - accuracy: 0.0944 - val_loss: 1.2745 - val_accuracy: 0.0862\n",
      "Epoch 88/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5170 - accuracy: 0.0965 - val_loss: 1.1834 - val_accuracy: 0.0872\n",
      "Epoch 89/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5185 - accuracy: 0.0965 - val_loss: 1.2182 - val_accuracy: 0.1097\n",
      "Epoch 90/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5193 - accuracy: 0.0934 - val_loss: 1.2227 - val_accuracy: 0.0892\n",
      "Epoch 91/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5155 - accuracy: 0.0935 - val_loss: 1.1796 - val_accuracy: 0.0887\n",
      "Epoch 92/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5140 - accuracy: 0.0949 - val_loss: 1.1850 - val_accuracy: 0.1042\n",
      "Epoch 93/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5141 - accuracy: 0.0941 - val_loss: 1.1962 - val_accuracy: 0.0997\n",
      "Epoch 94/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5217 - accuracy: 0.0960 - val_loss: 1.2185 - val_accuracy: 0.0867\n",
      "Epoch 95/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5216 - accuracy: 0.0925 - val_loss: 1.1853 - val_accuracy: 0.0942\n",
      "Epoch 96/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5182 - accuracy: 0.0971 - val_loss: 1.2015 - val_accuracy: 0.0862\n",
      "Epoch 97/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5161 - accuracy: 0.0966 - val_loss: 1.2076 - val_accuracy: 0.0867\n",
      "Epoch 98/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5174 - accuracy: 0.0954 - val_loss: 1.2741 - val_accuracy: 0.0927\n",
      "Epoch 99/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5245 - accuracy: 0.0932 - val_loss: 1.2053 - val_accuracy: 0.0932\n",
      "Epoch 100/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5192 - accuracy: 0.0960 - val_loss: 1.2303 - val_accuracy: 0.1097\n",
      "Epoch 101/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5232 - accuracy: 0.0944 - val_loss: 1.2169 - val_accuracy: 0.0937\n",
      "Epoch 102/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5137 - accuracy: 0.0925 - val_loss: 1.2228 - val_accuracy: 0.0997\n",
      "Epoch 103/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5184 - accuracy: 0.0963 - val_loss: 1.1908 - val_accuracy: 0.1047\n",
      "Epoch 104/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5141 - accuracy: 0.0951 - val_loss: 1.2077 - val_accuracy: 0.0892\n",
      "Epoch 105/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5208 - accuracy: 0.0967 - val_loss: 1.2755 - val_accuracy: 0.0927\n",
      "Epoch 106/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5193 - accuracy: 0.0932 - val_loss: 1.2034 - val_accuracy: 0.0917\n",
      "Epoch 107/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5123 - accuracy: 0.0961 - val_loss: 1.2031 - val_accuracy: 0.0942\n",
      "Epoch 108/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5110 - accuracy: 0.0942 - val_loss: 1.1863 - val_accuracy: 0.0952\n",
      "Epoch 109/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5131 - accuracy: 0.0966 - val_loss: 1.2111 - val_accuracy: 0.0837\n",
      "Epoch 110/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5120 - accuracy: 0.0982 - val_loss: 1.2253 - val_accuracy: 0.0912\n",
      "Epoch 111/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5147 - accuracy: 0.0987 - val_loss: 1.2188 - val_accuracy: 0.0992\n",
      "Epoch 112/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5137 - accuracy: 0.0947 - val_loss: 1.1762 - val_accuracy: 0.0952\n",
      "Epoch 113/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5227 - accuracy: 0.0951 - val_loss: 1.2210 - val_accuracy: 0.1012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5191 - accuracy: 0.0962 - val_loss: 1.2025 - val_accuracy: 0.0887\n",
      "Epoch 115/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5122 - accuracy: 0.0965 - val_loss: 1.2013 - val_accuracy: 0.0897\n",
      "Epoch 116/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5100 - accuracy: 0.1012 - val_loss: 1.2306 - val_accuracy: 0.1092\n",
      "Epoch 117/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5211 - accuracy: 0.0954 - val_loss: 1.2735 - val_accuracy: 0.0927\n",
      "Epoch 118/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5218 - accuracy: 0.0952 - val_loss: 1.2246 - val_accuracy: 0.1042\n",
      "Epoch 119/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5133 - accuracy: 0.0973 - val_loss: 1.3460 - val_accuracy: 0.0997\n",
      "Epoch 120/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5164 - accuracy: 0.0941 - val_loss: 1.2340 - val_accuracy: 0.0912\n",
      "Epoch 121/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5128 - accuracy: 0.0941 - val_loss: 1.2271 - val_accuracy: 0.0932\n",
      "Epoch 122/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5142 - accuracy: 0.0968 - val_loss: 1.2687 - val_accuracy: 0.0892\n",
      "Epoch 123/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5195 - accuracy: 0.0927 - val_loss: 1.2098 - val_accuracy: 0.0922\n",
      "Epoch 124/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5143 - accuracy: 0.0961 - val_loss: 1.1911 - val_accuracy: 0.0997\n",
      "Epoch 125/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5049 - accuracy: 0.0958 - val_loss: 1.3189 - val_accuracy: 0.0862\n",
      "Epoch 126/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5056 - accuracy: 0.0968 - val_loss: 1.2474 - val_accuracy: 0.1057\n",
      "Epoch 127/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5117 - accuracy: 0.0971 - val_loss: 1.2310 - val_accuracy: 0.1032\n",
      "Epoch 128/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5135 - accuracy: 0.0944 - val_loss: 1.1892 - val_accuracy: 0.0912\n",
      "Epoch 129/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5103 - accuracy: 0.0993 - val_loss: 1.2179 - val_accuracy: 0.0897\n",
      "Epoch 130/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5136 - accuracy: 0.0941 - val_loss: 1.2667 - val_accuracy: 0.1052\n",
      "Epoch 131/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5040 - accuracy: 0.0963 - val_loss: 1.2498 - val_accuracy: 0.1052\n",
      "Epoch 132/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5096 - accuracy: 0.0919 - val_loss: 1.1800 - val_accuracy: 0.0842\n",
      "Epoch 133/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5067 - accuracy: 0.0930 - val_loss: 1.3498 - val_accuracy: 0.0842\n",
      "Epoch 134/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5095 - accuracy: 0.0973 - val_loss: 1.2215 - val_accuracy: 0.0907\n",
      "Epoch 135/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5127 - accuracy: 0.0932 - val_loss: 1.2604 - val_accuracy: 0.0902\n",
      "Epoch 136/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5048 - accuracy: 0.0975 - val_loss: 1.2211 - val_accuracy: 0.0932\n",
      "Epoch 137/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5112 - accuracy: 0.0956 - val_loss: 1.2447 - val_accuracy: 0.0867\n",
      "Epoch 138/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5127 - accuracy: 0.0939 - val_loss: 1.2205 - val_accuracy: 0.0997\n",
      "Epoch 139/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5158 - accuracy: 0.0894 - val_loss: 1.1774 - val_accuracy: 0.0867\n",
      "Epoch 140/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5127 - accuracy: 0.0914 - val_loss: 1.1977 - val_accuracy: 0.1077\n",
      "Epoch 141/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5043 - accuracy: 0.0952 - val_loss: 1.3982 - val_accuracy: 0.0892\n",
      "Epoch 142/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5122 - accuracy: 0.1035 - val_loss: 1.2574 - val_accuracy: 0.0937\n",
      "Epoch 143/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5156 - accuracy: 0.0978 - val_loss: 1.1863 - val_accuracy: 0.0852\n",
      "Epoch 144/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5145 - accuracy: 0.0932 - val_loss: 1.1874 - val_accuracy: 0.0897\n",
      "Epoch 145/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5127 - accuracy: 0.0976 - val_loss: 1.2144 - val_accuracy: 0.0897\n",
      "Epoch 146/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5158 - accuracy: 0.0922 - val_loss: 1.2014 - val_accuracy: 0.0907\n",
      "Epoch 147/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5006 - accuracy: 0.0906 - val_loss: 1.2110 - val_accuracy: 0.0892\n",
      "Epoch 148/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5100 - accuracy: 0.0966 - val_loss: 1.2047 - val_accuracy: 0.0852\n",
      "Epoch 149/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4976 - accuracy: 0.0956 - val_loss: 1.2045 - val_accuracy: 0.0942\n",
      "Epoch 150/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5018 - accuracy: 0.0963 - val_loss: 1.2214 - val_accuracy: 0.0927\n",
      "Epoch 151/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5079 - accuracy: 0.0954 - val_loss: 1.2035 - val_accuracy: 0.0857\n",
      "Epoch 152/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5144 - accuracy: 0.0935 - val_loss: 1.2487 - val_accuracy: 0.0887\n",
      "Epoch 153/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5085 - accuracy: 0.0915 - val_loss: 1.2389 - val_accuracy: 0.0882\n",
      "Epoch 154/200\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.5061 - accuracy: 0.1016 - val_loss: 1.2007 - val_accuracy: 0.0997\n",
      "Epoch 155/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4989 - accuracy: 0.0976 - val_loss: 1.2843 - val_accuracy: 0.0897\n",
      "Epoch 156/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5015 - accuracy: 0.0906 - val_loss: 1.3561 - val_accuracy: 0.0867\n",
      "Epoch 157/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5063 - accuracy: 0.0990 - val_loss: 1.2887 - val_accuracy: 0.0837\n",
      "Epoch 158/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5034 - accuracy: 0.0975 - val_loss: 1.2456 - val_accuracy: 0.1037\n",
      "Epoch 159/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4989 - accuracy: 0.1021 - val_loss: 1.1916 - val_accuracy: 0.0902\n",
      "Epoch 160/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5042 - accuracy: 0.0945 - val_loss: 1.2436 - val_accuracy: 0.0997\n",
      "Epoch 161/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4988 - accuracy: 0.0914 - val_loss: 1.1842 - val_accuracy: 0.0922\n",
      "Epoch 162/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5041 - accuracy: 0.0988 - val_loss: 1.2066 - val_accuracy: 0.0937\n",
      "Epoch 163/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5004 - accuracy: 0.0968 - val_loss: 1.2247 - val_accuracy: 0.0932\n",
      "Epoch 164/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5096 - accuracy: 0.0981 - val_loss: 1.2764 - val_accuracy: 0.0922\n",
      "Epoch 165/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5030 - accuracy: 0.0981 - val_loss: 1.2294 - val_accuracy: 0.1062\n",
      "Epoch 166/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5023 - accuracy: 0.0930 - val_loss: 1.3170 - val_accuracy: 0.0922\n",
      "Epoch 167/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4951 - accuracy: 0.0961 - val_loss: 1.2346 - val_accuracy: 0.0922\n",
      "Epoch 168/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5037 - accuracy: 0.0973 - val_loss: 1.1865 - val_accuracy: 0.0867\n",
      "Epoch 169/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5005 - accuracy: 0.0961 - val_loss: 1.2892 - val_accuracy: 0.0922\n",
      "Epoch 170/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4996 - accuracy: 0.0907 - val_loss: 1.2379 - val_accuracy: 0.0867\n",
      "Epoch 171/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5028 - accuracy: 0.1011 - val_loss: 1.2142 - val_accuracy: 0.0862\n",
      "Epoch 172/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4951 - accuracy: 0.0988 - val_loss: 1.1930 - val_accuracy: 0.0912\n",
      "Epoch 173/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5070 - accuracy: 0.0961 - val_loss: 1.1795 - val_accuracy: 0.1057\n",
      "Epoch 174/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4942 - accuracy: 0.1005 - val_loss: 1.2927 - val_accuracy: 0.0907\n",
      "Epoch 175/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4854 - accuracy: 0.0972 - val_loss: 1.2088 - val_accuracy: 0.1057\n",
      "Epoch 176/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5003 - accuracy: 0.0987 - val_loss: 1.2277 - val_accuracy: 0.1037\n",
      "Epoch 177/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4929 - accuracy: 0.0982 - val_loss: 1.2298 - val_accuracy: 0.1037\n",
      "Epoch 178/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4911 - accuracy: 0.0978 - val_loss: 1.2515 - val_accuracy: 0.0997\n",
      "Epoch 179/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4939 - accuracy: 0.0932 - val_loss: 1.2015 - val_accuracy: 0.0932\n",
      "Epoch 180/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4995 - accuracy: 0.0965 - val_loss: 1.2114 - val_accuracy: 0.0957\n",
      "Epoch 181/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5041 - accuracy: 0.1018 - val_loss: 1.1795 - val_accuracy: 0.1047\n",
      "Epoch 182/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4954 - accuracy: 0.0966 - val_loss: 1.1997 - val_accuracy: 0.0997\n",
      "Epoch 183/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4919 - accuracy: 0.1025 - val_loss: 1.2422 - val_accuracy: 0.0877\n",
      "Epoch 184/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5041 - accuracy: 0.0905 - val_loss: 1.2088 - val_accuracy: 0.1032\n",
      "Epoch 185/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4893 - accuracy: 0.0972 - val_loss: 1.2781 - val_accuracy: 0.0932\n",
      "Epoch 186/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4935 - accuracy: 0.0879 - val_loss: 1.2264 - val_accuracy: 0.1002\n",
      "Epoch 187/200\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4927 - accuracy: 0.0914 - val_loss: 1.2850 - val_accuracy: 0.0872\n",
      "Epoch 188/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5029 - accuracy: 0.0926 - val_loss: 1.2308 - val_accuracy: 0.1057\n",
      "Epoch 189/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4934 - accuracy: 0.1007 - val_loss: 1.2806 - val_accuracy: 0.0912\n",
      "Epoch 190/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4924 - accuracy: 0.0954 - val_loss: 1.2467 - val_accuracy: 0.0922\n",
      "Epoch 191/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4938 - accuracy: 0.0967 - val_loss: 1.2244 - val_accuracy: 0.0932\n",
      "Epoch 192/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4887 - accuracy: 0.0985 - val_loss: 1.2303 - val_accuracy: 0.0897\n",
      "Epoch 193/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4967 - accuracy: 0.0939 - val_loss: 1.1885 - val_accuracy: 0.0892\n",
      "Epoch 194/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5022 - accuracy: 0.0946 - val_loss: 1.1670 - val_accuracy: 0.0962\n",
      "Epoch 195/200\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4979 - accuracy: 0.0981 - val_loss: 1.3019 - val_accuracy: 0.0947\n",
      "Epoch 196/200\n",
      "8023/8023 [==============================] - 8s 967us/step - loss: 1.4890 - accuracy: 0.0956 - val_loss: 1.1755 - val_accuracy: 0.0917\n",
      "Epoch 197/200\n",
      "8023/8023 [==============================] - 8s 975us/step - loss: 1.4992 - accuracy: 0.0939 - val_loss: 1.2288 - val_accuracy: 0.0842\n",
      "Epoch 198/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4872 - accuracy: 0.0936 - val_loss: 1.2812 - val_accuracy: 0.0897\n",
      "Epoch 199/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4974 - accuracy: 0.1003 - val_loss: 1.2286 - val_accuracy: 0.0872\n",
      "Epoch 200/200\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4946 - accuracy: 0.0958 - val_loss: 1.2065 - val_accuracy: 0.0887\n",
      "the NMAE for epochs= 200 is: ReadsAvg    0.028665\n",
      "ReadsAvg    0.028666\n",
      "ReadsAvg    0.028667\n",
      "ReadsAvg    0.028667\n",
      "ReadsAvg    0.028667\n",
      "ReadsAvg    0.028668\n",
      "ReadsAvg    0.028668\n",
      "ReadsAvg    0.028669\n",
      "ReadsAvg    0.028670\n",
      "ReadsAvg    0.028670\n",
      "ReadsAvg    0.028671\n",
      "dtype: float64\n",
      "Train on 8023 samples, validate on 2006 samples\n",
      "Epoch 1/500\n",
      "8023/8023 [==============================] - 31s 4ms/step - loss: 11.8604 - accuracy: 0.0904 - val_loss: 1.6029 - val_accuracy: 0.0912\n",
      "Epoch 2/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 3.1676 - accuracy: 0.0855 - val_loss: 1.5142 - val_accuracy: 0.0917\n",
      "Epoch 3/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 2.1373 - accuracy: 0.0899 - val_loss: 1.3412 - val_accuracy: 0.1037\n",
      "Epoch 4/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.6436 - accuracy: 0.0921 - val_loss: 1.4484 - val_accuracy: 0.0927\n",
      "Epoch 5/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.6231 - accuracy: 0.0936 - val_loss: 1.2457 - val_accuracy: 0.0917\n",
      "Epoch 6/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5910 - accuracy: 0.1035 - val_loss: 1.1856 - val_accuracy: 0.0927\n",
      "Epoch 7/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5902 - accuracy: 0.0897 - val_loss: 1.2041 - val_accuracy: 0.0897\n",
      "Epoch 8/500\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.5905 - accuracy: 0.0935 - val_loss: 1.2465 - val_accuracy: 0.0922\n",
      "Epoch 9/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5837 - accuracy: 0.0947 - val_loss: 1.2152 - val_accuracy: 0.0962\n",
      "Epoch 10/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5959 - accuracy: 0.0931 - val_loss: 1.2149 - val_accuracy: 0.1072\n",
      "Epoch 11/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5728 - accuracy: 0.0931 - val_loss: 1.2081 - val_accuracy: 0.0847\n",
      "Epoch 12/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5645 - accuracy: 0.0876 - val_loss: 1.3052 - val_accuracy: 0.1007\n",
      "Epoch 13/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5764 - accuracy: 0.0902 - val_loss: 1.2157 - val_accuracy: 0.0927\n",
      "Epoch 14/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5714 - accuracy: 0.0920 - val_loss: 1.2906 - val_accuracy: 0.0862\n",
      "Epoch 15/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5631 - accuracy: 0.0978 - val_loss: 1.2855 - val_accuracy: 0.0867\n",
      "Epoch 16/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5673 - accuracy: 0.0921 - val_loss: 1.2372 - val_accuracy: 0.1067\n",
      "Epoch 17/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5651 - accuracy: 0.0944 - val_loss: 1.2245 - val_accuracy: 0.0897\n",
      "Epoch 18/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5632 - accuracy: 0.0936 - val_loss: 1.2756 - val_accuracy: 0.0862\n",
      "Epoch 19/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5576 - accuracy: 0.0916 - val_loss: 1.1945 - val_accuracy: 0.0902\n",
      "Epoch 20/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5467 - accuracy: 0.0971 - val_loss: 1.2993 - val_accuracy: 0.0882\n",
      "Epoch 21/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5477 - accuracy: 0.0955 - val_loss: 1.2244 - val_accuracy: 0.1057\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5533 - accuracy: 0.0950 - val_loss: 1.2344 - val_accuracy: 0.0917\n",
      "Epoch 23/500\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 1.5379 - accuracy: 0.1000 - val_loss: 1.2143 - val_accuracy: 0.0927\n",
      "Epoch 24/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5406 - accuracy: 0.0904 - val_loss: 1.1762 - val_accuracy: 0.0887\n",
      "Epoch 25/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5407 - accuracy: 0.0942 - val_loss: 1.2138 - val_accuracy: 0.0927\n",
      "Epoch 26/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5379 - accuracy: 0.0944 - val_loss: 1.2098 - val_accuracy: 0.0907\n",
      "Epoch 27/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5381 - accuracy: 0.0987 - val_loss: 1.1916 - val_accuracy: 0.0852\n",
      "Epoch 28/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5354 - accuracy: 0.0915 - val_loss: 1.2131 - val_accuracy: 0.0897\n",
      "Epoch 29/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5320 - accuracy: 0.0956 - val_loss: 1.2254 - val_accuracy: 0.1052\n",
      "Epoch 30/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5254 - accuracy: 0.1020 - val_loss: 1.1938 - val_accuracy: 0.0937\n",
      "Epoch 31/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5324 - accuracy: 0.0947 - val_loss: 1.1946 - val_accuracy: 0.0922\n",
      "Epoch 32/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5337 - accuracy: 0.0952 - val_loss: 1.1693 - val_accuracy: 0.0937\n",
      "Epoch 33/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5422 - accuracy: 0.0954 - val_loss: 1.1944 - val_accuracy: 0.0942\n",
      "Epoch 34/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5372 - accuracy: 0.0926 - val_loss: 1.2745 - val_accuracy: 0.0897\n",
      "Epoch 35/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5255 - accuracy: 0.0954 - val_loss: 1.2647 - val_accuracy: 0.0857\n",
      "Epoch 36/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5391 - accuracy: 0.0930 - val_loss: 1.1996 - val_accuracy: 0.0927\n",
      "Epoch 37/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5320 - accuracy: 0.0997 - val_loss: 1.3407 - val_accuracy: 0.0897\n",
      "Epoch 38/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5266 - accuracy: 0.0941 - val_loss: 1.1794 - val_accuracy: 0.0927\n",
      "Epoch 39/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5295 - accuracy: 0.0941 - val_loss: 1.1999 - val_accuracy: 0.0917\n",
      "Epoch 40/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5270 - accuracy: 0.0931 - val_loss: 1.1844 - val_accuracy: 0.0892\n",
      "Epoch 41/500\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.5244 - accuracy: 0.0978 - val_loss: 1.2098 - val_accuracy: 0.0842\n",
      "Epoch 42/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5262 - accuracy: 0.0939 - val_loss: 1.2128 - val_accuracy: 0.0897\n",
      "Epoch 43/500\n",
      "8023/8023 [==============================] - 13s 2ms/step - loss: 1.5316 - accuracy: 0.0874 - val_loss: 1.1987 - val_accuracy: 0.0947\n",
      "Epoch 44/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5202 - accuracy: 0.0912 - val_loss: 1.1932 - val_accuracy: 0.1052\n",
      "Epoch 45/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5353 - accuracy: 0.0947 - val_loss: 1.1692 - val_accuracy: 0.0852\n",
      "Epoch 46/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5364 - accuracy: 0.0904 - val_loss: 1.1847 - val_accuracy: 0.1032\n",
      "Epoch 47/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5262 - accuracy: 0.1037 - val_loss: 1.1906 - val_accuracy: 0.0877\n",
      "Epoch 48/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5186 - accuracy: 0.0951 - val_loss: 1.1822 - val_accuracy: 0.0997\n",
      "Epoch 49/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5305 - accuracy: 0.0939 - val_loss: 1.2158 - val_accuracy: 0.0852\n",
      "Epoch 50/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5258 - accuracy: 0.0919 - val_loss: 1.2133 - val_accuracy: 0.1082\n",
      "Epoch 51/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5288 - accuracy: 0.0977 - val_loss: 1.2259 - val_accuracy: 0.1007\n",
      "Epoch 52/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5186 - accuracy: 0.0951 - val_loss: 1.1881 - val_accuracy: 0.0997\n",
      "Epoch 53/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5158 - accuracy: 0.0963 - val_loss: 1.1876 - val_accuracy: 0.0872\n",
      "Epoch 54/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5245 - accuracy: 0.0946 - val_loss: 1.1828 - val_accuracy: 0.0947\n",
      "Epoch 55/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5258 - accuracy: 0.0995 - val_loss: 1.2039 - val_accuracy: 0.0852\n",
      "Epoch 56/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5211 - accuracy: 0.1000 - val_loss: 1.1875 - val_accuracy: 0.1017\n",
      "Epoch 57/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5227 - accuracy: 0.0870 - val_loss: 1.1821 - val_accuracy: 0.0867\n",
      "Epoch 58/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5246 - accuracy: 0.0934 - val_loss: 1.1872 - val_accuracy: 0.0902\n",
      "Epoch 59/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5248 - accuracy: 0.0970 - val_loss: 1.1928 - val_accuracy: 0.1062\n",
      "Epoch 60/500\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 1.5183 - accuracy: 0.0945 - val_loss: 1.2302 - val_accuracy: 0.0927\n",
      "Epoch 61/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5318 - accuracy: 0.0940 - val_loss: 1.1743 - val_accuracy: 0.0942\n",
      "Epoch 62/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5184 - accuracy: 0.0997 - val_loss: 1.1799 - val_accuracy: 0.0967\n",
      "Epoch 63/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5305 - accuracy: 0.0963 - val_loss: 1.1847 - val_accuracy: 0.0932\n",
      "Epoch 64/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5225 - accuracy: 0.0924 - val_loss: 1.1926 - val_accuracy: 0.0877\n",
      "Epoch 65/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5264 - accuracy: 0.0904 - val_loss: 1.2098 - val_accuracy: 0.0877\n",
      "Epoch 66/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5174 - accuracy: 0.0947 - val_loss: 1.1630 - val_accuracy: 0.0917\n",
      "Epoch 67/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5219 - accuracy: 0.0939 - val_loss: 1.1720 - val_accuracy: 0.1082\n",
      "Epoch 68/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5118 - accuracy: 0.0932 - val_loss: 1.2359 - val_accuracy: 0.0997\n",
      "Epoch 69/500\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.5347 - accuracy: 0.0932 - val_loss: 1.1765 - val_accuracy: 0.0927\n",
      "Epoch 70/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5299 - accuracy: 0.0968 - val_loss: 1.2851 - val_accuracy: 0.0857\n",
      "Epoch 71/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5339 - accuracy: 0.0875 - val_loss: 1.1897 - val_accuracy: 0.1112\n",
      "Epoch 72/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5294 - accuracy: 0.0911 - val_loss: 1.2798 - val_accuracy: 0.0847\n",
      "Epoch 73/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5184 - accuracy: 0.0986 - val_loss: 1.2600 - val_accuracy: 0.0917\n",
      "Epoch 74/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5237 - accuracy: 0.0946 - val_loss: 1.2085 - val_accuracy: 0.0902\n",
      "Epoch 75/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5198 - accuracy: 0.0967 - val_loss: 1.1704 - val_accuracy: 0.0833\n",
      "Epoch 76/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5177 - accuracy: 0.0957 - val_loss: 1.2344 - val_accuracy: 0.0847\n",
      "Epoch 77/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5247 - accuracy: 0.0934 - val_loss: 1.1831 - val_accuracy: 0.0932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5231 - accuracy: 0.0916 - val_loss: 1.2096 - val_accuracy: 0.0852\n",
      "Epoch 79/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5199 - accuracy: 0.0922 - val_loss: 1.1777 - val_accuracy: 0.0947\n",
      "Epoch 80/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5243 - accuracy: 0.0985 - val_loss: 1.1912 - val_accuracy: 0.0917\n",
      "Epoch 81/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5246 - accuracy: 0.0920 - val_loss: 1.1801 - val_accuracy: 0.1022\n",
      "Epoch 82/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5160 - accuracy: 0.0985 - val_loss: 1.1885 - val_accuracy: 0.0987\n",
      "Epoch 83/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5257 - accuracy: 0.0961 - val_loss: 1.2191 - val_accuracy: 0.0867\n",
      "Epoch 84/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5309 - accuracy: 0.0971 - val_loss: 1.1910 - val_accuracy: 0.0897\n",
      "Epoch 85/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5213 - accuracy: 0.0963 - val_loss: 1.2132 - val_accuracy: 0.0987\n",
      "Epoch 86/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5119 - accuracy: 0.0931 - val_loss: 1.3000 - val_accuracy: 0.1052\n",
      "Epoch 87/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5456 - accuracy: 0.0934 - val_loss: 1.2277 - val_accuracy: 0.0897\n",
      "Epoch 88/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5257 - accuracy: 0.0954 - val_loss: 1.2166 - val_accuracy: 0.1077\n",
      "Epoch 89/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5133 - accuracy: 0.0906 - val_loss: 1.2072 - val_accuracy: 0.0922\n",
      "Epoch 90/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5106 - accuracy: 0.0985 - val_loss: 1.1881 - val_accuracy: 0.1027\n",
      "Epoch 91/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5168 - accuracy: 0.0949 - val_loss: 1.2096 - val_accuracy: 0.1057\n",
      "Epoch 92/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5167 - accuracy: 0.0915 - val_loss: 1.2122 - val_accuracy: 0.0907\n",
      "Epoch 93/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5194 - accuracy: 0.1012 - val_loss: 1.2112 - val_accuracy: 0.0828\n",
      "Epoch 94/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5102 - accuracy: 0.0942 - val_loss: 1.1907 - val_accuracy: 0.0852\n",
      "Epoch 95/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5146 - accuracy: 0.0915 - val_loss: 1.2149 - val_accuracy: 0.0987\n",
      "Epoch 96/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5157 - accuracy: 0.0924 - val_loss: 1.2460 - val_accuracy: 0.0912\n",
      "Epoch 97/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5171 - accuracy: 0.0965 - val_loss: 1.2063 - val_accuracy: 0.0917\n",
      "Epoch 98/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5092 - accuracy: 0.0955 - val_loss: 1.2133 - val_accuracy: 0.0912\n",
      "Epoch 99/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5233 - accuracy: 0.0977 - val_loss: 1.2119 - val_accuracy: 0.1047\n",
      "Epoch 100/500\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.5183 - accuracy: 0.0936 - val_loss: 1.2783 - val_accuracy: 0.0862\n",
      "Epoch 101/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5183 - accuracy: 0.0963 - val_loss: 1.2088 - val_accuracy: 0.0902\n",
      "Epoch 102/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5038 - accuracy: 0.0919 - val_loss: 1.2070 - val_accuracy: 0.0972\n",
      "Epoch 103/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5153 - accuracy: 0.0887 - val_loss: 1.1787 - val_accuracy: 0.0867\n",
      "Epoch 104/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5257 - accuracy: 0.0885 - val_loss: 1.2820 - val_accuracy: 0.1002\n",
      "Epoch 105/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5227 - accuracy: 0.0931 - val_loss: 1.2163 - val_accuracy: 0.0937\n",
      "Epoch 106/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5204 - accuracy: 0.0941 - val_loss: 1.2717 - val_accuracy: 0.0857\n",
      "Epoch 107/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5145 - accuracy: 0.1002 - val_loss: 1.1947 - val_accuracy: 0.0847\n",
      "Epoch 108/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5128 - accuracy: 0.0930 - val_loss: 1.2989 - val_accuracy: 0.0872\n",
      "Epoch 109/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5063 - accuracy: 0.0960 - val_loss: 1.1942 - val_accuracy: 0.0982\n",
      "Epoch 110/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5106 - accuracy: 0.0997 - val_loss: 1.2348 - val_accuracy: 0.0912\n",
      "Epoch 111/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5188 - accuracy: 0.0910 - val_loss: 1.2291 - val_accuracy: 0.0957\n",
      "Epoch 112/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5169 - accuracy: 0.0971 - val_loss: 1.2243 - val_accuracy: 0.0887\n",
      "Epoch 113/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5052 - accuracy: 0.1023 - val_loss: 1.2049 - val_accuracy: 0.0917\n",
      "Epoch 114/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5220 - accuracy: 0.0935 - val_loss: 1.2268 - val_accuracy: 0.0897\n",
      "Epoch 115/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5161 - accuracy: 0.0946 - val_loss: 1.1979 - val_accuracy: 0.0892\n",
      "Epoch 116/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5053 - accuracy: 0.0961 - val_loss: 1.2364 - val_accuracy: 0.0877\n",
      "Epoch 117/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5115 - accuracy: 0.0881 - val_loss: 1.2209 - val_accuracy: 0.0887\n",
      "Epoch 118/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5128 - accuracy: 0.0946 - val_loss: 1.2344 - val_accuracy: 0.0917\n",
      "Epoch 119/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5251 - accuracy: 0.0971 - val_loss: 1.1837 - val_accuracy: 0.0897\n",
      "Epoch 120/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5134 - accuracy: 0.0935 - val_loss: 1.2153 - val_accuracy: 0.0897\n",
      "Epoch 121/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5161 - accuracy: 0.0902 - val_loss: 1.1875 - val_accuracy: 0.0892\n",
      "Epoch 122/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5105 - accuracy: 0.0965 - val_loss: 1.3102 - val_accuracy: 0.0917\n",
      "Epoch 123/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5071 - accuracy: 0.0960 - val_loss: 1.2252 - val_accuracy: 0.0987\n",
      "Epoch 124/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5129 - accuracy: 0.0954 - val_loss: 1.2520 - val_accuracy: 0.0897\n",
      "Epoch 125/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5050 - accuracy: 0.0920 - val_loss: 1.1856 - val_accuracy: 0.0892\n",
      "Epoch 126/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4992 - accuracy: 0.0987 - val_loss: 1.2391 - val_accuracy: 0.0917\n",
      "Epoch 127/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5103 - accuracy: 0.0936 - val_loss: 1.3616 - val_accuracy: 0.0972\n",
      "Epoch 128/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5147 - accuracy: 0.1007 - val_loss: 1.2272 - val_accuracy: 0.0917\n",
      "Epoch 129/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5125 - accuracy: 0.0944 - val_loss: 1.1943 - val_accuracy: 0.0912\n",
      "Epoch 130/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5069 - accuracy: 0.0991 - val_loss: 1.2161 - val_accuracy: 0.0852\n",
      "Epoch 131/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5095 - accuracy: 0.0961 - val_loss: 1.1925 - val_accuracy: 0.0852\n",
      "Epoch 132/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5117 - accuracy: 0.0936 - val_loss: 1.2871 - val_accuracy: 0.0872\n",
      "Epoch 133/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5181 - accuracy: 0.0904 - val_loss: 1.2520 - val_accuracy: 0.0897\n",
      "Epoch 134/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5074 - accuracy: 0.0985 - val_loss: 1.2444 - val_accuracy: 0.0897\n",
      "Epoch 135/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5055 - accuracy: 0.0910 - val_loss: 1.2506 - val_accuracy: 0.0907\n",
      "Epoch 136/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5100 - accuracy: 0.0963 - val_loss: 1.2685 - val_accuracy: 0.0852\n",
      "Epoch 137/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4985 - accuracy: 0.0975 - val_loss: 1.3284 - val_accuracy: 0.0852\n",
      "Epoch 138/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5085 - accuracy: 0.0945 - val_loss: 1.2544 - val_accuracy: 0.1052\n",
      "Epoch 139/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5144 - accuracy: 0.0986 - val_loss: 1.2009 - val_accuracy: 0.0927\n",
      "Epoch 140/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5012 - accuracy: 0.0957 - val_loss: 1.2309 - val_accuracy: 0.1022\n",
      "Epoch 141/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5064 - accuracy: 0.0950 - val_loss: 1.3443 - val_accuracy: 0.1057\n",
      "Epoch 142/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5108 - accuracy: 0.0935 - val_loss: 1.1749 - val_accuracy: 0.0922\n",
      "Epoch 143/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5186 - accuracy: 0.0941 - val_loss: 1.2310 - val_accuracy: 0.1052\n",
      "Epoch 144/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5182 - accuracy: 0.0962 - val_loss: 1.2164 - val_accuracy: 0.0897\n",
      "Epoch 145/500\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 1.5127 - accuracy: 0.0973 - val_loss: 1.2581 - val_accuracy: 0.0952\n",
      "Epoch 146/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5139 - accuracy: 0.0963 - val_loss: 1.2062 - val_accuracy: 0.1052\n",
      "Epoch 147/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5242 - accuracy: 0.0985 - val_loss: 1.2084 - val_accuracy: 0.0872\n",
      "Epoch 148/500\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 1.5129 - accuracy: 0.0940 - val_loss: 1.2402 - val_accuracy: 0.0852\n",
      "Epoch 149/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5183 - accuracy: 0.0931 - val_loss: 1.1712 - val_accuracy: 0.0922\n",
      "Epoch 150/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5041 - accuracy: 0.0926 - val_loss: 1.3046 - val_accuracy: 0.0847\n",
      "Epoch 151/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5022 - accuracy: 0.0920 - val_loss: 1.2423 - val_accuracy: 0.0852\n",
      "Epoch 152/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5079 - accuracy: 0.0997 - val_loss: 1.2796 - val_accuracy: 0.0922\n",
      "Epoch 153/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5117 - accuracy: 0.1001 - val_loss: 1.2245 - val_accuracy: 0.0887\n",
      "Epoch 154/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5050 - accuracy: 0.0972 - val_loss: 1.2214 - val_accuracy: 0.0897\n",
      "Epoch 155/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5097 - accuracy: 0.0965 - val_loss: 1.2544 - val_accuracy: 0.0942\n",
      "Epoch 156/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4994 - accuracy: 0.0976 - val_loss: 1.2354 - val_accuracy: 0.1032\n",
      "Epoch 157/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5061 - accuracy: 0.0936 - val_loss: 1.1957 - val_accuracy: 0.0917\n",
      "Epoch 158/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5052 - accuracy: 0.0987 - val_loss: 1.1959 - val_accuracy: 0.0982\n",
      "Epoch 159/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5039 - accuracy: 0.0971 - val_loss: 1.2119 - val_accuracy: 0.0927\n",
      "Epoch 160/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4988 - accuracy: 0.0961 - val_loss: 1.2862 - val_accuracy: 0.1042\n",
      "Epoch 161/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5022 - accuracy: 0.0961 - val_loss: 1.2339 - val_accuracy: 0.0942\n",
      "Epoch 162/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5027 - accuracy: 0.0909 - val_loss: 1.2026 - val_accuracy: 0.0992\n",
      "Epoch 163/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5029 - accuracy: 0.0966 - val_loss: 1.1875 - val_accuracy: 0.1012\n",
      "Epoch 164/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5050 - accuracy: 0.0929 - val_loss: 1.2107 - val_accuracy: 0.0852\n",
      "Epoch 165/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5065 - accuracy: 0.0952 - val_loss: 1.2083 - val_accuracy: 0.0907\n",
      "Epoch 166/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5026 - accuracy: 0.0930 - val_loss: 1.2563 - val_accuracy: 0.0882\n",
      "Epoch 167/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5067 - accuracy: 0.1003 - val_loss: 1.1821 - val_accuracy: 0.0932\n",
      "Epoch 168/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5088 - accuracy: 0.0998 - val_loss: 1.2035 - val_accuracy: 0.1007\n",
      "Epoch 169/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5040 - accuracy: 0.0980 - val_loss: 1.1751 - val_accuracy: 0.0997\n",
      "Epoch 170/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5099 - accuracy: 0.0952 - val_loss: 1.2401 - val_accuracy: 0.0857\n",
      "Epoch 171/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4994 - accuracy: 0.0985 - val_loss: 1.2518 - val_accuracy: 0.0927\n",
      "Epoch 172/500\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 1.5035 - accuracy: 0.1001 - val_loss: 1.2955 - val_accuracy: 0.0887\n",
      "Epoch 173/500\n",
      "8023/8023 [==============================] - 12s 2ms/step - loss: 1.5078 - accuracy: 0.0949 - val_loss: 1.1998 - val_accuracy: 0.0852\n",
      "Epoch 174/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5024 - accuracy: 0.0960 - val_loss: 1.2909 - val_accuracy: 0.0912\n",
      "Epoch 175/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5026 - accuracy: 0.1044 - val_loss: 1.3406 - val_accuracy: 0.0877\n",
      "Epoch 176/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5080 - accuracy: 0.0997 - val_loss: 1.2441 - val_accuracy: 0.0917\n",
      "Epoch 177/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5035 - accuracy: 0.0925 - val_loss: 1.2164 - val_accuracy: 0.1052\n",
      "Epoch 178/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4967 - accuracy: 0.0952 - val_loss: 1.2238 - val_accuracy: 0.0897\n",
      "Epoch 179/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5124 - accuracy: 0.0957 - val_loss: 1.2482 - val_accuracy: 0.1002\n",
      "Epoch 180/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5037 - accuracy: 0.0996 - val_loss: 1.2375 - val_accuracy: 0.0887\n",
      "Epoch 181/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5203 - accuracy: 0.0904 - val_loss: 1.1934 - val_accuracy: 0.0907\n",
      "Epoch 182/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5182 - accuracy: 0.0963 - val_loss: 1.2006 - val_accuracy: 0.1052\n",
      "Epoch 183/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5046 - accuracy: 0.0931 - val_loss: 1.2052 - val_accuracy: 0.0877\n",
      "Epoch 184/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4969 - accuracy: 0.0957 - val_loss: 1.2844 - val_accuracy: 0.0882\n",
      "Epoch 185/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4970 - accuracy: 0.1000 - val_loss: 1.2253 - val_accuracy: 0.0852\n",
      "Epoch 186/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4976 - accuracy: 0.0906 - val_loss: 1.2064 - val_accuracy: 0.0882\n",
      "Epoch 187/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5056 - accuracy: 0.0955 - val_loss: 1.2137 - val_accuracy: 0.0877\n",
      "Epoch 188/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5039 - accuracy: 0.0972 - val_loss: 1.3245 - val_accuracy: 0.0922\n",
      "Epoch 189/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5042 - accuracy: 0.0986 - val_loss: 1.2553 - val_accuracy: 0.0877\n",
      "Epoch 190/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4994 - accuracy: 0.0980 - val_loss: 1.2042 - val_accuracy: 0.0902\n",
      "Epoch 191/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5018 - accuracy: 0.1007 - val_loss: 1.2015 - val_accuracy: 0.0917\n",
      "Epoch 192/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4975 - accuracy: 0.1013 - val_loss: 1.2001 - val_accuracy: 0.0997\n",
      "Epoch 193/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5016 - accuracy: 0.0905 - val_loss: 1.2518 - val_accuracy: 0.1032\n",
      "Epoch 194/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5036 - accuracy: 0.1012 - val_loss: 1.2569 - val_accuracy: 0.0937\n",
      "Epoch 195/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5045 - accuracy: 0.1003 - val_loss: 1.2164 - val_accuracy: 0.0877\n",
      "Epoch 196/500\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.4974 - accuracy: 0.0936 - val_loss: 1.2067 - val_accuracy: 0.0852\n",
      "Epoch 197/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4957 - accuracy: 0.1006 - val_loss: 1.1914 - val_accuracy: 0.0862\n",
      "Epoch 198/500\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 1.5000 - accuracy: 0.1002 - val_loss: 1.2325 - val_accuracy: 0.0897\n",
      "Epoch 199/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4999 - accuracy: 0.0961 - val_loss: 1.2879 - val_accuracy: 0.0932\n",
      "Epoch 200/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5047 - accuracy: 0.0968 - val_loss: 1.2868 - val_accuracy: 0.0902\n",
      "Epoch 201/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4927 - accuracy: 0.0983 - val_loss: 1.2072 - val_accuracy: 0.0932\n",
      "Epoch 202/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4993 - accuracy: 0.0975 - val_loss: 1.1916 - val_accuracy: 0.0932\n",
      "Epoch 203/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4909 - accuracy: 0.1033 - val_loss: 1.2240 - val_accuracy: 0.0877\n",
      "Epoch 204/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4964 - accuracy: 0.0957 - val_loss: 1.2402 - val_accuracy: 0.1052\n",
      "Epoch 205/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4983 - accuracy: 0.0916 - val_loss: 1.2045 - val_accuracy: 0.0932\n",
      "Epoch 206/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4961 - accuracy: 0.1010 - val_loss: 1.2266 - val_accuracy: 0.0852\n",
      "Epoch 207/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5008 - accuracy: 0.0995 - val_loss: 1.2508 - val_accuracy: 0.0847\n",
      "Epoch 208/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4996 - accuracy: 0.0972 - val_loss: 1.2172 - val_accuracy: 0.0937\n",
      "Epoch 209/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4977 - accuracy: 0.0951 - val_loss: 1.2845 - val_accuracy: 0.0892\n",
      "Epoch 210/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5118 - accuracy: 0.0925 - val_loss: 1.2984 - val_accuracy: 0.0897\n",
      "Epoch 211/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5151 - accuracy: 0.0920 - val_loss: 1.2353 - val_accuracy: 0.1002\n",
      "Epoch 212/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4946 - accuracy: 0.0939 - val_loss: 1.1883 - val_accuracy: 0.0897\n",
      "Epoch 213/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5026 - accuracy: 0.0949 - val_loss: 1.2211 - val_accuracy: 0.0912\n",
      "Epoch 214/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5050 - accuracy: 0.0951 - val_loss: 1.1838 - val_accuracy: 0.0897\n",
      "Epoch 215/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4979 - accuracy: 0.0985 - val_loss: 1.2159 - val_accuracy: 0.0902\n",
      "Epoch 216/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4958 - accuracy: 0.0896 - val_loss: 1.2848 - val_accuracy: 0.0917\n",
      "Epoch 217/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4970 - accuracy: 0.0919 - val_loss: 1.2246 - val_accuracy: 0.0957\n",
      "Epoch 218/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4927 - accuracy: 0.0956 - val_loss: 1.2348 - val_accuracy: 0.0837\n",
      "Epoch 219/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4960 - accuracy: 0.1006 - val_loss: 1.3068 - val_accuracy: 0.0952\n",
      "Epoch 220/500\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 1.5008 - accuracy: 0.0997 - val_loss: 1.2217 - val_accuracy: 0.0932\n",
      "Epoch 221/500\n",
      "8023/8023 [==============================] - 8s 993us/step - loss: 1.4969 - accuracy: 0.0987 - val_loss: 1.2024 - val_accuracy: 0.0892\n",
      "Epoch 222/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5013 - accuracy: 0.0966 - val_loss: 1.1907 - val_accuracy: 0.0897\n",
      "Epoch 223/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5034 - accuracy: 0.0914 - val_loss: 1.2002 - val_accuracy: 0.0887\n",
      "Epoch 224/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5051 - accuracy: 0.0992 - val_loss: 1.2001 - val_accuracy: 0.0977\n",
      "Epoch 225/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4941 - accuracy: 0.0972 - val_loss: 1.1852 - val_accuracy: 0.0942\n",
      "Epoch 226/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5017 - accuracy: 0.0954 - val_loss: 1.1993 - val_accuracy: 0.1002\n",
      "Epoch 227/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4980 - accuracy: 0.0950 - val_loss: 1.2174 - val_accuracy: 0.0847\n",
      "Epoch 228/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4999 - accuracy: 0.0929 - val_loss: 1.2864 - val_accuracy: 0.0887\n",
      "Epoch 229/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5013 - accuracy: 0.0909 - val_loss: 1.2428 - val_accuracy: 0.0852\n",
      "Epoch 230/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4920 - accuracy: 0.0940 - val_loss: 1.2329 - val_accuracy: 0.0947\n",
      "Epoch 231/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5042 - accuracy: 0.1013 - val_loss: 1.1963 - val_accuracy: 0.1022\n",
      "Epoch 232/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5027 - accuracy: 0.0983 - val_loss: 1.3326 - val_accuracy: 0.0892\n",
      "Epoch 233/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4920 - accuracy: 0.0980 - val_loss: 1.4653 - val_accuracy: 0.0842\n",
      "Epoch 234/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5000 - accuracy: 0.0950 - val_loss: 1.2283 - val_accuracy: 0.0947\n",
      "Epoch 235/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4975 - accuracy: 0.0993 - val_loss: 1.2702 - val_accuracy: 0.0912\n",
      "Epoch 236/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4949 - accuracy: 0.0879 - val_loss: 1.1805 - val_accuracy: 0.1047\n",
      "Epoch 237/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4898 - accuracy: 0.1002 - val_loss: 1.2652 - val_accuracy: 0.0897\n",
      "Epoch 238/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4948 - accuracy: 0.0936 - val_loss: 1.2599 - val_accuracy: 0.0837\n",
      "Epoch 239/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4980 - accuracy: 0.0992 - val_loss: 1.1937 - val_accuracy: 0.1002\n",
      "Epoch 240/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4909 - accuracy: 0.0980 - val_loss: 1.2243 - val_accuracy: 0.0942\n",
      "Epoch 241/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4918 - accuracy: 0.1010 - val_loss: 1.2289 - val_accuracy: 0.1052\n",
      "Epoch 242/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4946 - accuracy: 0.0965 - val_loss: 1.2617 - val_accuracy: 0.0867\n",
      "Epoch 243/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4864 - accuracy: 0.0995 - val_loss: 1.2175 - val_accuracy: 0.0932\n",
      "Epoch 244/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5106 - accuracy: 0.1018 - val_loss: 1.1982 - val_accuracy: 0.0877\n",
      "Epoch 245/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4983 - accuracy: 0.0960 - val_loss: 1.1857 - val_accuracy: 0.0907\n",
      "Epoch 246/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4942 - accuracy: 0.1001 - val_loss: 1.2253 - val_accuracy: 0.0937\n",
      "Epoch 247/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4899 - accuracy: 0.0907 - val_loss: 1.2103 - val_accuracy: 0.0867\n",
      "Epoch 248/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4937 - accuracy: 0.0914 - val_loss: 1.1924 - val_accuracy: 0.1002\n",
      "Epoch 249/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5025 - accuracy: 0.0924 - val_loss: 1.2544 - val_accuracy: 0.1047\n",
      "Epoch 250/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4972 - accuracy: 0.0960 - val_loss: 1.2248 - val_accuracy: 0.0917\n",
      "Epoch 251/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4978 - accuracy: 0.0966 - val_loss: 1.2129 - val_accuracy: 0.0847\n",
      "Epoch 252/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4904 - accuracy: 0.0917 - val_loss: 1.2023 - val_accuracy: 0.0902\n",
      "Epoch 253/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5050 - accuracy: 0.0961 - val_loss: 1.2347 - val_accuracy: 0.0847\n",
      "Epoch 254/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4877 - accuracy: 0.0899 - val_loss: 1.2273 - val_accuracy: 0.1047\n",
      "Epoch 255/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4912 - accuracy: 0.0900 - val_loss: 1.2165 - val_accuracy: 0.0877\n",
      "Epoch 256/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4989 - accuracy: 0.0977 - val_loss: 1.2379 - val_accuracy: 0.0852\n",
      "Epoch 257/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.5035 - accuracy: 0.0967 - val_loss: 1.2721 - val_accuracy: 0.0857\n",
      "Epoch 258/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5033 - accuracy: 0.0971 - val_loss: 1.2064 - val_accuracy: 0.0852\n",
      "Epoch 259/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4957 - accuracy: 0.0951 - val_loss: 1.3289 - val_accuracy: 0.0927\n",
      "Epoch 260/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4929 - accuracy: 0.0970 - val_loss: 1.3056 - val_accuracy: 0.1052\n",
      "Epoch 261/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4874 - accuracy: 0.0946 - val_loss: 1.1795 - val_accuracy: 0.0942\n",
      "Epoch 262/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4908 - accuracy: 0.0947 - val_loss: 1.1967 - val_accuracy: 0.0937\n",
      "Epoch 263/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4902 - accuracy: 0.0976 - val_loss: 1.2158 - val_accuracy: 0.0922\n",
      "Epoch 264/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4908 - accuracy: 0.0976 - val_loss: 1.1904 - val_accuracy: 0.0937\n",
      "Epoch 265/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4894 - accuracy: 0.0997 - val_loss: 1.2789 - val_accuracy: 0.0882\n",
      "Epoch 266/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4922 - accuracy: 0.0977 - val_loss: 1.2163 - val_accuracy: 0.1032\n",
      "Epoch 267/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4939 - accuracy: 0.0934 - val_loss: 1.2103 - val_accuracy: 0.1127\n",
      "Epoch 268/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5005 - accuracy: 0.0932 - val_loss: 1.2101 - val_accuracy: 0.1042\n",
      "Epoch 269/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5036 - accuracy: 0.0966 - val_loss: 1.2443 - val_accuracy: 0.1052\n",
      "Epoch 270/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4918 - accuracy: 0.0960 - val_loss: 1.2461 - val_accuracy: 0.0852\n",
      "Epoch 271/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.5034 - accuracy: 0.0899 - val_loss: 1.2233 - val_accuracy: 0.0902\n",
      "Epoch 272/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4884 - accuracy: 0.0932 - val_loss: 1.1898 - val_accuracy: 0.0912\n",
      "Epoch 273/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4901 - accuracy: 0.0978 - val_loss: 1.2364 - val_accuracy: 0.0867\n",
      "Epoch 274/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4909 - accuracy: 0.0976 - val_loss: 1.2653 - val_accuracy: 0.1052\n",
      "Epoch 275/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4906 - accuracy: 0.0985 - val_loss: 1.2196 - val_accuracy: 0.0852\n",
      "Epoch 276/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4929 - accuracy: 0.0904 - val_loss: 1.1772 - val_accuracy: 0.1002\n",
      "Epoch 277/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4959 - accuracy: 0.0935 - val_loss: 1.2169 - val_accuracy: 0.0897\n",
      "Epoch 278/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4878 - accuracy: 0.0981 - val_loss: 1.2644 - val_accuracy: 0.1002\n",
      "Epoch 279/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4907 - accuracy: 0.0975 - val_loss: 1.2381 - val_accuracy: 0.0912\n",
      "Epoch 280/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4983 - accuracy: 0.0950 - val_loss: 1.1906 - val_accuracy: 0.1052\n",
      "Epoch 281/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.5000 - accuracy: 0.0927 - val_loss: 1.2520 - val_accuracy: 0.0982\n",
      "Epoch 282/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4936 - accuracy: 0.0897 - val_loss: 1.2140 - val_accuracy: 0.0927\n",
      "Epoch 283/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4876 - accuracy: 0.0914 - val_loss: 1.1806 - val_accuracy: 0.0912\n",
      "Epoch 284/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4876 - accuracy: 0.0975 - val_loss: 1.2041 - val_accuracy: 0.0952\n",
      "Epoch 285/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4861 - accuracy: 0.0983 - val_loss: 1.2420 - val_accuracy: 0.0902\n",
      "Epoch 286/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4884 - accuracy: 0.0972 - val_loss: 1.2605 - val_accuracy: 0.1052\n",
      "Epoch 287/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4843 - accuracy: 0.0960 - val_loss: 1.2078 - val_accuracy: 0.1057\n",
      "Epoch 288/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4872 - accuracy: 0.0981 - val_loss: 1.2293 - val_accuracy: 0.1052\n",
      "Epoch 289/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4940 - accuracy: 0.0951 - val_loss: 1.2153 - val_accuracy: 0.0932\n",
      "Epoch 290/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4915 - accuracy: 0.0917 - val_loss: 1.2074 - val_accuracy: 0.0902\n",
      "Epoch 291/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4884 - accuracy: 0.0996 - val_loss: 1.1874 - val_accuracy: 0.0907\n",
      "Epoch 292/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4836 - accuracy: 0.0926 - val_loss: 1.2325 - val_accuracy: 0.0877\n",
      "Epoch 293/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4859 - accuracy: 0.0997 - val_loss: 1.2015 - val_accuracy: 0.0912\n",
      "Epoch 294/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4877 - accuracy: 0.0968 - val_loss: 1.2382 - val_accuracy: 0.0897\n",
      "Epoch 295/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4898 - accuracy: 0.0962 - val_loss: 1.2502 - val_accuracy: 0.0947\n",
      "Epoch 296/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4914 - accuracy: 0.1020 - val_loss: 1.2191 - val_accuracy: 0.0882\n",
      "Epoch 297/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4845 - accuracy: 0.0982 - val_loss: 1.2262 - val_accuracy: 0.1002\n",
      "Epoch 298/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4815 - accuracy: 0.0990 - val_loss: 1.2384 - val_accuracy: 0.0912\n",
      "Epoch 299/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4920 - accuracy: 0.0987 - val_loss: 1.2397 - val_accuracy: 0.1012\n",
      "Epoch 300/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4863 - accuracy: 0.0973 - val_loss: 1.2239 - val_accuracy: 0.0877\n",
      "Epoch 301/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4868 - accuracy: 0.0972 - val_loss: 1.1759 - val_accuracy: 0.0932\n",
      "Epoch 302/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4854 - accuracy: 0.0880 - val_loss: 1.2158 - val_accuracy: 0.0867\n",
      "Epoch 303/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4823 - accuracy: 0.0936 - val_loss: 1.2282 - val_accuracy: 0.0917\n",
      "Epoch 304/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4867 - accuracy: 0.0945 - val_loss: 1.2082 - val_accuracy: 0.1057\n",
      "Epoch 305/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4824 - accuracy: 0.0971 - val_loss: 1.1877 - val_accuracy: 0.0947\n",
      "Epoch 306/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4878 - accuracy: 0.0956 - val_loss: 1.2637 - val_accuracy: 0.0877\n",
      "Epoch 307/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4897 - accuracy: 0.1018 - val_loss: 1.2486 - val_accuracy: 0.0862\n",
      "Epoch 308/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4952 - accuracy: 0.0916 - val_loss: 1.1904 - val_accuracy: 0.0877\n",
      "Epoch 309/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4855 - accuracy: 0.0926 - val_loss: 1.2109 - val_accuracy: 0.0907\n",
      "Epoch 310/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4950 - accuracy: 0.0986 - val_loss: 1.2108 - val_accuracy: 0.0902\n",
      "Epoch 311/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4873 - accuracy: 0.0985 - val_loss: 1.1867 - val_accuracy: 0.0922\n",
      "Epoch 312/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4814 - accuracy: 0.0905 - val_loss: 1.2043 - val_accuracy: 0.0922\n",
      "Epoch 313/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4915 - accuracy: 0.0995 - val_loss: 1.2016 - val_accuracy: 0.0962\n",
      "Epoch 314/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4889 - accuracy: 0.0985 - val_loss: 1.1779 - val_accuracy: 0.1012\n",
      "Epoch 315/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4821 - accuracy: 0.1037 - val_loss: 1.1919 - val_accuracy: 0.0857\n",
      "Epoch 316/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4748 - accuracy: 0.0971 - val_loss: 1.1822 - val_accuracy: 0.0897\n",
      "Epoch 317/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4888 - accuracy: 0.1017 - val_loss: 1.2263 - val_accuracy: 0.1057\n",
      "Epoch 318/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4889 - accuracy: 0.0977 - val_loss: 1.2879 - val_accuracy: 0.0917\n",
      "Epoch 319/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4788 - accuracy: 0.0966 - val_loss: 1.1829 - val_accuracy: 0.0912\n",
      "Epoch 320/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4818 - accuracy: 0.1007 - val_loss: 1.2187 - val_accuracy: 0.0892\n",
      "Epoch 321/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4927 - accuracy: 0.0917 - val_loss: 1.1986 - val_accuracy: 0.1022\n",
      "Epoch 322/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4834 - accuracy: 0.0954 - val_loss: 1.2645 - val_accuracy: 0.0942\n",
      "Epoch 323/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4856 - accuracy: 0.0939 - val_loss: 1.2036 - val_accuracy: 0.0852\n",
      "Epoch 324/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4868 - accuracy: 0.1005 - val_loss: 1.2149 - val_accuracy: 0.0907\n",
      "Epoch 325/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4832 - accuracy: 0.1011 - val_loss: 1.2060 - val_accuracy: 0.1052\n",
      "Epoch 326/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4828 - accuracy: 0.1011 - val_loss: 1.2213 - val_accuracy: 0.0932\n",
      "Epoch 327/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4797 - accuracy: 0.0958 - val_loss: 1.2470 - val_accuracy: 0.0872\n",
      "Epoch 328/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4784 - accuracy: 0.1025 - val_loss: 1.2251 - val_accuracy: 0.0997\n",
      "Epoch 329/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4829 - accuracy: 0.0977 - val_loss: 1.1992 - val_accuracy: 0.0907\n",
      "Epoch 330/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4927 - accuracy: 0.0932 - val_loss: 1.2283 - val_accuracy: 0.0862\n",
      "Epoch 331/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4846 - accuracy: 0.0937 - val_loss: 1.2551 - val_accuracy: 0.0892\n",
      "Epoch 332/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4846 - accuracy: 0.0950 - val_loss: 1.2170 - val_accuracy: 0.0907\n",
      "Epoch 333/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4901 - accuracy: 0.0944 - val_loss: 1.2312 - val_accuracy: 0.0937\n",
      "Epoch 334/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4890 - accuracy: 0.0967 - val_loss: 1.2113 - val_accuracy: 0.0917\n",
      "Epoch 335/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4835 - accuracy: 0.1023 - val_loss: 1.1913 - val_accuracy: 0.0882\n",
      "Epoch 336/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4820 - accuracy: 0.1027 - val_loss: 1.2058 - val_accuracy: 0.0997\n",
      "Epoch 337/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4784 - accuracy: 0.0980 - val_loss: 1.2054 - val_accuracy: 0.0887\n",
      "Epoch 338/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4794 - accuracy: 0.0966 - val_loss: 1.1973 - val_accuracy: 0.1052\n",
      "Epoch 339/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4771 - accuracy: 0.1006 - val_loss: 1.1915 - val_accuracy: 0.1052\n",
      "Epoch 340/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4768 - accuracy: 0.0957 - val_loss: 1.2239 - val_accuracy: 0.0862\n",
      "Epoch 341/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4786 - accuracy: 0.1005 - val_loss: 1.2482 - val_accuracy: 0.0882\n",
      "Epoch 342/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4836 - accuracy: 0.0962 - val_loss: 1.1997 - val_accuracy: 0.0912\n",
      "Epoch 343/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4825 - accuracy: 0.1017 - val_loss: 1.2328 - val_accuracy: 0.0892\n",
      "Epoch 344/500\n",
      "8023/8023 [==============================] - 12s 2ms/step - loss: 1.4805 - accuracy: 0.1002 - val_loss: 1.2416 - val_accuracy: 0.0872\n",
      "Epoch 345/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4848 - accuracy: 0.1032 - val_loss: 1.1808 - val_accuracy: 0.1002\n",
      "Epoch 346/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4816 - accuracy: 0.0993 - val_loss: 1.2667 - val_accuracy: 0.1057\n",
      "Epoch 347/500\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 1.4779 - accuracy: 0.1008 - val_loss: 1.2258 - val_accuracy: 0.0862\n",
      "Epoch 348/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4802 - accuracy: 0.0954 - val_loss: 1.2222 - val_accuracy: 0.0902\n",
      "Epoch 349/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4740 - accuracy: 0.1062 - val_loss: 1.1836 - val_accuracy: 0.0862\n",
      "Epoch 350/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4822 - accuracy: 0.0971 - val_loss: 1.2421 - val_accuracy: 0.0877\n",
      "Epoch 351/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4808 - accuracy: 0.0958 - val_loss: 1.2241 - val_accuracy: 0.0932\n",
      "Epoch 352/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4753 - accuracy: 0.1012 - val_loss: 1.2189 - val_accuracy: 0.0922\n",
      "Epoch 353/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4724 - accuracy: 0.0957 - val_loss: 1.2144 - val_accuracy: 0.0912\n",
      "Epoch 354/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4758 - accuracy: 0.1016 - val_loss: 1.1986 - val_accuracy: 0.0922\n",
      "Epoch 355/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4710 - accuracy: 0.1002 - val_loss: 1.1909 - val_accuracy: 0.0907\n",
      "Epoch 356/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4736 - accuracy: 0.0932 - val_loss: 1.2207 - val_accuracy: 0.0862\n",
      "Epoch 357/500\n",
      "8023/8023 [==============================] - 12s 2ms/step - loss: 1.4768 - accuracy: 0.1018 - val_loss: 1.2584 - val_accuracy: 0.0917\n",
      "Epoch 358/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4789 - accuracy: 0.0957 - val_loss: 1.2097 - val_accuracy: 0.0867\n",
      "Epoch 359/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4827 - accuracy: 0.0997 - val_loss: 1.2842 - val_accuracy: 0.0852\n",
      "Epoch 360/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4847 - accuracy: 0.0941 - val_loss: 1.2066 - val_accuracy: 0.0897\n",
      "Epoch 361/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4709 - accuracy: 0.1036 - val_loss: 1.2619 - val_accuracy: 0.0897\n",
      "Epoch 362/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4791 - accuracy: 0.1008 - val_loss: 1.2130 - val_accuracy: 0.0887\n",
      "Epoch 363/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4723 - accuracy: 0.0980 - val_loss: 1.2092 - val_accuracy: 0.0902\n",
      "Epoch 364/500\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.4718 - accuracy: 0.0946 - val_loss: 1.2594 - val_accuracy: 0.0932\n",
      "Epoch 365/500\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.4705 - accuracy: 0.0949 - val_loss: 1.2197 - val_accuracy: 0.0952\n",
      "Epoch 366/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4731 - accuracy: 0.1040 - val_loss: 1.1743 - val_accuracy: 0.0972\n",
      "Epoch 367/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4754 - accuracy: 0.1068 - val_loss: 1.2042 - val_accuracy: 0.0952\n",
      "Epoch 368/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4739 - accuracy: 0.0960 - val_loss: 1.1894 - val_accuracy: 0.1007\n",
      "Epoch 369/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4742 - accuracy: 0.1000 - val_loss: 1.2777 - val_accuracy: 0.1052\n",
      "Epoch 370/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4802 - accuracy: 0.1005 - val_loss: 1.2267 - val_accuracy: 0.0852\n",
      "Epoch 371/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4740 - accuracy: 0.1036 - val_loss: 1.2335 - val_accuracy: 0.0927\n",
      "Epoch 372/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4710 - accuracy: 0.1018 - val_loss: 1.1974 - val_accuracy: 0.0907\n",
      "Epoch 373/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4737 - accuracy: 0.1013 - val_loss: 1.2601 - val_accuracy: 0.0862\n",
      "Epoch 374/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4663 - accuracy: 0.0996 - val_loss: 1.2087 - val_accuracy: 0.0947\n",
      "Epoch 375/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4698 - accuracy: 0.0956 - val_loss: 1.2130 - val_accuracy: 0.1052\n",
      "Epoch 376/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4743 - accuracy: 0.0992 - val_loss: 1.1978 - val_accuracy: 0.0957\n",
      "Epoch 377/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4756 - accuracy: 0.0963 - val_loss: 1.1738 - val_accuracy: 0.0862\n",
      "Epoch 378/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4781 - accuracy: 0.0957 - val_loss: 1.1912 - val_accuracy: 0.0877\n",
      "Epoch 379/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4765 - accuracy: 0.0997 - val_loss: 1.1827 - val_accuracy: 0.0912\n",
      "Epoch 380/500\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.4760 - accuracy: 0.1016 - val_loss: 1.2324 - val_accuracy: 0.0867\n",
      "Epoch 381/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4759 - accuracy: 0.1001 - val_loss: 1.2714 - val_accuracy: 0.0982\n",
      "Epoch 382/500\n",
      "8023/8023 [==============================] - 12s 1ms/step - loss: 1.4861 - accuracy: 0.1073 - val_loss: 1.2547 - val_accuracy: 0.0892\n",
      "Epoch 383/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4743 - accuracy: 0.0946 - val_loss: 1.1837 - val_accuracy: 0.0932\n",
      "Epoch 384/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4739 - accuracy: 0.0990 - val_loss: 1.1816 - val_accuracy: 0.0867\n",
      "Epoch 385/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4701 - accuracy: 0.0967 - val_loss: 1.2232 - val_accuracy: 0.0882\n",
      "Epoch 386/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4685 - accuracy: 0.0982 - val_loss: 1.2144 - val_accuracy: 0.0937\n",
      "Epoch 387/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4731 - accuracy: 0.0976 - val_loss: 1.2672 - val_accuracy: 0.0907\n",
      "Epoch 388/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4735 - accuracy: 0.0954 - val_loss: 1.2404 - val_accuracy: 0.0902\n",
      "Epoch 389/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4742 - accuracy: 0.0965 - val_loss: 1.2110 - val_accuracy: 0.0897\n",
      "Epoch 390/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4754 - accuracy: 0.1022 - val_loss: 1.2223 - val_accuracy: 0.0892\n",
      "Epoch 391/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4777 - accuracy: 0.1026 - val_loss: 1.1873 - val_accuracy: 0.0937\n",
      "Epoch 392/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4702 - accuracy: 0.0965 - val_loss: 1.3172 - val_accuracy: 0.1007\n",
      "Epoch 393/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4700 - accuracy: 0.0937 - val_loss: 1.2020 - val_accuracy: 0.0927\n",
      "Epoch 394/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4694 - accuracy: 0.1037 - val_loss: 1.2346 - val_accuracy: 0.1042\n",
      "Epoch 395/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4768 - accuracy: 0.1000 - val_loss: 1.2238 - val_accuracy: 0.0932\n",
      "Epoch 396/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4718 - accuracy: 0.1015 - val_loss: 1.1994 - val_accuracy: 0.1012\n",
      "Epoch 397/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4759 - accuracy: 0.0975 - val_loss: 1.2177 - val_accuracy: 0.0927\n",
      "Epoch 398/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4712 - accuracy: 0.0963 - val_loss: 1.1952 - val_accuracy: 0.0917\n",
      "Epoch 399/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4793 - accuracy: 0.0945 - val_loss: 1.2308 - val_accuracy: 0.0942\n",
      "Epoch 400/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4764 - accuracy: 0.1011 - val_loss: 1.1936 - val_accuracy: 0.0837\n",
      "Epoch 401/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4698 - accuracy: 0.0990 - val_loss: 1.2188 - val_accuracy: 0.0872\n",
      "Epoch 402/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4735 - accuracy: 0.0990 - val_loss: 1.2235 - val_accuracy: 0.0942\n",
      "Epoch 403/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4779 - accuracy: 0.1042 - val_loss: 1.2037 - val_accuracy: 0.0952\n",
      "Epoch 404/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4704 - accuracy: 0.0986 - val_loss: 1.2100 - val_accuracy: 0.0917\n",
      "Epoch 405/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4667 - accuracy: 0.1020 - val_loss: 1.2364 - val_accuracy: 0.0927\n",
      "Epoch 406/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4797 - accuracy: 0.0960 - val_loss: 1.2023 - val_accuracy: 0.0922\n",
      "Epoch 407/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4620 - accuracy: 0.0930 - val_loss: 1.1847 - val_accuracy: 0.1052\n",
      "Epoch 408/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4777 - accuracy: 0.0996 - val_loss: 1.2161 - val_accuracy: 0.0887\n",
      "Epoch 409/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4780 - accuracy: 0.1010 - val_loss: 1.2005 - val_accuracy: 0.0932\n",
      "Epoch 410/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4715 - accuracy: 0.0931 - val_loss: 1.2032 - val_accuracy: 0.1022\n",
      "Epoch 411/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4682 - accuracy: 0.1041 - val_loss: 1.2805 - val_accuracy: 0.0872\n",
      "Epoch 412/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4719 - accuracy: 0.0990 - val_loss: 1.2910 - val_accuracy: 0.0852\n",
      "Epoch 413/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4701 - accuracy: 0.1025 - val_loss: 1.2299 - val_accuracy: 0.0902\n",
      "Epoch 414/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4704 - accuracy: 0.1027 - val_loss: 1.2696 - val_accuracy: 0.1052\n",
      "Epoch 415/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4786 - accuracy: 0.0949 - val_loss: 1.2599 - val_accuracy: 0.0912\n",
      "Epoch 416/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4720 - accuracy: 0.1018 - val_loss: 1.2670 - val_accuracy: 0.1037\n",
      "Epoch 417/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4637 - accuracy: 0.0968 - val_loss: 1.2187 - val_accuracy: 0.0992\n",
      "Epoch 418/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4802 - accuracy: 0.0972 - val_loss: 1.2273 - val_accuracy: 0.0877\n",
      "Epoch 419/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4725 - accuracy: 0.0956 - val_loss: 1.2097 - val_accuracy: 0.0912\n",
      "Epoch 420/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4797 - accuracy: 0.0952 - val_loss: 1.2002 - val_accuracy: 0.0902\n",
      "Epoch 421/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4721 - accuracy: 0.1030 - val_loss: 1.2267 - val_accuracy: 0.0907\n",
      "Epoch 422/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4653 - accuracy: 0.1008 - val_loss: 1.1887 - val_accuracy: 0.0927\n",
      "Epoch 423/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4713 - accuracy: 0.1003 - val_loss: 1.2368 - val_accuracy: 0.0852\n",
      "Epoch 424/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4730 - accuracy: 0.0992 - val_loss: 1.2314 - val_accuracy: 0.0997\n",
      "Epoch 425/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4718 - accuracy: 0.0985 - val_loss: 1.2169 - val_accuracy: 0.0847\n",
      "Epoch 426/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4719 - accuracy: 0.0967 - val_loss: 1.2058 - val_accuracy: 0.1052\n",
      "Epoch 427/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4761 - accuracy: 0.1054 - val_loss: 1.2271 - val_accuracy: 0.0862\n",
      "Epoch 428/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4662 - accuracy: 0.0986 - val_loss: 1.2329 - val_accuracy: 0.0942\n",
      "Epoch 429/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4720 - accuracy: 0.0961 - val_loss: 1.2075 - val_accuracy: 0.0982\n",
      "Epoch 430/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4729 - accuracy: 0.0975 - val_loss: 1.2153 - val_accuracy: 0.1007\n",
      "Epoch 431/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4686 - accuracy: 0.1022 - val_loss: 1.1924 - val_accuracy: 0.0912\n",
      "Epoch 432/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4701 - accuracy: 0.1005 - val_loss: 1.1746 - val_accuracy: 0.0852\n",
      "Epoch 433/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4796 - accuracy: 0.0985 - val_loss: 1.2554 - val_accuracy: 0.0877\n",
      "Epoch 434/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4737 - accuracy: 0.1076 - val_loss: 1.2232 - val_accuracy: 0.0917\n",
      "Epoch 435/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4694 - accuracy: 0.0995 - val_loss: 1.2009 - val_accuracy: 0.0887\n",
      "Epoch 436/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4659 - accuracy: 0.1003 - val_loss: 1.2778 - val_accuracy: 0.0877\n",
      "Epoch 437/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4740 - accuracy: 0.1015 - val_loss: 1.2067 - val_accuracy: 0.0977\n",
      "Epoch 438/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4720 - accuracy: 0.1042 - val_loss: 1.1909 - val_accuracy: 0.0892\n",
      "Epoch 439/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4689 - accuracy: 0.0958 - val_loss: 1.1728 - val_accuracy: 0.0917\n",
      "Epoch 440/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4669 - accuracy: 0.0968 - val_loss: 1.2354 - val_accuracy: 0.0902\n",
      "Epoch 441/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4678 - accuracy: 0.0935 - val_loss: 1.1723 - val_accuracy: 0.0877\n",
      "Epoch 442/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4692 - accuracy: 0.1021 - val_loss: 1.2006 - val_accuracy: 0.0847\n",
      "Epoch 443/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4675 - accuracy: 0.1016 - val_loss: 1.2091 - val_accuracy: 0.0892\n",
      "Epoch 444/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4770 - accuracy: 0.0973 - val_loss: 1.2355 - val_accuracy: 0.0867\n",
      "Epoch 445/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4675 - accuracy: 0.0992 - val_loss: 1.2045 - val_accuracy: 0.0937\n",
      "Epoch 446/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4656 - accuracy: 0.0988 - val_loss: 1.2817 - val_accuracy: 0.1067\n",
      "Epoch 447/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4761 - accuracy: 0.1007 - val_loss: 1.2868 - val_accuracy: 0.0992\n",
      "Epoch 448/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4774 - accuracy: 0.0945 - val_loss: 1.2009 - val_accuracy: 0.0882\n",
      "Epoch 449/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4719 - accuracy: 0.0946 - val_loss: 1.2130 - val_accuracy: 0.0867\n",
      "Epoch 450/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4639 - accuracy: 0.0975 - val_loss: 1.2369 - val_accuracy: 0.0847\n",
      "Epoch 451/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4707 - accuracy: 0.1051 - val_loss: 1.1959 - val_accuracy: 0.0882\n",
      "Epoch 452/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4809 - accuracy: 0.0936 - val_loss: 1.2364 - val_accuracy: 0.0977\n",
      "Epoch 453/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4704 - accuracy: 0.1027 - val_loss: 1.2426 - val_accuracy: 0.0992\n",
      "Epoch 454/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4701 - accuracy: 0.0965 - val_loss: 1.2454 - val_accuracy: 0.0997\n",
      "Epoch 455/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4643 - accuracy: 0.0991 - val_loss: 1.2426 - val_accuracy: 0.0917\n",
      "Epoch 456/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4751 - accuracy: 0.0946 - val_loss: 1.2311 - val_accuracy: 0.0932\n",
      "Epoch 457/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4668 - accuracy: 0.1026 - val_loss: 1.2222 - val_accuracy: 0.0892\n",
      "Epoch 458/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4677 - accuracy: 0.0988 - val_loss: 1.2162 - val_accuracy: 0.0912\n",
      "Epoch 459/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4644 - accuracy: 0.1067 - val_loss: 1.2252 - val_accuracy: 0.0907\n",
      "Epoch 460/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4705 - accuracy: 0.1011 - val_loss: 1.2003 - val_accuracy: 0.0957\n",
      "Epoch 461/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4650 - accuracy: 0.1031 - val_loss: 1.1786 - val_accuracy: 0.0912\n",
      "Epoch 462/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4690 - accuracy: 0.0966 - val_loss: 1.1884 - val_accuracy: 0.0847\n",
      "Epoch 463/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4613 - accuracy: 0.0992 - val_loss: 1.1953 - val_accuracy: 0.0887\n",
      "Epoch 464/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4641 - accuracy: 0.0966 - val_loss: 1.1978 - val_accuracy: 0.0872\n",
      "Epoch 465/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4626 - accuracy: 0.1022 - val_loss: 1.1945 - val_accuracy: 0.0842\n",
      "Epoch 466/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4642 - accuracy: 0.1054 - val_loss: 1.2245 - val_accuracy: 0.0937\n",
      "Epoch 467/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4681 - accuracy: 0.1068 - val_loss: 1.2094 - val_accuracy: 0.0932\n",
      "Epoch 468/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4702 - accuracy: 0.1047 - val_loss: 1.2017 - val_accuracy: 0.1052\n",
      "Epoch 469/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4653 - accuracy: 0.0998 - val_loss: 1.2243 - val_accuracy: 0.0922\n",
      "Epoch 470/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4722 - accuracy: 0.1073 - val_loss: 1.2149 - val_accuracy: 0.0942\n",
      "Epoch 471/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4885 - accuracy: 0.0975 - val_loss: 1.2257 - val_accuracy: 0.0907\n",
      "Epoch 472/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4748 - accuracy: 0.1005 - val_loss: 1.3452 - val_accuracy: 0.0907\n",
      "Epoch 473/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4644 - accuracy: 0.0917 - val_loss: 1.1745 - val_accuracy: 0.0892\n",
      "Epoch 474/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4643 - accuracy: 0.1033 - val_loss: 1.2330 - val_accuracy: 0.0922\n",
      "Epoch 475/500\n",
      "8023/8023 [==============================] - 8s 998us/step - loss: 1.4665 - accuracy: 0.0981 - val_loss: 1.2368 - val_accuracy: 0.1052\n",
      "Epoch 476/500\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 1.4679 - accuracy: 0.0961 - val_loss: 1.2321 - val_accuracy: 0.0897\n",
      "Epoch 477/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4794 - accuracy: 0.1023 - val_loss: 1.2199 - val_accuracy: 0.0892\n",
      "Epoch 478/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4677 - accuracy: 0.1012 - val_loss: 1.1849 - val_accuracy: 0.0962\n",
      "Epoch 479/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4705 - accuracy: 0.0980 - val_loss: 1.1965 - val_accuracy: 0.0887\n",
      "Epoch 480/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4775 - accuracy: 0.1006 - val_loss: 1.2559 - val_accuracy: 0.0872\n",
      "Epoch 481/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4625 - accuracy: 0.0927 - val_loss: 1.2161 - val_accuracy: 0.1022\n",
      "Epoch 482/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4707 - accuracy: 0.1017 - val_loss: 1.2252 - val_accuracy: 0.0927\n",
      "Epoch 483/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4709 - accuracy: 0.0981 - val_loss: 1.2727 - val_accuracy: 0.0947\n",
      "Epoch 484/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4680 - accuracy: 0.1028 - val_loss: 1.1845 - val_accuracy: 0.0902\n",
      "Epoch 485/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4694 - accuracy: 0.1002 - val_loss: 1.2375 - val_accuracy: 0.0917\n",
      "Epoch 486/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4696 - accuracy: 0.1028 - val_loss: 1.1907 - val_accuracy: 0.1057\n",
      "Epoch 487/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4649 - accuracy: 0.1001 - val_loss: 1.2081 - val_accuracy: 0.0862\n",
      "Epoch 488/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4620 - accuracy: 0.1015 - val_loss: 1.2187 - val_accuracy: 0.0877\n",
      "Epoch 489/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4646 - accuracy: 0.0965 - val_loss: 1.1954 - val_accuracy: 0.1052\n",
      "Epoch 490/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4670 - accuracy: 0.0988 - val_loss: 1.1911 - val_accuracy: 0.0912\n",
      "Epoch 491/500\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 1.4759 - accuracy: 0.0965 - val_loss: 1.1892 - val_accuracy: 0.0892\n",
      "Epoch 492/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4636 - accuracy: 0.1021 - val_loss: 1.1839 - val_accuracy: 0.1052\n",
      "Epoch 493/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4670 - accuracy: 0.1054 - val_loss: 1.1894 - val_accuracy: 0.0867\n",
      "Epoch 494/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4639 - accuracy: 0.0961 - val_loss: 1.2053 - val_accuracy: 0.1052\n",
      "Epoch 495/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4646 - accuracy: 0.1022 - val_loss: 1.1695 - val_accuracy: 0.1012\n",
      "Epoch 496/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4661 - accuracy: 0.0961 - val_loss: 1.2365 - val_accuracy: 0.0912\n",
      "Epoch 497/500\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 1.4616 - accuracy: 0.0965 - val_loss: 1.1910 - val_accuracy: 0.0892\n",
      "Epoch 498/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4674 - accuracy: 0.0985 - val_loss: 1.2302 - val_accuracy: 0.0907\n",
      "Epoch 499/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4625 - accuracy: 0.0966 - val_loss: 1.2109 - val_accuracy: 0.0957\n",
      "Epoch 500/500\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 1.4621 - accuracy: 0.1017 - val_loss: 1.1959 - val_accuracy: 0.0907\n",
      "the NMAE for epochs= 500 is: ReadsAvg    0.028811\n",
      "ReadsAvg    0.028813\n",
      "ReadsAvg    0.028813\n",
      "ReadsAvg    0.028813\n",
      "ReadsAvg    0.028814\n",
      "ReadsAvg    0.028814\n",
      "ReadsAvg    0.028815\n",
      "ReadsAvg    0.028815\n",
      "ReadsAvg    0.028816\n",
      "ReadsAvg    0.028817\n",
      "ReadsAvg    0.028818\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#commenting everything since tuning is done\n",
    "# tuning batch size\n",
    "#Rnn_xtrain, Rnn_ytrain = fetch_data(X_train, Y_train, 11, 1)\n",
    "#Rnn_xtest, Rnn_ytest = fetch_data(X_test, Y_test, 11, 1)\n",
    "#Rnn_model = LSTM_tune_bs(Rnn_xtrain, Rnn_ytrain,Rnn_xtest, Rnn_ytest, 1, 11, 16)\n",
    "#tuning epochs\n",
    "#Rnn_xtrain, Rnn_ytrain = fetch_data(X_train, Y_train, 11, 1)\n",
    "#Rnn_xtest, Rnn_ytest = fetch_data(X_test, Y_test, 11, 1)\n",
    "#Rnn_model = LSTM_tune_epochs(Rnn_xtrain, Rnn_ytrain,Rnn_xtest, Rnn_ytest, 1, 11, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8023 samples, validate on 2006 samples\n",
      "Epoch 1/50\n",
      "8023/8023 [==============================] - 8s 1ms/step - loss: 17.6802 - accuracy: 0.0889 - val_loss: 1.8815 - val_accuracy: 0.0982\n",
      "Epoch 2/50\n",
      "8023/8023 [==============================] - 4s 499us/step - loss: 2.1614 - accuracy: 0.0869 - val_loss: 1.2673 - val_accuracy: 0.0857\n",
      "Epoch 3/50\n",
      "8023/8023 [==============================] - 4s 507us/step - loss: 1.7635 - accuracy: 0.0906 - val_loss: 1.2762 - val_accuracy: 0.0837\n",
      "Epoch 4/50\n",
      "8023/8023 [==============================] - 4s 484us/step - loss: 1.7234 - accuracy: 0.0949 - val_loss: 1.1912 - val_accuracy: 0.0887\n",
      "Epoch 5/50\n",
      "8023/8023 [==============================] - 4s 481us/step - loss: 1.7174 - accuracy: 0.0934 - val_loss: 1.2055 - val_accuracy: 0.0872\n",
      "Epoch 6/50\n",
      "8023/8023 [==============================] - 4s 438us/step - loss: 1.7059 - accuracy: 0.0905 - val_loss: 1.2115 - val_accuracy: 0.0922\n",
      "Epoch 7/50\n",
      "8023/8023 [==============================] - 3s 428us/step - loss: 1.6720 - accuracy: 0.0841 - val_loss: 1.1966 - val_accuracy: 0.0872\n",
      "Epoch 8/50\n",
      "8023/8023 [==============================] - 4s 461us/step - loss: 1.6530 - accuracy: 0.0949 - val_loss: 1.2074 - val_accuracy: 0.0877\n",
      "Epoch 9/50\n",
      "8023/8023 [==============================] - 4s 454us/step - loss: 1.6588 - accuracy: 0.0865 - val_loss: 1.3137 - val_accuracy: 0.0862\n",
      "Epoch 10/50\n",
      "8023/8023 [==============================] - 4s 449us/step - loss: 1.6581 - accuracy: 0.0922 - val_loss: 1.2452 - val_accuracy: 0.0922\n",
      "Epoch 11/50\n",
      "8023/8023 [==============================] - 4s 442us/step - loss: 1.6581 - accuracy: 0.0940 - val_loss: 1.3220 - val_accuracy: 0.0917\n",
      "Epoch 12/50\n",
      "8023/8023 [==============================] - 3s 421us/step - loss: 1.6349 - accuracy: 0.0993 - val_loss: 1.1730 - val_accuracy: 0.0897\n",
      "Epoch 13/50\n",
      "8023/8023 [==============================] - 3s 402us/step - loss: 1.6109 - accuracy: 0.0922 - val_loss: 1.2772 - val_accuracy: 0.0932\n",
      "Epoch 14/50\n",
      "8023/8023 [==============================] - 3s 431us/step - loss: 1.6274 - accuracy: 0.0909 - val_loss: 1.2070 - val_accuracy: 0.0897\n",
      "Epoch 15/50\n",
      "8023/8023 [==============================] - 3s 418us/step - loss: 1.6040 - accuracy: 0.0957 - val_loss: 1.2059 - val_accuracy: 0.0902\n",
      "Epoch 16/50\n",
      "8023/8023 [==============================] - 3s 427us/step - loss: 1.5913 - accuracy: 0.0939 - val_loss: 1.2233 - val_accuracy: 0.0857\n",
      "Epoch 17/50\n",
      "8023/8023 [==============================] - 3s 406us/step - loss: 1.6052 - accuracy: 0.0929 - val_loss: 1.2713 - val_accuracy: 0.0927\n",
      "Epoch 18/50\n",
      "8023/8023 [==============================] - 3s 392us/step - loss: 1.5871 - accuracy: 0.0896 - val_loss: 1.1892 - val_accuracy: 0.0967\n",
      "Epoch 19/50\n",
      "8023/8023 [==============================] - 3s 411us/step - loss: 1.5883 - accuracy: 0.0930 - val_loss: 1.1953 - val_accuracy: 0.0912\n",
      "Epoch 20/50\n",
      "8023/8023 [==============================] - 3s 405us/step - loss: 1.5667 - accuracy: 0.0976 - val_loss: 1.1683 - val_accuracy: 0.0872\n",
      "Epoch 21/50\n",
      "8023/8023 [==============================] - 3s 407us/step - loss: 1.5711 - accuracy: 0.0961 - val_loss: 1.1821 - val_accuracy: 0.0852\n",
      "Epoch 22/50\n",
      "8023/8023 [==============================] - 3s 406us/step - loss: 1.5562 - accuracy: 0.0985 - val_loss: 1.2054 - val_accuracy: 0.0852\n",
      "Epoch 23/50\n",
      "8023/8023 [==============================] - 3s 419us/step - loss: 1.5559 - accuracy: 0.0962 - val_loss: 1.2074 - val_accuracy: 0.0942\n",
      "Epoch 24/50\n",
      "8023/8023 [==============================] - 3s 404us/step - loss: 1.5485 - accuracy: 0.0990 - val_loss: 1.1966 - val_accuracy: 0.0857\n",
      "Epoch 25/50\n",
      "8023/8023 [==============================] - 3s 393us/step - loss: 1.5582 - accuracy: 0.0967 - val_loss: 1.2018 - val_accuracy: 0.0912\n",
      "Epoch 26/50\n",
      "8023/8023 [==============================] - 4s 499us/step - loss: 1.5605 - accuracy: 0.0946 - val_loss: 1.1773 - val_accuracy: 0.1052\n",
      "Epoch 27/50\n",
      "8023/8023 [==============================] - 3s 409us/step - loss: 1.5504 - accuracy: 0.0960 - val_loss: 1.1777 - val_accuracy: 0.0887\n",
      "Epoch 28/50\n",
      "8023/8023 [==============================] - 3s 403us/step - loss: 1.5437 - accuracy: 0.0947 - val_loss: 1.2031 - val_accuracy: 0.0892\n",
      "Epoch 29/50\n",
      "8023/8023 [==============================] - 4s 444us/step - loss: 1.5303 - accuracy: 0.0922 - val_loss: 1.1779 - val_accuracy: 0.0917\n",
      "Epoch 30/50\n",
      "8023/8023 [==============================] - 4s 446us/step - loss: 1.5300 - accuracy: 0.0905 - val_loss: 1.1792 - val_accuracy: 0.0897\n",
      "Epoch 31/50\n",
      "8023/8023 [==============================] - 4s 438us/step - loss: 1.5281 - accuracy: 0.0915 - val_loss: 1.1745 - val_accuracy: 0.0877\n",
      "Epoch 32/50\n",
      "8023/8023 [==============================] - 4s 437us/step - loss: 1.5398 - accuracy: 0.0936 - val_loss: 1.1688 - val_accuracy: 0.0997\n",
      "Epoch 33/50\n",
      "8023/8023 [==============================] - 3s 425us/step - loss: 1.5311 - accuracy: 0.0987 - val_loss: 1.1889 - val_accuracy: 0.0892\n",
      "Epoch 34/50\n",
      "8023/8023 [==============================] - 3s 404us/step - loss: 1.5298 - accuracy: 0.0952 - val_loss: 1.2210 - val_accuracy: 0.0997\n",
      "Epoch 35/50\n",
      "8023/8023 [==============================] - 3s 420us/step - loss: 1.5294 - accuracy: 0.0991 - val_loss: 1.1880 - val_accuracy: 0.0907\n",
      "Epoch 36/50\n",
      "8023/8023 [==============================] - 4s 457us/step - loss: 1.5330 - accuracy: 0.0978 - val_loss: 1.1854 - val_accuracy: 0.0867\n",
      "Epoch 37/50\n",
      "8023/8023 [==============================] - 3s 422us/step - loss: 1.5280 - accuracy: 0.0958 - val_loss: 1.2087 - val_accuracy: 0.0882\n",
      "Epoch 38/50\n",
      "8023/8023 [==============================] - 3s 431us/step - loss: 1.5412 - accuracy: 0.0925 - val_loss: 1.2418 - val_accuracy: 0.0967\n",
      "Epoch 39/50\n",
      "8023/8023 [==============================] - 3s 408us/step - loss: 1.5300 - accuracy: 0.0983 - val_loss: 1.1623 - val_accuracy: 0.0882\n",
      "Epoch 40/50\n",
      "8023/8023 [==============================] - 3s 429us/step - loss: 1.5279 - accuracy: 0.0973 - val_loss: 1.1734 - val_accuracy: 0.0952\n",
      "Epoch 41/50\n",
      "8023/8023 [==============================] - 4s 437us/step - loss: 1.5378 - accuracy: 0.0968 - val_loss: 1.1848 - val_accuracy: 0.0997\n",
      "Epoch 42/50\n",
      "8023/8023 [==============================] - 3s 421us/step - loss: 1.5213 - accuracy: 0.0947 - val_loss: 1.1715 - val_accuracy: 0.0947\n",
      "Epoch 43/50\n",
      "8023/8023 [==============================] - 3s 399us/step - loss: 1.5237 - accuracy: 0.0968 - val_loss: 1.2062 - val_accuracy: 0.0897\n",
      "Epoch 44/50\n",
      "8023/8023 [==============================] - 3s 406us/step - loss: 1.5255 - accuracy: 0.0966 - val_loss: 1.2131 - val_accuracy: 0.0882\n",
      "Epoch 45/50\n",
      "8023/8023 [==============================] - 3s 397us/step - loss: 1.5313 - accuracy: 0.0932 - val_loss: 1.1890 - val_accuracy: 0.0992\n",
      "Epoch 46/50\n",
      "8023/8023 [==============================] - 3s 404us/step - loss: 1.5226 - accuracy: 0.0952 - val_loss: 1.1823 - val_accuracy: 0.1037\n",
      "Epoch 47/50\n",
      "8023/8023 [==============================] - 3s 417us/step - loss: 1.5280 - accuracy: 0.0972 - val_loss: 1.1738 - val_accuracy: 0.0932\n",
      "Epoch 48/50\n",
      "8023/8023 [==============================] - 3s 393us/step - loss: 1.5261 - accuracy: 0.0911 - val_loss: 1.1751 - val_accuracy: 0.0997\n",
      "Epoch 49/50\n",
      "8023/8023 [==============================] - 3s 412us/step - loss: 1.5286 - accuracy: 0.0961 - val_loss: 1.1757 - val_accuracy: 0.0862\n",
      "Epoch 50/50\n",
      "8023/8023 [==============================] - 3s 404us/step - loss: 1.5260 - accuracy: 0.0934 - val_loss: 1.1769 - val_accuracy: 0.0937\n",
      "the NMAE for units= 10 is: ReadsAvg    0.028919\n",
      "ReadsAvg    0.028921\n",
      "ReadsAvg    0.028921\n",
      "ReadsAvg    0.028921\n",
      "ReadsAvg    0.028922\n",
      "ReadsAvg    0.028922\n",
      "ReadsAvg    0.028923\n",
      "ReadsAvg    0.028923\n",
      "ReadsAvg    0.028924\n",
      "ReadsAvg    0.028925\n",
      "ReadsAvg    0.028926\n",
      "dtype: float64\n",
      "Train on 8023 samples, validate on 2006 samples\n",
      "Epoch 1/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 13.1032 - accuracy: 0.0971 - val_loss: 1.7548 - val_accuracy: 0.0837\n",
      "Epoch 2/50\n",
      "8023/8023 [==============================] - 5s 608us/step - loss: 3.4098 - accuracy: 0.0924 - val_loss: 1.2362 - val_accuracy: 0.0927\n",
      "Epoch 3/50\n",
      "8023/8023 [==============================] - 4s 543us/step - loss: 2.0674 - accuracy: 0.0924 - val_loss: 1.2466 - val_accuracy: 0.0833\n",
      "Epoch 4/50\n",
      "8023/8023 [==============================] - 4s 497us/step - loss: 1.6317 - accuracy: 0.0960 - val_loss: 1.2058 - val_accuracy: 0.0942\n",
      "Epoch 5/50\n",
      "8023/8023 [==============================] - 4s 491us/step - loss: 1.6140 - accuracy: 0.0950 - val_loss: 1.2136 - val_accuracy: 0.0922\n",
      "Epoch 6/50\n",
      "8023/8023 [==============================] - 4s 513us/step - loss: 1.6065 - accuracy: 0.0962 - val_loss: 1.1855 - val_accuracy: 0.1052\n",
      "Epoch 7/50\n",
      "8023/8023 [==============================] - 4s 512us/step - loss: 1.5949 - accuracy: 0.1022 - val_loss: 1.2077 - val_accuracy: 0.0907\n",
      "Epoch 8/50\n",
      "8023/8023 [==============================] - 4s 534us/step - loss: 1.5627 - accuracy: 0.0956 - val_loss: 1.3293 - val_accuracy: 0.0897\n",
      "Epoch 9/50\n",
      "8023/8023 [==============================] - 4s 482us/step - loss: 1.5581 - accuracy: 0.0879 - val_loss: 1.2006 - val_accuracy: 0.0917\n",
      "Epoch 10/50\n",
      "8023/8023 [==============================] - 4s 498us/step - loss: 1.5567 - accuracy: 0.1010 - val_loss: 1.2079 - val_accuracy: 0.0927\n",
      "Epoch 11/50\n",
      "8023/8023 [==============================] - 4s 506us/step - loss: 1.5411 - accuracy: 0.0930 - val_loss: 1.3310 - val_accuracy: 0.0882\n",
      "Epoch 12/50\n",
      "8023/8023 [==============================] - 4s 528us/step - loss: 1.5429 - accuracy: 0.0963 - val_loss: 1.1883 - val_accuracy: 0.0992\n",
      "Epoch 13/50\n",
      "8023/8023 [==============================] - 4s 508us/step - loss: 1.5439 - accuracy: 0.0929 - val_loss: 1.2452 - val_accuracy: 0.0857\n",
      "Epoch 14/50\n",
      "8023/8023 [==============================] - 4s 528us/step - loss: 1.5523 - accuracy: 0.0971 - val_loss: 1.2202 - val_accuracy: 0.0922\n",
      "Epoch 15/50\n",
      "8023/8023 [==============================] - 4s 510us/step - loss: 1.5511 - accuracy: 0.0966 - val_loss: 1.2305 - val_accuracy: 0.0892\n",
      "Epoch 16/50\n",
      "8023/8023 [==============================] - 4s 542us/step - loss: 1.5430 - accuracy: 0.0950 - val_loss: 1.1887 - val_accuracy: 0.1052\n",
      "Epoch 17/50\n",
      "8023/8023 [==============================] - 4s 532us/step - loss: 1.5388 - accuracy: 0.0972 - val_loss: 1.2482 - val_accuracy: 0.0922\n",
      "Epoch 18/50\n",
      "8023/8023 [==============================] - 4s 533us/step - loss: 1.5375 - accuracy: 0.0960 - val_loss: 1.1966 - val_accuracy: 0.0902\n",
      "Epoch 19/50\n",
      "8023/8023 [==============================] - 4s 499us/step - loss: 1.5343 - accuracy: 0.0958 - val_loss: 1.1849 - val_accuracy: 0.0917\n",
      "Epoch 20/50\n",
      "8023/8023 [==============================] - 4s 516us/step - loss: 1.5541 - accuracy: 0.0906 - val_loss: 1.2244 - val_accuracy: 0.0887\n",
      "Epoch 21/50\n",
      "8023/8023 [==============================] - 4s 509us/step - loss: 1.5495 - accuracy: 0.0916 - val_loss: 1.2297 - val_accuracy: 0.0952\n",
      "Epoch 22/50\n",
      "8023/8023 [==============================] - 4s 496us/step - loss: 1.5356 - accuracy: 0.0962 - val_loss: 1.1806 - val_accuracy: 0.0997\n",
      "Epoch 23/50\n",
      "8023/8023 [==============================] - 5s 624us/step - loss: 1.5408 - accuracy: 0.0919 - val_loss: 1.1919 - val_accuracy: 0.0897\n",
      "Epoch 24/50\n",
      "8023/8023 [==============================] - 4s 539us/step - loss: 1.5407 - accuracy: 0.0931 - val_loss: 1.1879 - val_accuracy: 0.0877\n",
      "Epoch 25/50\n",
      "8023/8023 [==============================] - 4s 514us/step - loss: 1.5306 - accuracy: 0.0934 - val_loss: 1.1951 - val_accuracy: 0.0897\n",
      "Epoch 26/50\n",
      "8023/8023 [==============================] - 4s 508us/step - loss: 1.5259 - accuracy: 0.0968 - val_loss: 1.1875 - val_accuracy: 0.1117\n",
      "Epoch 27/50\n",
      "8023/8023 [==============================] - 5s 611us/step - loss: 1.5276 - accuracy: 0.0995 - val_loss: 1.1768 - val_accuracy: 0.0947\n",
      "Epoch 28/50\n",
      "8023/8023 [==============================] - 4s 524us/step - loss: 1.5256 - accuracy: 0.0921 - val_loss: 1.1723 - val_accuracy: 0.0882\n",
      "Epoch 29/50\n",
      "8023/8023 [==============================] - 4s 520us/step - loss: 1.5329 - accuracy: 0.0950 - val_loss: 1.2526 - val_accuracy: 0.0867\n",
      "Epoch 30/50\n",
      "8023/8023 [==============================] - 5s 611us/step - loss: 1.5275 - accuracy: 0.0930 - val_loss: 1.1905 - val_accuracy: 0.0902\n",
      "Epoch 31/50\n",
      "8023/8023 [==============================] - 4s 505us/step - loss: 1.5369 - accuracy: 0.0947 - val_loss: 1.1970 - val_accuracy: 0.0907\n",
      "Epoch 32/50\n",
      "8023/8023 [==============================] - 4s 512us/step - loss: 1.5454 - accuracy: 0.0968 - val_loss: 1.2381 - val_accuracy: 0.0907\n",
      "Epoch 33/50\n",
      "8023/8023 [==============================] - 5s 565us/step - loss: 1.5392 - accuracy: 0.0978 - val_loss: 1.2060 - val_accuracy: 0.0803\n",
      "Epoch 34/50\n",
      "8023/8023 [==============================] - 5s 573us/step - loss: 1.5571 - accuracy: 0.0942 - val_loss: 1.2252 - val_accuracy: 0.0957\n",
      "Epoch 35/50\n",
      "8023/8023 [==============================] - 4s 503us/step - loss: 1.5351 - accuracy: 0.0899 - val_loss: 1.1808 - val_accuracy: 0.0847\n",
      "Epoch 36/50\n",
      "8023/8023 [==============================] - 4s 523us/step - loss: 1.5341 - accuracy: 0.0949 - val_loss: 1.2090 - val_accuracy: 0.1002\n",
      "Epoch 37/50\n",
      "8023/8023 [==============================] - 5s 573us/step - loss: 1.5402 - accuracy: 0.0956 - val_loss: 1.2066 - val_accuracy: 0.1022\n",
      "Epoch 38/50\n",
      "8023/8023 [==============================] - 4s 490us/step - loss: 1.5237 - accuracy: 0.0853 - val_loss: 1.1851 - val_accuracy: 0.0922\n",
      "Epoch 39/50\n",
      "8023/8023 [==============================] - 4s 554us/step - loss: 1.5271 - accuracy: 0.0993 - val_loss: 1.2075 - val_accuracy: 0.0987\n",
      "Epoch 40/50\n",
      "8023/8023 [==============================] - 5s 615us/step - loss: 1.5355 - accuracy: 0.0946 - val_loss: 1.1868 - val_accuracy: 0.0902\n",
      "Epoch 41/50\n",
      "8023/8023 [==============================] - 5s 563us/step - loss: 1.5281 - accuracy: 0.0940 - val_loss: 1.2176 - val_accuracy: 0.0922\n",
      "Epoch 42/50\n",
      "8023/8023 [==============================] - 4s 524us/step - loss: 1.5243 - accuracy: 0.0905 - val_loss: 1.1896 - val_accuracy: 0.0887\n",
      "Epoch 43/50\n",
      "8023/8023 [==============================] - 4s 501us/step - loss: 1.5295 - accuracy: 0.0937 - val_loss: 1.2012 - val_accuracy: 0.1047\n",
      "Epoch 44/50\n",
      "8023/8023 [==============================] - 4s 490us/step - loss: 1.5314 - accuracy: 0.0947 - val_loss: 1.1828 - val_accuracy: 0.1102\n",
      "Epoch 45/50\n",
      "8023/8023 [==============================] - 4s 495us/step - loss: 1.5312 - accuracy: 0.0952 - val_loss: 1.1865 - val_accuracy: 0.0897\n",
      "Epoch 46/50\n",
      "8023/8023 [==============================] - 4s 478us/step - loss: 1.5207 - accuracy: 0.0963 - val_loss: 1.2025 - val_accuracy: 0.0892\n",
      "Epoch 47/50\n",
      "8023/8023 [==============================] - 4s 505us/step - loss: 1.5181 - accuracy: 0.0947 - val_loss: 1.2324 - val_accuracy: 0.1097\n",
      "Epoch 48/50\n",
      "8023/8023 [==============================] - 4s 488us/step - loss: 1.5225 - accuracy: 0.0927 - val_loss: 1.2533 - val_accuracy: 0.0922\n",
      "Epoch 49/50\n",
      "8023/8023 [==============================] - 4s 499us/step - loss: 1.5176 - accuracy: 0.0952 - val_loss: 1.1715 - val_accuracy: 0.0917\n",
      "Epoch 50/50\n",
      "8023/8023 [==============================] - 4s 482us/step - loss: 1.5192 - accuracy: 0.0907 - val_loss: 1.1956 - val_accuracy: 0.0902\n",
      "the NMAE for units= 50 is: ReadsAvg    0.028585\n",
      "ReadsAvg    0.028587\n",
      "ReadsAvg    0.028587\n",
      "ReadsAvg    0.028587\n",
      "ReadsAvg    0.028588\n",
      "ReadsAvg    0.028588\n",
      "ReadsAvg    0.028589\n",
      "ReadsAvg    0.028590\n",
      "ReadsAvg    0.028590\n",
      "ReadsAvg    0.028591\n",
      "ReadsAvg    0.028592\n",
      "dtype: float64\n",
      "Train on 8023 samples, validate on 2006 samples\n",
      "Epoch 1/50\n",
      "8023/8023 [==============================] - 11s 1ms/step - loss: 11.4648 - accuracy: 0.0915 - val_loss: 1.3867 - val_accuracy: 0.0897\n",
      "Epoch 2/50\n",
      "8023/8023 [==============================] - 5s 639us/step - loss: 3.2017 - accuracy: 0.0851 - val_loss: 1.3107 - val_accuracy: 0.0962\n",
      "Epoch 3/50\n",
      "8023/8023 [==============================] - 5s 610us/step - loss: 2.2468 - accuracy: 0.0926 - val_loss: 1.2518 - val_accuracy: 0.0882\n",
      "Epoch 4/50\n",
      "8023/8023 [==============================] - 5s 599us/step - loss: 1.7383 - accuracy: 0.0950 - val_loss: 1.4469 - val_accuracy: 0.0897\n",
      "Epoch 5/50\n",
      "8023/8023 [==============================] - 5s 599us/step - loss: 1.6450 - accuracy: 0.0952 - val_loss: 1.3871 - val_accuracy: 0.0987\n",
      "Epoch 6/50\n",
      "8023/8023 [==============================] - 5s 600us/step - loss: 1.5962 - accuracy: 0.0921 - val_loss: 1.3530 - val_accuracy: 0.0882\n",
      "Epoch 7/50\n",
      "8023/8023 [==============================] - 5s 624us/step - loss: 1.6118 - accuracy: 0.0921 - val_loss: 1.3344 - val_accuracy: 0.0887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "8023/8023 [==============================] - 5s 635us/step - loss: 1.5744 - accuracy: 0.0861 - val_loss: 1.2141 - val_accuracy: 0.0912\n",
      "Epoch 9/50\n",
      "8023/8023 [==============================] - 5s 588us/step - loss: 1.5769 - accuracy: 0.0939 - val_loss: 1.2151 - val_accuracy: 0.0872\n",
      "Epoch 10/50\n",
      "8023/8023 [==============================] - 5s 639us/step - loss: 1.5705 - accuracy: 0.0920 - val_loss: 1.2153 - val_accuracy: 0.0907\n",
      "Epoch 11/50\n",
      "8023/8023 [==============================] - 5s 641us/step - loss: 1.5790 - accuracy: 0.0939 - val_loss: 1.2691 - val_accuracy: 0.1077\n",
      "Epoch 12/50\n",
      "8023/8023 [==============================] - 5s 611us/step - loss: 1.5787 - accuracy: 0.0939 - val_loss: 1.1857 - val_accuracy: 0.0897\n",
      "Epoch 13/50\n",
      "8023/8023 [==============================] - 5s 610us/step - loss: 1.5623 - accuracy: 0.0935 - val_loss: 1.2020 - val_accuracy: 0.0897\n",
      "Epoch 14/50\n",
      "8023/8023 [==============================] - 5s 600us/step - loss: 1.5526 - accuracy: 0.0912 - val_loss: 1.2222 - val_accuracy: 0.0857\n",
      "Epoch 15/50\n",
      "8023/8023 [==============================] - 5s 599us/step - loss: 1.5883 - accuracy: 0.0914 - val_loss: 1.1959 - val_accuracy: 0.0907\n",
      "Epoch 16/50\n",
      "8023/8023 [==============================] - 5s 596us/step - loss: 1.5550 - accuracy: 0.0882 - val_loss: 1.1944 - val_accuracy: 0.1052\n",
      "Epoch 17/50\n",
      "8023/8023 [==============================] - 5s 601us/step - loss: 1.5689 - accuracy: 0.0902 - val_loss: 1.2349 - val_accuracy: 0.0907\n",
      "Epoch 18/50\n",
      "8023/8023 [==============================] - 5s 581us/step - loss: 1.5599 - accuracy: 0.0892 - val_loss: 1.2657 - val_accuracy: 0.1007\n",
      "Epoch 19/50\n",
      "8023/8023 [==============================] - 5s 610us/step - loss: 1.5697 - accuracy: 0.0887 - val_loss: 1.2336 - val_accuracy: 0.0897\n",
      "Epoch 20/50\n",
      "8023/8023 [==============================] - 5s 590us/step - loss: 1.5444 - accuracy: 0.0922 - val_loss: 1.1986 - val_accuracy: 0.0922\n",
      "Epoch 21/50\n",
      "8023/8023 [==============================] - 5s 594us/step - loss: 1.5411 - accuracy: 0.0968 - val_loss: 1.2321 - val_accuracy: 0.0912\n",
      "Epoch 22/50\n",
      "8023/8023 [==============================] - 5s 602us/step - loss: 1.5408 - accuracy: 0.0940 - val_loss: 1.2198 - val_accuracy: 0.0912\n",
      "Epoch 23/50\n",
      "8023/8023 [==============================] - 5s 597us/step - loss: 1.5345 - accuracy: 0.0976 - val_loss: 1.1986 - val_accuracy: 0.0892\n",
      "Epoch 24/50\n",
      "8023/8023 [==============================] - 5s 658us/step - loss: 1.5394 - accuracy: 0.0952 - val_loss: 1.1816 - val_accuracy: 0.0937\n",
      "Epoch 25/50\n",
      "8023/8023 [==============================] - 5s 592us/step - loss: 1.5366 - accuracy: 0.0956 - val_loss: 1.2104 - val_accuracy: 0.0847\n",
      "Epoch 26/50\n",
      "8023/8023 [==============================] - 5s 593us/step - loss: 1.5314 - accuracy: 0.0917 - val_loss: 1.2483 - val_accuracy: 0.0852\n",
      "Epoch 27/50\n",
      "8023/8023 [==============================] - 5s 602us/step - loss: 1.5324 - accuracy: 0.0941 - val_loss: 1.1942 - val_accuracy: 0.0907\n",
      "Epoch 28/50\n",
      "8023/8023 [==============================] - 5s 575us/step - loss: 1.5394 - accuracy: 0.0929 - val_loss: 1.2051 - val_accuracy: 0.0877\n",
      "Epoch 29/50\n",
      "8023/8023 [==============================] - 5s 650us/step - loss: 1.5274 - accuracy: 0.1011 - val_loss: 1.2839 - val_accuracy: 0.0857\n",
      "Epoch 30/50\n",
      "8023/8023 [==============================] - 5s 645us/step - loss: 1.5366 - accuracy: 0.0921 - val_loss: 1.2814 - val_accuracy: 0.1097\n",
      "Epoch 31/50\n",
      "8023/8023 [==============================] - 5s 629us/step - loss: 1.5245 - accuracy: 0.0976 - val_loss: 1.2742 - val_accuracy: 0.0942\n",
      "Epoch 32/50\n",
      "8023/8023 [==============================] - 5s 644us/step - loss: 1.5433 - accuracy: 0.0962 - val_loss: 1.2508 - val_accuracy: 0.0942\n",
      "Epoch 33/50\n",
      "8023/8023 [==============================] - 5s 613us/step - loss: 1.5454 - accuracy: 0.0849 - val_loss: 1.2400 - val_accuracy: 0.1002\n",
      "Epoch 34/50\n",
      "8023/8023 [==============================] - 6s 705us/step - loss: 1.5381 - accuracy: 0.0917 - val_loss: 1.2065 - val_accuracy: 0.0977\n",
      "Epoch 35/50\n",
      "8023/8023 [==============================] - 5s 648us/step - loss: 1.5307 - accuracy: 0.0921 - val_loss: 1.1777 - val_accuracy: 0.0882\n",
      "Epoch 36/50\n",
      "8023/8023 [==============================] - 5s 614us/step - loss: 1.5343 - accuracy: 0.0957 - val_loss: 1.1980 - val_accuracy: 0.0837\n",
      "Epoch 37/50\n",
      "8023/8023 [==============================] - 5s 650us/step - loss: 1.5308 - accuracy: 0.0937 - val_loss: 1.2543 - val_accuracy: 0.0852\n",
      "Epoch 38/50\n",
      "8023/8023 [==============================] - 5s 678us/step - loss: 1.5453 - accuracy: 0.0924 - val_loss: 1.1899 - val_accuracy: 0.0847\n",
      "Epoch 39/50\n",
      "8023/8023 [==============================] - 5s 604us/step - loss: 1.5336 - accuracy: 0.0833 - val_loss: 1.2745 - val_accuracy: 0.0872\n",
      "Epoch 40/50\n",
      "8023/8023 [==============================] - 5s 683us/step - loss: 1.5470 - accuracy: 0.0972 - val_loss: 1.1996 - val_accuracy: 0.0917\n",
      "Epoch 41/50\n",
      "8023/8023 [==============================] - 6s 728us/step - loss: 1.5344 - accuracy: 0.0921 - val_loss: 1.1985 - val_accuracy: 0.0912\n",
      "Epoch 42/50\n",
      "8023/8023 [==============================] - 5s 634us/step - loss: 1.5374 - accuracy: 0.0982 - val_loss: 1.2176 - val_accuracy: 0.0862\n",
      "Epoch 43/50\n",
      "8023/8023 [==============================] - 5s 662us/step - loss: 1.5468 - accuracy: 0.0982 - val_loss: 1.2240 - val_accuracy: 0.0852\n",
      "Epoch 44/50\n",
      "8023/8023 [==============================] - 5s 606us/step - loss: 1.5365 - accuracy: 0.0934 - val_loss: 1.1931 - val_accuracy: 0.0897\n",
      "Epoch 45/50\n",
      "8023/8023 [==============================] - 5s 615us/step - loss: 1.5371 - accuracy: 0.0952 - val_loss: 1.2069 - val_accuracy: 0.0857\n",
      "Epoch 46/50\n",
      "8023/8023 [==============================] - 5s 647us/step - loss: 1.5607 - accuracy: 0.0965 - val_loss: 1.2431 - val_accuracy: 0.0907\n",
      "Epoch 47/50\n",
      "8023/8023 [==============================] - 5s 565us/step - loss: 1.5314 - accuracy: 0.0952 - val_loss: 1.2672 - val_accuracy: 0.1092\n",
      "Epoch 48/50\n",
      "8023/8023 [==============================] - 6s 694us/step - loss: 1.5309 - accuracy: 0.0935 - val_loss: 1.1857 - val_accuracy: 0.0982\n",
      "Epoch 49/50\n",
      "8023/8023 [==============================] - 5s 625us/step - loss: 1.5230 - accuracy: 0.0945 - val_loss: 1.2032 - val_accuracy: 0.0877\n",
      "Epoch 50/50\n",
      "8023/8023 [==============================] - 5s 617us/step - loss: 1.5298 - accuracy: 0.0926 - val_loss: 1.2935 - val_accuracy: 0.0922\n",
      "the NMAE for units= 80 is: ReadsAvg    0.029880\n",
      "ReadsAvg    0.029881\n",
      "ReadsAvg    0.029882\n",
      "ReadsAvg    0.029882\n",
      "ReadsAvg    0.029883\n",
      "ReadsAvg    0.029883\n",
      "ReadsAvg    0.029883\n",
      "ReadsAvg    0.029884\n",
      "ReadsAvg    0.029885\n",
      "ReadsAvg    0.029886\n",
      "ReadsAvg    0.029887\n",
      "dtype: float64\n",
      "Train on 8023 samples, validate on 2006 samples\n",
      "Epoch 1/50\n",
      "8023/8023 [==============================] - 13s 2ms/step - loss: 10.2356 - accuracy: 0.0863 - val_loss: 2.0076 - val_accuracy: 0.0877\n",
      "Epoch 2/50\n",
      "8023/8023 [==============================] - 7s 864us/step - loss: 3.3625 - accuracy: 0.0954 - val_loss: 3.3042 - val_accuracy: 0.0897\n",
      "Epoch 3/50\n",
      "8023/8023 [==============================] - 7s 879us/step - loss: 2.7276 - accuracy: 0.0954 - val_loss: 1.2502 - val_accuracy: 0.1097\n",
      "Epoch 4/50\n",
      "8023/8023 [==============================] - 7s 869us/step - loss: 2.2595 - accuracy: 0.0990 - val_loss: 1.2151 - val_accuracy: 0.0877\n",
      "Epoch 5/50\n",
      "8023/8023 [==============================] - 7s 910us/step - loss: 1.7804 - accuracy: 0.0966 - val_loss: 1.6049 - val_accuracy: 0.0897\n",
      "Epoch 6/50\n",
      "8023/8023 [==============================] - 7s 843us/step - loss: 1.6470 - accuracy: 0.0950 - val_loss: 1.2072 - val_accuracy: 0.0977\n",
      "Epoch 7/50\n",
      "8023/8023 [==============================] - 7s 829us/step - loss: 1.6085 - accuracy: 0.0957 - val_loss: 1.3308 - val_accuracy: 0.0902\n",
      "Epoch 8/50\n",
      "8023/8023 [==============================] - 7s 888us/step - loss: 1.5853 - accuracy: 0.0941 - val_loss: 1.2768 - val_accuracy: 0.0912\n",
      "Epoch 9/50\n",
      "8023/8023 [==============================] - 7s 851us/step - loss: 1.5788 - accuracy: 0.0955 - val_loss: 1.2447 - val_accuracy: 0.0837\n",
      "Epoch 10/50\n",
      "8023/8023 [==============================] - 7s 858us/step - loss: 1.6282 - accuracy: 0.0991 - val_loss: 1.2166 - val_accuracy: 0.0902\n",
      "Epoch 11/50\n",
      "8023/8023 [==============================] - 7s 812us/step - loss: 1.5704 - accuracy: 0.0940 - val_loss: 1.2740 - val_accuracy: 0.0977\n",
      "Epoch 12/50\n",
      "8023/8023 [==============================] - 7s 828us/step - loss: 1.5903 - accuracy: 0.1023 - val_loss: 1.1925 - val_accuracy: 0.0962\n",
      "Epoch 13/50\n",
      "8023/8023 [==============================] - 7s 844us/step - loss: 1.5833 - accuracy: 0.0950 - val_loss: 1.2169 - val_accuracy: 0.1097\n",
      "Epoch 14/50\n",
      "8023/8023 [==============================] - 7s 890us/step - loss: 1.5777 - accuracy: 0.1017 - val_loss: 1.2106 - val_accuracy: 0.0947\n",
      "Epoch 15/50\n",
      "8023/8023 [==============================] - 7s 844us/step - loss: 1.5622 - accuracy: 0.0955 - val_loss: 1.2178 - val_accuracy: 0.0902\n",
      "Epoch 16/50\n",
      "8023/8023 [==============================] - 7s 891us/step - loss: 1.5485 - accuracy: 0.0965 - val_loss: 1.1891 - val_accuracy: 0.0882\n",
      "Epoch 17/50\n",
      "8023/8023 [==============================] - 7s 887us/step - loss: 1.5598 - accuracy: 0.0981 - val_loss: 1.2506 - val_accuracy: 0.0982\n",
      "Epoch 18/50\n",
      "8023/8023 [==============================] - 7s 870us/step - loss: 1.5494 - accuracy: 0.0977 - val_loss: 1.1897 - val_accuracy: 0.0967\n",
      "Epoch 19/50\n",
      "8023/8023 [==============================] - 7s 893us/step - loss: 1.5495 - accuracy: 0.0915 - val_loss: 1.2132 - val_accuracy: 0.0887\n",
      "Epoch 20/50\n",
      "8023/8023 [==============================] - 6s 803us/step - loss: 1.5478 - accuracy: 0.0951 - val_loss: 1.2400 - val_accuracy: 0.0917\n",
      "Epoch 21/50\n",
      "8023/8023 [==============================] - 7s 904us/step - loss: 1.5599 - accuracy: 0.0995 - val_loss: 1.1934 - val_accuracy: 0.0997\n",
      "Epoch 22/50\n",
      "8023/8023 [==============================] - 7s 893us/step - loss: 1.5517 - accuracy: 0.0876 - val_loss: 1.1879 - val_accuracy: 0.0897\n",
      "Epoch 23/50\n",
      "8023/8023 [==============================] - 7s 873us/step - loss: 1.5451 - accuracy: 0.1011 - val_loss: 1.2066 - val_accuracy: 0.1047\n",
      "Epoch 24/50\n",
      "8023/8023 [==============================] - 7s 919us/step - loss: 1.5445 - accuracy: 0.0965 - val_loss: 1.2027 - val_accuracy: 0.0867\n",
      "Epoch 25/50\n",
      "8023/8023 [==============================] - 7s 910us/step - loss: 1.5547 - accuracy: 0.0963 - val_loss: 1.2590 - val_accuracy: 0.0852\n",
      "Epoch 26/50\n",
      "8023/8023 [==============================] - 7s 815us/step - loss: 1.5441 - accuracy: 0.0992 - val_loss: 1.2512 - val_accuracy: 0.0907\n",
      "Epoch 27/50\n",
      "8023/8023 [==============================] - 8s 966us/step - loss: 1.5392 - accuracy: 0.0962 - val_loss: 1.2163 - val_accuracy: 0.0857\n",
      "Epoch 28/50\n",
      "8023/8023 [==============================] - 8s 944us/step - loss: 1.5443 - accuracy: 0.0965 - val_loss: 1.2515 - val_accuracy: 0.1052\n",
      "Epoch 29/50\n",
      "8023/8023 [==============================] - 7s 904us/step - loss: 1.5429 - accuracy: 0.0962 - val_loss: 1.2189 - val_accuracy: 0.0922\n",
      "Epoch 30/50\n",
      "8023/8023 [==============================] - 7s 850us/step - loss: 1.5519 - accuracy: 0.0917 - val_loss: 1.2058 - val_accuracy: 0.0972\n",
      "Epoch 31/50\n",
      "8023/8023 [==============================] - 7s 893us/step - loss: 1.5710 - accuracy: 0.0960 - val_loss: 1.2503 - val_accuracy: 0.1077\n",
      "Epoch 32/50\n",
      "8023/8023 [==============================] - 7s 922us/step - loss: 1.5396 - accuracy: 0.0982 - val_loss: 1.1936 - val_accuracy: 0.0892\n",
      "Epoch 33/50\n",
      "8023/8023 [==============================] - 7s 913us/step - loss: 1.5474 - accuracy: 0.0915 - val_loss: 1.1744 - val_accuracy: 0.0877\n",
      "Epoch 34/50\n",
      "8023/8023 [==============================] - 7s 902us/step - loss: 1.5487 - accuracy: 0.0941 - val_loss: 1.2018 - val_accuracy: 0.0927\n",
      "Epoch 35/50\n",
      "8023/8023 [==============================] - 7s 934us/step - loss: 1.5384 - accuracy: 0.0946 - val_loss: 1.1975 - val_accuracy: 0.0902\n",
      "Epoch 36/50\n",
      "8023/8023 [==============================] - 7s 890us/step - loss: 1.5332 - accuracy: 0.0895 - val_loss: 1.2154 - val_accuracy: 0.0852\n",
      "Epoch 37/50\n",
      "8023/8023 [==============================] - 7s 869us/step - loss: 1.5493 - accuracy: 0.0886 - val_loss: 1.3112 - val_accuracy: 0.0852\n",
      "Epoch 38/50\n",
      "8023/8023 [==============================] - 7s 924us/step - loss: 1.5569 - accuracy: 0.0957 - val_loss: 1.3355 - val_accuracy: 0.0852\n",
      "Epoch 39/50\n",
      "8023/8023 [==============================] - 7s 837us/step - loss: 1.5556 - accuracy: 0.0925 - val_loss: 1.2319 - val_accuracy: 0.0932\n",
      "Epoch 40/50\n",
      "8023/8023 [==============================] - 7s 866us/step - loss: 1.5392 - accuracy: 0.0904 - val_loss: 1.1962 - val_accuracy: 0.0937\n",
      "Epoch 41/50\n",
      "8023/8023 [==============================] - 7s 879us/step - loss: 1.5920 - accuracy: 0.0946 - val_loss: 1.1833 - val_accuracy: 0.0987\n",
      "Epoch 42/50\n",
      "8023/8023 [==============================] - 7s 878us/step - loss: 1.5639 - accuracy: 0.0925 - val_loss: 1.2106 - val_accuracy: 0.0887\n",
      "Epoch 43/50\n",
      "8023/8023 [==============================] - 7s 883us/step - loss: 1.5550 - accuracy: 0.0886 - val_loss: 1.2164 - val_accuracy: 0.0977\n",
      "Epoch 44/50\n",
      "8023/8023 [==============================] - 7s 902us/step - loss: 1.5362 - accuracy: 0.0951 - val_loss: 1.2298 - val_accuracy: 0.1062\n",
      "Epoch 45/50\n",
      "8023/8023 [==============================] - 7s 899us/step - loss: 1.5507 - accuracy: 0.0931 - val_loss: 1.1908 - val_accuracy: 0.0987\n",
      "Epoch 46/50\n",
      "8023/8023 [==============================] - 7s 859us/step - loss: 1.5954 - accuracy: 0.0941 - val_loss: 1.2397 - val_accuracy: 0.0997\n",
      "Epoch 47/50\n",
      "8023/8023 [==============================] - 7s 845us/step - loss: 1.5391 - accuracy: 0.0951 - val_loss: 1.2115 - val_accuracy: 0.0877\n",
      "Epoch 48/50\n",
      "8023/8023 [==============================] - 6s 762us/step - loss: 1.5846 - accuracy: 0.0946 - val_loss: 1.2166 - val_accuracy: 0.0927\n",
      "Epoch 49/50\n",
      "8023/8023 [==============================] - 6s 776us/step - loss: 1.5454 - accuracy: 0.0952 - val_loss: 1.2346 - val_accuracy: 0.0897\n",
      "Epoch 50/50\n",
      "8023/8023 [==============================] - 7s 873us/step - loss: 1.5431 - accuracy: 0.0970 - val_loss: 1.2102 - val_accuracy: 0.0877\n",
      "the NMAE for units= 128 is: ReadsAvg    0.029581\n",
      "ReadsAvg    0.029582\n",
      "ReadsAvg    0.029583\n",
      "ReadsAvg    0.029583\n",
      "ReadsAvg    0.029584\n",
      "ReadsAvg    0.029584\n",
      "ReadsAvg    0.029584\n",
      "ReadsAvg    0.029585\n",
      "ReadsAvg    0.029586\n",
      "ReadsAvg    0.029587\n",
      "ReadsAvg    0.029588\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#tuning the no. of nodes/units\n",
    "#Rnn_xtrain, Rnn_ytrain = fetch_data(X_train, Y_train, 11, 1)\n",
    "#Rnn_xtest, Rnn_ytest = fetch_data(X_test, Y_test, 11, 1)\n",
    "#Rnn_model = LSTM_tune_nodes(Rnn_xtrain, Rnn_ytrain,Rnn_xtest, Rnn_ytest, 1, 11, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8023 samples, validate on 2006 samples\n",
      "Epoch 1/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 13.2054 - accuracy: 0.0896 - val_loss: 2.1368 - val_accuracy: 0.0892\n",
      "Epoch 2/50\n",
      "8023/8023 [==============================] - 4s 518us/step - loss: 3.4839 - accuracy: 0.0849 - val_loss: 1.6236 - val_accuracy: 0.0947\n",
      "Epoch 3/50\n",
      "8023/8023 [==============================] - 4s 465us/step - loss: 2.0445 - accuracy: 0.0952 - val_loss: 1.3228 - val_accuracy: 0.0887\n",
      "Epoch 4/50\n",
      "8023/8023 [==============================] - 4s 540us/step - loss: 1.6413 - accuracy: 0.0945 - val_loss: 1.2211 - val_accuracy: 0.1012\n",
      "Epoch 5/50\n",
      "8023/8023 [==============================] - 4s 524us/step - loss: 1.5948 - accuracy: 0.0961 - val_loss: 1.2007 - val_accuracy: 0.0892\n",
      "Epoch 6/50\n",
      "8023/8023 [==============================] - 4s 475us/step - loss: 1.5837 - accuracy: 0.0947 - val_loss: 1.2307 - val_accuracy: 0.0922\n",
      "Epoch 7/50\n",
      "8023/8023 [==============================] - 4s 503us/step - loss: 1.5688 - accuracy: 0.0925 - val_loss: 1.1907 - val_accuracy: 0.0872\n",
      "Epoch 8/50\n",
      "8023/8023 [==============================] - 4s 524us/step - loss: 1.5703 - accuracy: 0.0950 - val_loss: 1.2680 - val_accuracy: 0.0897\n",
      "Epoch 9/50\n",
      "8023/8023 [==============================] - 4s 513us/step - loss: 1.5674 - accuracy: 0.0970 - val_loss: 1.2253 - val_accuracy: 0.1027\n",
      "Epoch 10/50\n",
      "8023/8023 [==============================] - 5s 567us/step - loss: 1.5803 - accuracy: 0.0970 - val_loss: 1.1920 - val_accuracy: 0.0987\n",
      "Epoch 11/50\n",
      "8023/8023 [==============================] - 5s 571us/step - loss: 1.5620 - accuracy: 0.0946 - val_loss: 1.2471 - val_accuracy: 0.0957\n",
      "Epoch 12/50\n",
      "8023/8023 [==============================] - 4s 538us/step - loss: 1.5506 - accuracy: 0.0909 - val_loss: 1.2030 - val_accuracy: 0.0992\n",
      "Epoch 13/50\n",
      "8023/8023 [==============================] - 4s 540us/step - loss: 1.5468 - accuracy: 0.0983 - val_loss: 1.2000 - val_accuracy: 0.0927\n",
      "Epoch 14/50\n",
      "8023/8023 [==============================] - 4s 542us/step - loss: 1.5471 - accuracy: 0.0968 - val_loss: 1.1914 - val_accuracy: 0.1017\n",
      "Epoch 15/50\n",
      "8023/8023 [==============================] - 4s 535us/step - loss: 1.5498 - accuracy: 0.0884 - val_loss: 1.2618 - val_accuracy: 0.0852\n",
      "Epoch 16/50\n",
      "8023/8023 [==============================] - 4s 538us/step - loss: 1.5436 - accuracy: 0.0981 - val_loss: 1.2151 - val_accuracy: 0.0917\n",
      "Epoch 17/50\n",
      "8023/8023 [==============================] - 4s 544us/step - loss: 1.5428 - accuracy: 0.0906 - val_loss: 1.2397 - val_accuracy: 0.0902\n",
      "Epoch 18/50\n",
      "8023/8023 [==============================] - 4s 539us/step - loss: 1.5423 - accuracy: 0.0919 - val_loss: 1.2512 - val_accuracy: 0.0902\n",
      "Epoch 19/50\n",
      "8023/8023 [==============================] - 4s 537us/step - loss: 1.5403 - accuracy: 0.0921 - val_loss: 1.2073 - val_accuracy: 0.0892\n",
      "Epoch 20/50\n",
      "8023/8023 [==============================] - 4s 538us/step - loss: 1.5305 - accuracy: 0.0972 - val_loss: 1.2253 - val_accuracy: 0.0877\n",
      "Epoch 21/50\n",
      "8023/8023 [==============================] - 4s 517us/step - loss: 1.5465 - accuracy: 0.0921 - val_loss: 1.1795 - val_accuracy: 0.0927\n",
      "Epoch 22/50\n",
      "8023/8023 [==============================] - 4s 551us/step - loss: 1.5311 - accuracy: 0.0988 - val_loss: 1.2168 - val_accuracy: 0.1042\n",
      "Epoch 23/50\n",
      "8023/8023 [==============================] - 4s 538us/step - loss: 1.5382 - accuracy: 0.0962 - val_loss: 1.1821 - val_accuracy: 0.0842\n",
      "Epoch 24/50\n",
      "8023/8023 [==============================] - 4s 536us/step - loss: 1.5343 - accuracy: 0.0904 - val_loss: 1.1862 - val_accuracy: 0.0922\n",
      "Epoch 25/50\n",
      "8023/8023 [==============================] - 4s 515us/step - loss: 1.5211 - accuracy: 0.0986 - val_loss: 1.1867 - val_accuracy: 0.0842\n",
      "Epoch 26/50\n",
      "8023/8023 [==============================] - 4s 543us/step - loss: 1.5230 - accuracy: 0.0886 - val_loss: 1.1846 - val_accuracy: 0.0912\n",
      "Epoch 27/50\n",
      "8023/8023 [==============================] - 4s 540us/step - loss: 1.5287 - accuracy: 0.0955 - val_loss: 1.1865 - val_accuracy: 0.0982\n",
      "Epoch 28/50\n",
      "8023/8023 [==============================] - 4s 534us/step - loss: 1.5229 - accuracy: 0.0941 - val_loss: 1.1941 - val_accuracy: 0.0857\n",
      "Epoch 29/50\n",
      "8023/8023 [==============================] - 4s 535us/step - loss: 1.5198 - accuracy: 0.0997 - val_loss: 1.2394 - val_accuracy: 0.0867\n",
      "Epoch 30/50\n",
      "8023/8023 [==============================] - 4s 534us/step - loss: 1.5168 - accuracy: 0.0946 - val_loss: 1.2946 - val_accuracy: 0.0962\n",
      "Epoch 31/50\n",
      "8023/8023 [==============================] - 5s 563us/step - loss: 1.5278 - accuracy: 0.1012 - val_loss: 1.2460 - val_accuracy: 0.0897\n",
      "Epoch 32/50\n",
      "8023/8023 [==============================] - 4s 537us/step - loss: 1.5381 - accuracy: 0.0976 - val_loss: 1.1956 - val_accuracy: 0.0907\n",
      "Epoch 33/50\n",
      "8023/8023 [==============================] - 5s 581us/step - loss: 1.5345 - accuracy: 0.0966 - val_loss: 1.2019 - val_accuracy: 0.0932\n",
      "Epoch 34/50\n",
      "8023/8023 [==============================] - 5s 591us/step - loss: 1.5273 - accuracy: 0.0978 - val_loss: 1.1880 - val_accuracy: 0.0902\n",
      "Epoch 35/50\n",
      "8023/8023 [==============================] - 4s 529us/step - loss: 1.5260 - accuracy: 0.0954 - val_loss: 1.1968 - val_accuracy: 0.0917\n",
      "Epoch 36/50\n",
      "8023/8023 [==============================] - 4s 501us/step - loss: 1.5146 - accuracy: 0.0968 - val_loss: 1.2233 - val_accuracy: 0.0877\n",
      "Epoch 37/50\n",
      "8023/8023 [==============================] - 4s 496us/step - loss: 1.5187 - accuracy: 0.0905 - val_loss: 1.2404 - val_accuracy: 0.0897\n",
      "Epoch 38/50\n",
      "8023/8023 [==============================] - 4s 527us/step - loss: 1.5359 - accuracy: 0.0924 - val_loss: 1.1873 - val_accuracy: 0.0852\n",
      "Epoch 39/50\n",
      "8023/8023 [==============================] - 4s 495us/step - loss: 1.5248 - accuracy: 0.0951 - val_loss: 1.1796 - val_accuracy: 0.0887\n",
      "Epoch 40/50\n",
      "8023/8023 [==============================] - 4s 506us/step - loss: 1.5220 - accuracy: 0.0993 - val_loss: 1.2127 - val_accuracy: 0.1082\n",
      "Epoch 41/50\n",
      "8023/8023 [==============================] - 4s 490us/step - loss: 1.5162 - accuracy: 0.1025 - val_loss: 1.2178 - val_accuracy: 0.0837\n",
      "Epoch 42/50\n",
      "8023/8023 [==============================] - 4s 508us/step - loss: 1.5753 - accuracy: 0.0935 - val_loss: 1.2507 - val_accuracy: 0.1022\n",
      "Epoch 43/50\n",
      "8023/8023 [==============================] - 5s 580us/step - loss: 1.5214 - accuracy: 0.0973 - val_loss: 1.1813 - val_accuracy: 0.1002\n",
      "Epoch 44/50\n",
      "8023/8023 [==============================] - 4s 480us/step - loss: 1.5284 - accuracy: 0.1023 - val_loss: 1.1913 - val_accuracy: 0.0842\n",
      "Epoch 45/50\n",
      "8023/8023 [==============================] - 4s 498us/step - loss: 1.5258 - accuracy: 0.0980 - val_loss: 1.1814 - val_accuracy: 0.0852\n",
      "Epoch 46/50\n",
      "8023/8023 [==============================] - 4s 536us/step - loss: 1.5212 - accuracy: 0.1000 - val_loss: 1.2150 - val_accuracy: 0.0922\n",
      "Epoch 47/50\n",
      "8023/8023 [==============================] - 3s 433us/step - loss: 1.5234 - accuracy: 0.0954 - val_loss: 1.2224 - val_accuracy: 0.0912\n",
      "Epoch 48/50\n",
      "8023/8023 [==============================] - 4s 459us/step - loss: 1.5192 - accuracy: 0.1017 - val_loss: 1.1833 - val_accuracy: 0.0917\n",
      "Epoch 49/50\n",
      "8023/8023 [==============================] - 4s 515us/step - loss: 1.5055 - accuracy: 0.0963 - val_loss: 1.2358 - val_accuracy: 0.0877\n",
      "Epoch 50/50\n",
      "8023/8023 [==============================] - 4s 503us/step - loss: 1.5178 - accuracy: 0.0870 - val_loss: 1.1987 - val_accuracy: 0.0867\n",
      "the NMAE for learning rate= 0.01 is: ReadsAvg    0.028785\n",
      "ReadsAvg    0.028786\n",
      "ReadsAvg    0.028787\n",
      "ReadsAvg    0.028787\n",
      "ReadsAvg    0.028788\n",
      "ReadsAvg    0.028788\n",
      "ReadsAvg    0.028788\n",
      "ReadsAvg    0.028789\n",
      "ReadsAvg    0.028790\n",
      "ReadsAvg    0.028791\n",
      "ReadsAvg    0.028792\n",
      "dtype: float64\n",
      "Train on 8023 samples, validate on 2006 samples\n",
      "Epoch 1/50\n",
      "8023/8023 [==============================] - 10s 1ms/step - loss: 35.0322 - accuracy: 0.0856 - val_loss: 22.8242 - val_accuracy: 0.0927\n",
      "Epoch 2/50\n",
      "8023/8023 [==============================] - 5s 605us/step - loss: 19.0029 - accuracy: 0.0900 - val_loss: 14.3970 - val_accuracy: 0.0857\n",
      "Epoch 3/50\n",
      "8023/8023 [==============================] - 4s 549us/step - loss: 8.2577 - accuracy: 0.0945 - val_loss: 2.2600 - val_accuracy: 0.0872\n",
      "Epoch 4/50\n",
      "8023/8023 [==============================] - 4s 561us/step - loss: 3.4195 - accuracy: 0.0906 - val_loss: 1.4648 - val_accuracy: 0.0882\n",
      "Epoch 5/50\n",
      "8023/8023 [==============================] - 4s 491us/step - loss: 2.8679 - accuracy: 0.0945 - val_loss: 1.3093 - val_accuracy: 0.0942\n",
      "Epoch 6/50\n",
      "8023/8023 [==============================] - 4s 540us/step - loss: 2.5291 - accuracy: 0.0944 - val_loss: 1.3382 - val_accuracy: 0.0932\n",
      "Epoch 7/50\n",
      "8023/8023 [==============================] - 4s 516us/step - loss: 2.2394 - accuracy: 0.0954 - val_loss: 1.2684 - val_accuracy: 0.0917\n",
      "Epoch 8/50\n",
      "8023/8023 [==============================] - 4s 498us/step - loss: 2.1268 - accuracy: 0.0957 - val_loss: 1.2155 - val_accuracy: 0.0967\n",
      "Epoch 9/50\n",
      "8023/8023 [==============================] - 4s 502us/step - loss: 2.0243 - accuracy: 0.0997 - val_loss: 1.2178 - val_accuracy: 0.0937\n",
      "Epoch 10/50\n",
      "8023/8023 [==============================] - 4s 500us/step - loss: 1.9978 - accuracy: 0.0956 - val_loss: 1.2392 - val_accuracy: 0.0917\n",
      "Epoch 11/50\n",
      "8023/8023 [==============================] - 4s 543us/step - loss: 1.9448 - accuracy: 0.0957 - val_loss: 1.2309 - val_accuracy: 0.0977\n",
      "Epoch 12/50\n",
      "8023/8023 [==============================] - 4s 525us/step - loss: 1.9214 - accuracy: 0.0992 - val_loss: 1.1992 - val_accuracy: 0.1037\n",
      "Epoch 13/50\n",
      "8023/8023 [==============================] - 4s 462us/step - loss: 1.8651 - accuracy: 0.0957 - val_loss: 1.2167 - val_accuracy: 0.0942\n",
      "Epoch 14/50\n",
      "8023/8023 [==============================] - 4s 558us/step - loss: 1.8594 - accuracy: 0.0973 - val_loss: 1.2112 - val_accuracy: 0.0947\n",
      "Epoch 15/50\n",
      "8023/8023 [==============================] - 4s 516us/step - loss: 1.8398 - accuracy: 0.0973 - val_loss: 1.2007 - val_accuracy: 0.0977\n",
      "Epoch 16/50\n",
      "8023/8023 [==============================] - 4s 505us/step - loss: 1.7965 - accuracy: 0.0892 - val_loss: 1.2084 - val_accuracy: 0.0987\n",
      "Epoch 17/50\n",
      "8023/8023 [==============================] - 4s 543us/step - loss: 1.7722 - accuracy: 0.0971 - val_loss: 1.1954 - val_accuracy: 0.0932\n",
      "Epoch 18/50\n",
      "8023/8023 [==============================] - 5s 633us/step - loss: 1.7602 - accuracy: 0.0963 - val_loss: 1.1804 - val_accuracy: 0.0947\n",
      "Epoch 19/50\n",
      "8023/8023 [==============================] - 4s 531us/step - loss: 1.7352 - accuracy: 0.0915 - val_loss: 1.2134 - val_accuracy: 0.0922\n",
      "Epoch 20/50\n",
      "8023/8023 [==============================] - 4s 508us/step - loss: 1.7222 - accuracy: 0.0931 - val_loss: 1.2024 - val_accuracy: 0.0907\n",
      "Epoch 21/50\n",
      "8023/8023 [==============================] - 4s 531us/step - loss: 1.7148 - accuracy: 0.0980 - val_loss: 1.2208 - val_accuracy: 0.0972\n",
      "Epoch 22/50\n",
      "8023/8023 [==============================] - 4s 535us/step - loss: 1.6946 - accuracy: 0.0963 - val_loss: 1.1876 - val_accuracy: 0.0932\n",
      "Epoch 23/50\n",
      "8023/8023 [==============================] - 4s 510us/step - loss: 1.6825 - accuracy: 0.0950 - val_loss: 1.1736 - val_accuracy: 0.0952\n",
      "Epoch 24/50\n",
      "8023/8023 [==============================] - 4s 495us/step - loss: 1.6840 - accuracy: 0.0978 - val_loss: 1.1781 - val_accuracy: 0.0967\n",
      "Epoch 25/50\n",
      "8023/8023 [==============================] - 4s 542us/step - loss: 1.6781 - accuracy: 0.0916 - val_loss: 1.2853 - val_accuracy: 0.0977\n",
      "Epoch 26/50\n",
      "8023/8023 [==============================] - 5s 605us/step - loss: 1.6691 - accuracy: 0.0983 - val_loss: 1.1789 - val_accuracy: 0.0997\n",
      "Epoch 27/50\n",
      "8023/8023 [==============================] - 4s 560us/step - loss: 1.6447 - accuracy: 0.0934 - val_loss: 1.2182 - val_accuracy: 0.1037\n",
      "Epoch 28/50\n",
      "8023/8023 [==============================] - 5s 566us/step - loss: 1.6377 - accuracy: 0.0996 - val_loss: 1.1779 - val_accuracy: 0.1052\n",
      "Epoch 29/50\n",
      "8023/8023 [==============================] - 4s 535us/step - loss: 1.6310 - accuracy: 0.0985 - val_loss: 1.1815 - val_accuracy: 0.0922\n",
      "Epoch 30/50\n",
      "8023/8023 [==============================] - 4s 551us/step - loss: 1.6288 - accuracy: 0.0889 - val_loss: 1.1896 - val_accuracy: 0.0967\n",
      "Epoch 31/50\n",
      "8023/8023 [==============================] - 4s 553us/step - loss: 1.6177 - accuracy: 0.0977 - val_loss: 1.1822 - val_accuracy: 0.0982\n",
      "Epoch 32/50\n",
      "8023/8023 [==============================] - 4s 555us/step - loss: 1.6122 - accuracy: 0.0996 - val_loss: 1.2312 - val_accuracy: 0.0927\n",
      "Epoch 33/50\n",
      "8023/8023 [==============================] - 4s 547us/step - loss: 1.6151 - accuracy: 0.0986 - val_loss: 1.1888 - val_accuracy: 0.0927\n",
      "Epoch 34/50\n",
      "8023/8023 [==============================] - 4s 544us/step - loss: 1.6085 - accuracy: 0.0935 - val_loss: 1.2227 - val_accuracy: 0.0957\n",
      "Epoch 35/50\n",
      "8023/8023 [==============================] - 4s 532us/step - loss: 1.5905 - accuracy: 0.1010 - val_loss: 1.2321 - val_accuracy: 0.0872\n",
      "Epoch 36/50\n",
      "8023/8023 [==============================] - 5s 576us/step - loss: 1.5853 - accuracy: 0.0895 - val_loss: 1.2164 - val_accuracy: 0.0957\n",
      "Epoch 37/50\n",
      "8023/8023 [==============================] - 5s 587us/step - loss: 1.5884 - accuracy: 0.0946 - val_loss: 1.2236 - val_accuracy: 0.1017\n",
      "Epoch 38/50\n",
      "8023/8023 [==============================] - 4s 484us/step - loss: 1.5791 - accuracy: 0.0971 - val_loss: 1.1993 - val_accuracy: 0.0932\n",
      "Epoch 39/50\n",
      "8023/8023 [==============================] - 5s 580us/step - loss: 1.5703 - accuracy: 0.0955 - val_loss: 1.2105 - val_accuracy: 0.0892\n",
      "Epoch 40/50\n",
      "8023/8023 [==============================] - 5s 575us/step - loss: 1.5753 - accuracy: 0.0987 - val_loss: 1.2473 - val_accuracy: 0.0927\n",
      "Epoch 41/50\n",
      "8023/8023 [==============================] - 5s 580us/step - loss: 1.5549 - accuracy: 0.0915 - val_loss: 1.1730 - val_accuracy: 0.0927\n",
      "Epoch 42/50\n",
      "8023/8023 [==============================] - 4s 509us/step - loss: 1.5655 - accuracy: 0.0932 - val_loss: 1.2163 - val_accuracy: 0.0982\n",
      "Epoch 43/50\n",
      "8023/8023 [==============================] - 5s 570us/step - loss: 1.5526 - accuracy: 0.1054 - val_loss: 1.2104 - val_accuracy: 0.0902\n",
      "Epoch 44/50\n",
      "8023/8023 [==============================] - 4s 503us/step - loss: 1.5582 - accuracy: 0.0995 - val_loss: 1.1990 - val_accuracy: 0.0952\n",
      "Epoch 45/50\n",
      "8023/8023 [==============================] - 4s 535us/step - loss: 1.5401 - accuracy: 0.0996 - val_loss: 1.1911 - val_accuracy: 0.0977\n",
      "Epoch 46/50\n",
      "8023/8023 [==============================] - 5s 599us/step - loss: 1.5374 - accuracy: 0.0934 - val_loss: 1.1589 - val_accuracy: 0.0952\n",
      "Epoch 47/50\n",
      "8023/8023 [==============================] - 5s 587us/step - loss: 1.5426 - accuracy: 0.0982 - val_loss: 1.1908 - val_accuracy: 0.0992\n",
      "Epoch 48/50\n",
      "8023/8023 [==============================] - 4s 525us/step - loss: 1.5353 - accuracy: 0.0962 - val_loss: 1.1597 - val_accuracy: 0.0952\n",
      "Epoch 49/50\n",
      "8023/8023 [==============================] - 4s 511us/step - loss: 1.5369 - accuracy: 0.0998 - val_loss: 1.2986 - val_accuracy: 0.0947\n",
      "Epoch 50/50\n",
      "8023/8023 [==============================] - 4s 501us/step - loss: 1.5261 - accuracy: 0.0962 - val_loss: 1.1882 - val_accuracy: 0.0927\n",
      "the NMAE for learning rate= 0.001 is: ReadsAvg    0.028833\n",
      "ReadsAvg    0.028834\n",
      "ReadsAvg    0.028835\n",
      "ReadsAvg    0.028835\n",
      "ReadsAvg    0.028835\n",
      "ReadsAvg    0.028836\n",
      "ReadsAvg    0.028836\n",
      "ReadsAvg    0.028837\n",
      "ReadsAvg    0.028838\n",
      "ReadsAvg    0.028838\n",
      "ReadsAvg    0.028839\n",
      "dtype: float64\n",
      "Train on 8023 samples, validate on 2006 samples\n",
      "Epoch 1/50\n",
      "8023/8023 [==============================] - 9s 1ms/step - loss: 55.9777 - accuracy: 0.0881 - val_loss: 54.8106 - val_accuracy: 0.0887\n",
      "Epoch 2/50\n",
      "8023/8023 [==============================] - 5s 601us/step - loss: 53.1932 - accuracy: 0.0877 - val_loss: 47.9532 - val_accuracy: 0.0907\n",
      "Epoch 3/50\n",
      "8023/8023 [==============================] - 5s 572us/step - loss: 35.8172 - accuracy: 0.0854 - val_loss: 29.9669 - val_accuracy: 0.0902\n",
      "Epoch 4/50\n",
      "8023/8023 [==============================] - 4s 534us/step - loss: 26.1615 - accuracy: 0.0870 - val_loss: 26.9881 - val_accuracy: 0.0892\n",
      "Epoch 5/50\n",
      "8023/8023 [==============================] - 4s 518us/step - loss: 25.1219 - accuracy: 0.0896 - val_loss: 26.1323 - val_accuracy: 0.0892\n",
      "Epoch 6/50\n",
      "8023/8023 [==============================] - 4s 503us/step - loss: 24.4154 - accuracy: 0.0895 - val_loss: 25.5135 - val_accuracy: 0.0917\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8023/8023 [==============================] - 4s 521us/step - loss: 23.8722 - accuracy: 0.0936 - val_loss: 24.8611 - val_accuracy: 0.0867\n",
      "Epoch 8/50\n",
      "8023/8023 [==============================] - 4s 515us/step - loss: 23.1571 - accuracy: 0.0919 - val_loss: 24.2300 - val_accuracy: 0.0877\n",
      "Epoch 9/50\n",
      "8023/8023 [==============================] - 4s 519us/step - loss: 22.7896 - accuracy: 0.0885 - val_loss: 23.8058 - val_accuracy: 0.0932\n",
      "Epoch 10/50\n",
      "8023/8023 [==============================] - 4s 535us/step - loss: 22.1401 - accuracy: 0.0871 - val_loss: 23.0521 - val_accuracy: 0.0882\n",
      "Epoch 11/50\n",
      "8023/8023 [==============================] - 4s 525us/step - loss: 21.3528 - accuracy: 0.0912 - val_loss: 22.5400 - val_accuracy: 0.0897\n",
      "Epoch 12/50\n",
      "8023/8023 [==============================] - 4s 466us/step - loss: 21.0375 - accuracy: 0.0869 - val_loss: 21.9877 - val_accuracy: 0.0937\n",
      "Epoch 13/50\n",
      "8023/8023 [==============================] - 4s 536us/step - loss: 20.6506 - accuracy: 0.0843 - val_loss: 21.3039 - val_accuracy: 0.0917\n",
      "Epoch 14/50\n",
      "8023/8023 [==============================] - 5s 627us/step - loss: 20.0339 - accuracy: 0.0907 - val_loss: 20.7184 - val_accuracy: 0.0892\n",
      "Epoch 15/50\n",
      "8023/8023 [==============================] - 5s 578us/step - loss: 19.5148 - accuracy: 0.0945 - val_loss: 20.1050 - val_accuracy: 0.0897\n",
      "Epoch 16/50\n",
      "8023/8023 [==============================] - 4s 559us/step - loss: 18.9161 - accuracy: 0.0925 - val_loss: 19.3105 - val_accuracy: 0.0907\n",
      "Epoch 17/50\n",
      "8023/8023 [==============================] - 5s 638us/step - loss: 18.0075 - accuracy: 0.0879 - val_loss: 18.5979 - val_accuracy: 0.0847\n",
      "Epoch 18/50\n",
      "8023/8023 [==============================] - 5s 593us/step - loss: 17.4011 - accuracy: 0.0906 - val_loss: 17.8117 - val_accuracy: 0.0867\n",
      "Epoch 19/50\n",
      "8023/8023 [==============================] - 5s 607us/step - loss: 16.7820 - accuracy: 0.0860 - val_loss: 17.0863 - val_accuracy: 0.0847\n",
      "Epoch 20/50\n",
      "8023/8023 [==============================] - 5s 599us/step - loss: 16.0251 - accuracy: 0.0881 - val_loss: 16.3437 - val_accuracy: 0.0842\n",
      "Epoch 21/50\n",
      "8023/8023 [==============================] - 5s 583us/step - loss: 15.3128 - accuracy: 0.0863 - val_loss: 15.4204 - val_accuracy: 0.0833\n",
      "Epoch 22/50\n",
      "8023/8023 [==============================] - 5s 572us/step - loss: 14.5938 - accuracy: 0.0906 - val_loss: 14.6071 - val_accuracy: 0.0818\n",
      "Epoch 23/50\n",
      "8023/8023 [==============================] - 4s 554us/step - loss: 13.7206 - accuracy: 0.0886 - val_loss: 13.5596 - val_accuracy: 0.0808\n",
      "Epoch 24/50\n",
      "8023/8023 [==============================] - 4s 548us/step - loss: 12.9476 - accuracy: 0.0886 - val_loss: 12.5031 - val_accuracy: 0.0833\n",
      "Epoch 25/50\n",
      "8023/8023 [==============================] - 4s 558us/step - loss: 11.9689 - accuracy: 0.0881 - val_loss: 11.5366 - val_accuracy: 0.0833\n",
      "Epoch 26/50\n",
      "8023/8023 [==============================] - 5s 593us/step - loss: 11.0210 - accuracy: 0.0906 - val_loss: 10.4057 - val_accuracy: 0.0837\n",
      "Epoch 27/50\n",
      "8023/8023 [==============================] - 4s 525us/step - loss: 10.0560 - accuracy: 0.0935 - val_loss: 9.2675 - val_accuracy: 0.0862\n",
      "Epoch 28/50\n",
      "8023/8023 [==============================] - 4s 505us/step - loss: 8.8819 - accuracy: 0.0889 - val_loss: 7.9687 - val_accuracy: 0.0842\n",
      "Epoch 29/50\n",
      "8023/8023 [==============================] - 4s 505us/step - loss: 7.6956 - accuracy: 0.0914 - val_loss: 6.4794 - val_accuracy: 0.0857\n",
      "Epoch 30/50\n",
      "8023/8023 [==============================] - 4s 508us/step - loss: 6.5695 - accuracy: 0.0907 - val_loss: 5.1406 - val_accuracy: 0.0887\n",
      "Epoch 31/50\n",
      "8023/8023 [==============================] - 4s 514us/step - loss: 5.4099 - accuracy: 0.0921 - val_loss: 3.6771 - val_accuracy: 0.0892\n",
      "Epoch 32/50\n",
      "8023/8023 [==============================] - 4s 512us/step - loss: 4.4328 - accuracy: 0.0947 - val_loss: 2.6495 - val_accuracy: 0.0837\n",
      "Epoch 33/50\n",
      "8023/8023 [==============================] - 5s 562us/step - loss: 3.7675 - accuracy: 0.0889 - val_loss: 2.1268 - val_accuracy: 0.0897\n",
      "Epoch 34/50\n",
      "8023/8023 [==============================] - 4s 509us/step - loss: 3.3732 - accuracy: 0.0920 - val_loss: 1.7821 - val_accuracy: 0.0852\n",
      "Epoch 35/50\n",
      "8023/8023 [==============================] - 4s 516us/step - loss: 3.1391 - accuracy: 0.0939 - val_loss: 1.6694 - val_accuracy: 0.0877\n",
      "Epoch 36/50\n",
      "8023/8023 [==============================] - 4s 518us/step - loss: 3.0138 - accuracy: 0.0907 - val_loss: 1.6094 - val_accuracy: 0.0862\n",
      "Epoch 37/50\n",
      "8023/8023 [==============================] - 4s 509us/step - loss: 2.9137 - accuracy: 0.0896 - val_loss: 1.5541 - val_accuracy: 0.0852\n",
      "Epoch 38/50\n",
      "8023/8023 [==============================] - 4s 498us/step - loss: 2.8651 - accuracy: 0.0881 - val_loss: 1.4123 - val_accuracy: 0.0872\n",
      "Epoch 39/50\n",
      "8023/8023 [==============================] - 4s 500us/step - loss: 2.7786 - accuracy: 0.0880 - val_loss: 1.3777 - val_accuracy: 0.0877\n",
      "Epoch 40/50\n",
      "8023/8023 [==============================] - 4s 506us/step - loss: 2.7253 - accuracy: 0.0907 - val_loss: 1.3537 - val_accuracy: 0.0927\n",
      "Epoch 41/50\n",
      "8023/8023 [==============================] - 4s 507us/step - loss: 2.6826 - accuracy: 0.0869 - val_loss: 1.3286 - val_accuracy: 0.0952\n",
      "Epoch 42/50\n",
      "8023/8023 [==============================] - 4s 518us/step - loss: 2.6017 - accuracy: 0.0931 - val_loss: 1.3226 - val_accuracy: 0.0917\n",
      "Epoch 43/50\n",
      "8023/8023 [==============================] - 4s 506us/step - loss: 2.5867 - accuracy: 0.0900 - val_loss: 1.2779 - val_accuracy: 0.0987\n",
      "Epoch 44/50\n",
      "8023/8023 [==============================] - 4s 509us/step - loss: 2.5338 - accuracy: 0.0916 - val_loss: 1.2734 - val_accuracy: 0.0937\n",
      "Epoch 45/50\n",
      "8023/8023 [==============================] - 5s 566us/step - loss: 2.4668 - accuracy: 0.0950 - val_loss: 1.2534 - val_accuracy: 0.0972\n",
      "Epoch 46/50\n",
      "8023/8023 [==============================] - 4s 506us/step - loss: 2.4609 - accuracy: 0.0921 - val_loss: 1.2496 - val_accuracy: 0.0907\n",
      "Epoch 47/50\n",
      "8023/8023 [==============================] - 4s 497us/step - loss: 2.4183 - accuracy: 0.0971 - val_loss: 1.2342 - val_accuracy: 0.0937\n",
      "Epoch 48/50\n",
      "8023/8023 [==============================] - 4s 548us/step - loss: 2.3829 - accuracy: 0.0902 - val_loss: 1.2303 - val_accuracy: 0.0922\n",
      "Epoch 49/50\n",
      "8023/8023 [==============================] - 5s 567us/step - loss: 2.3293 - accuracy: 0.0920 - val_loss: 1.2305 - val_accuracy: 0.0972\n",
      "Epoch 50/50\n",
      "8023/8023 [==============================] - 5s 567us/step - loss: 2.3393 - accuracy: 0.0931 - val_loss: 1.2364 - val_accuracy: 0.0927\n",
      "the NMAE for learning rate= 0.0001 is: ReadsAvg    0.029736\n",
      "ReadsAvg    0.029738\n",
      "ReadsAvg    0.029738\n",
      "ReadsAvg    0.029738\n",
      "ReadsAvg    0.029739\n",
      "ReadsAvg    0.029739\n",
      "ReadsAvg    0.029740\n",
      "ReadsAvg    0.029741\n",
      "ReadsAvg    0.029741\n",
      "ReadsAvg    0.029742\n",
      "ReadsAvg    0.029743\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#tuning learning parameters\n",
    "Rnn_xtrain, Rnn_ytrain = fetch_data(X_train, Y_train, 11, 1)\n",
    "Rnn_xtest, Rnn_ytest = fetch_data(X_test, Y_test, 11, 1)\n",
    "Rnn_model = LSTM_tune_learningrate(Rnn_xtrain, Rnn_ytrain,Rnn_xtest, Rnn_ytest, 1, 11, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ca1d3dd118f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# unable to fetch input to model data for l=0 -Reshape error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mRnn_xtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRnn_ytrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mRnn_xtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRnn_ytest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mRnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_tune_learningrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRnn_xtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRnn_ytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mRnn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRnn_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRnn_xtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# unable to fetch input to model data for l=0 -Reshape error\n",
    "Rnn_xtrain, Rnn_ytrain = fetch_data(X_train, Y_train, 11, 0)\n",
    "Rnn_xtest, Rnn_ytest = fetch_data(X_test, Y_test, 11, 0)\n",
    "Rnn_model = LSTM_tune_learningrate(Rnn_xtrain, Rnn_ytrain, 0, 11, 16)\n",
    "Rnn_test = Rnn_reshape(Rnn_xtest, 0, 16)\n",
    "Rnn_pred = Rnn_model.predict(Rnn_test,verbose=0)\n",
    "nmaes_l = nmaes_array(Rnn_ytest, Rnn_pred, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best hyper parameters are : learning rate: 0.01; epochs: 50; batch_size: 32; nodes: 50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to step 7 of Task I, for values of l = 0,...,10 and h = 0,...,10 train LSTM models. Evaluate\n",
    "the models by computing the error (NMAE) on the test set. Display the results in a table with rows\n",
    "representing the time horizon h = 0,...,10 and columns representing the lag l = 0,...,10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fa67ce619a46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnmaesdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mRnn_xtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRnn_ytrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mRnn_xtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRnn_ytest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mRnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRnn_xtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRnn_ytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "nmaesdf = pd.DataFrame()\n",
    "for l in range(0, 11):\n",
    "    Rnn_xtrain, Rnn_ytrain = fetch_data(X_train, Y_train, 11, l+1)\n",
    "    Rnn_xtest, Rnn_ytest = fetch_data(X_test, Y_test, 11, l+1)\n",
    "    Rnn_model = LSTM_model(Rnn_xtrain, Rnn_ytrain, l+1, 11, 16)\n",
    "    Rnn_test = Rnn_reshape(Rnn_xtest, l+1, 16)\n",
    "    Rnn_pred = Rnn_model.predict(Rnn_test,verbose=0)\n",
    "    nmaes_l = nmaes_array(Rnn_ytest, Rnn_pred, 10)\n",
    "    nmaesdf['nmaes_l'+str(l)] = nmaes_l\n",
    "    print(nmaesdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nmaesdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b2127c261f2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnmaesdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nmaesdf' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'nmaesdf' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store nmaesdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nmaes_l0</th>\n",
       "      <th>nmaes_l1</th>\n",
       "      <th>nmaes_l2</th>\n",
       "      <th>nmaes_l3</th>\n",
       "      <th>nmaes_l4</th>\n",
       "      <th>nmaes_l5</th>\n",
       "      <th>nmaes_l6</th>\n",
       "      <th>nmaes_l7</th>\n",
       "      <th>nmaes_l8</th>\n",
       "      <th>nmaes_l9</th>\n",
       "      <th>nmaes_l10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031264</td>\n",
       "      <td>0.029403</td>\n",
       "      <td>0.027921</td>\n",
       "      <td>0.027615</td>\n",
       "      <td>0.027406</td>\n",
       "      <td>0.027237</td>\n",
       "      <td>0.027024</td>\n",
       "      <td>0.026847</td>\n",
       "      <td>0.026818</td>\n",
       "      <td>0.026838</td>\n",
       "      <td>0.026852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027512</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.026215</td>\n",
       "      <td>0.026261</td>\n",
       "      <td>0.026266</td>\n",
       "      <td>0.026172</td>\n",
       "      <td>0.026029</td>\n",
       "      <td>0.026104</td>\n",
       "      <td>0.026204</td>\n",
       "      <td>0.026251</td>\n",
       "      <td>0.026285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025726</td>\n",
       "      <td>0.025804</td>\n",
       "      <td>0.025950</td>\n",
       "      <td>0.026134</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.026048</td>\n",
       "      <td>0.026091</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.026223</td>\n",
       "      <td>0.026287</td>\n",
       "      <td>0.026274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026760</td>\n",
       "      <td>0.027050</td>\n",
       "      <td>0.027372</td>\n",
       "      <td>0.027666</td>\n",
       "      <td>0.027652</td>\n",
       "      <td>0.027636</td>\n",
       "      <td>0.027540</td>\n",
       "      <td>0.027592</td>\n",
       "      <td>0.027577</td>\n",
       "      <td>0.027668</td>\n",
       "      <td>0.027726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028625</td>\n",
       "      <td>0.029147</td>\n",
       "      <td>0.029629</td>\n",
       "      <td>0.029707</td>\n",
       "      <td>0.029772</td>\n",
       "      <td>0.029554</td>\n",
       "      <td>0.029551</td>\n",
       "      <td>0.029501</td>\n",
       "      <td>0.029505</td>\n",
       "      <td>0.029638</td>\n",
       "      <td>0.029653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.030524</td>\n",
       "      <td>0.031262</td>\n",
       "      <td>0.031352</td>\n",
       "      <td>0.031388</td>\n",
       "      <td>0.031178</td>\n",
       "      <td>0.031098</td>\n",
       "      <td>0.031006</td>\n",
       "      <td>0.030922</td>\n",
       "      <td>0.030986</td>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.031141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.032876</td>\n",
       "      <td>0.032920</td>\n",
       "      <td>0.032605</td>\n",
       "      <td>0.032497</td>\n",
       "      <td>0.032294</td>\n",
       "      <td>0.032133</td>\n",
       "      <td>0.032073</td>\n",
       "      <td>0.032015</td>\n",
       "      <td>0.032159</td>\n",
       "      <td>0.032117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.033674</td>\n",
       "      <td>0.034057</td>\n",
       "      <td>0.033689</td>\n",
       "      <td>0.033511</td>\n",
       "      <td>0.033290</td>\n",
       "      <td>0.033061</td>\n",
       "      <td>0.032954</td>\n",
       "      <td>0.032793</td>\n",
       "      <td>0.032844</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.032781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.034617</td>\n",
       "      <td>0.034562</td>\n",
       "      <td>0.034357</td>\n",
       "      <td>0.034045</td>\n",
       "      <td>0.033710</td>\n",
       "      <td>0.033553</td>\n",
       "      <td>0.033313</td>\n",
       "      <td>0.033303</td>\n",
       "      <td>0.033167</td>\n",
       "      <td>0.033155</td>\n",
       "      <td>0.032993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.034570</td>\n",
       "      <td>0.034677</td>\n",
       "      <td>0.034423</td>\n",
       "      <td>0.034033</td>\n",
       "      <td>0.033869</td>\n",
       "      <td>0.033578</td>\n",
       "      <td>0.033505</td>\n",
       "      <td>0.033370</td>\n",
       "      <td>0.033266</td>\n",
       "      <td>0.033104</td>\n",
       "      <td>0.033063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.035133</td>\n",
       "      <td>0.035168</td>\n",
       "      <td>0.034741</td>\n",
       "      <td>0.034471</td>\n",
       "      <td>0.034089</td>\n",
       "      <td>0.033972</td>\n",
       "      <td>0.033766</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>0.033394</td>\n",
       "      <td>0.033341</td>\n",
       "      <td>0.033184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nmaes_l0  nmaes_l1  nmaes_l2  nmaes_l3  nmaes_l4  nmaes_l5  nmaes_l6  \\\n",
       "0   0.031264  0.029403  0.027921  0.027615  0.027406  0.027237  0.027024   \n",
       "1   0.027512  0.026217  0.026215  0.026261  0.026266  0.026172  0.026029   \n",
       "2   0.025726  0.025804  0.025950  0.026134  0.026200  0.026048  0.026091   \n",
       "3   0.026760  0.027050  0.027372  0.027666  0.027652  0.027636  0.027540   \n",
       "4   0.028625  0.029147  0.029629  0.029707  0.029772  0.029554  0.029551   \n",
       "5   0.030524  0.031262  0.031352  0.031388  0.031178  0.031098  0.031006   \n",
       "6   0.032410  0.032876  0.032920  0.032605  0.032497  0.032294  0.032133   \n",
       "7   0.033674  0.034057  0.033689  0.033511  0.033290  0.033061  0.032954   \n",
       "8   0.034617  0.034562  0.034357  0.034045  0.033710  0.033553  0.033313   \n",
       "9   0.034570  0.034677  0.034423  0.034033  0.033869  0.033578  0.033505   \n",
       "10  0.035133  0.035168  0.034741  0.034471  0.034089  0.033972  0.033766   \n",
       "\n",
       "    nmaes_l7  nmaes_l8  nmaes_l9  nmaes_l10  \n",
       "0   0.026847  0.026818  0.026838   0.026852  \n",
       "1   0.026104  0.026204  0.026251   0.026285  \n",
       "2   0.026144  0.026223  0.026287   0.026274  \n",
       "3   0.027592  0.027577  0.027668   0.027726  \n",
       "4   0.029501  0.029505  0.029638   0.029653  \n",
       "5   0.030922  0.030986  0.030981   0.031141  \n",
       "6   0.032073  0.032015  0.032159   0.032117  \n",
       "7   0.032793  0.032844  0.032800   0.032781  \n",
       "8   0.033303  0.033167  0.033155   0.032993  \n",
       "9   0.033370  0.033266  0.033104   0.033063  \n",
       "10  0.033642  0.033394  0.033341   0.033184  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmaesdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=11, step=1)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmaesdf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'NMAE'), Text(0.5, 0, 'lag')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, linep_ = plt.subplots(figsize = (10,6), dpi = 100)\n",
    "\n",
    "linep_ = sns.lineplot(data=nmaesdf, x=nmaesdf.index, y=\"nmaes_l0\", label=\"h=0\")\n",
    "sns.lineplot(data=nmaesdf, x=nmaesdf.index, y=\"nmaes_l1\", label=\"h=1\")\n",
    "sns.lineplot(data=nmaesdf, x=nmaesdf.index, y=\"nmaes_l2\", label=\"h=2\")\n",
    "sns.lineplot(data=nmaesdf, x=nmaesdf.index, y=\"nmaes_l3\", label=\"h=3\")\n",
    "sns.lineplot(data=nmaesdf, x=nmaesdf.index, y=\"nmaes_l4\", label=\"h=4\")\n",
    "sns.lineplot(data=nmaesdf, x=nmaesdf.index, y=\"nmaes_l5\", label=\"h=5\")\n",
    "sns.lineplot(data=nmaesdf, x=nmaesdf.index, y=\"nmaes_l6\", label=\"h=6\")\n",
    "sns.lineplot(data=nmaesdf, x=nmaesdf.index, y=\"nmaes_l7\", label=\"h=7\")\n",
    "sns.lineplot(data=nmaesdf, x=nmaesdf.index, y=\"nmaes_l8\", label=\"h=8\")\n",
    "sns.lineplot(data=nmaesdf, x=nmaesdf.index, y=\"nmaes_l9\", label=\"h=9\")\n",
    "sns.lineplot(data=nmaesdf, x=nmaesdf.index, y=\"nmaes_l10\", label=\"h=10\")\n",
    "linep_.set(title=\"NMAE vs lag\")\n",
    "\n",
    "linep_.set(xlabel='lag', ylabel='NMAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('lags.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
